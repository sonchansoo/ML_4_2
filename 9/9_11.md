## 🔍 강의 요약
이번 강의에서는 대표적인 분류 알고리즘 중 하나인 Support Vector Machine (SVM)에 대해 학습합니다. SVM의 핵심 목표는 각 클래스를 구분하는 최적의 결정 경계(Decision Boundary)를 찾는 것이며, 이때 마진(Margin)을 최대화하는 것이 중요합니다. 선형으로 분리가 어려운 데이터를 다루기 위한 두 가지 핵심 전략인 Soft Margin과 Kernel Trick에 대해 자세히 알아보고, 이와 관련된 하이퍼파라미터 C와 gamma의 역할을 이해합니다. 마지막으로, 본질적으로 이진 분류기인 SVM을 다중 클래스 문제에 적용하는 One-vs-All과 One-vs-One 방법에 대해 배웁니다.

## 💡 핵심 포인트
- **최대 마진 분류기**: SVM은 두 클래스 간의 간격, 즉 마진(Margin)을 최대화하는 결정 경계(Hyperplane)를 찾는 것을 목표로 합니다.
- **서포트 벡터 (Support Vectors)**: 마진을 결정하는 데 직접적으로 영향을 미치는, 결정 경계에 가장 가까운 데이터 포인트들입니다.
- **Soft Margin (C 파라미터)**: 선형 분리가 완벽하지 않은 데이터에 대해 일부 오류를 허용하는 방법입니다. C 값이 클수록 오류에 대한 페널티가 커져 마진이 좁아지고, 작을수록 페널티가 작아져 마진이 넓어집니다.
- **Kernel Trick (kernel, gamma 파라미터)**: 저차원에서 선형 분리가 불가능한 데이터를 고차원으로 매핑하여 선형 분리가 가능하도록 만드는 기법입니다. `linear`, `poly`, `rbf` 등의 커널 함수가 사용됩니다.
- **다중 클래스 확장**: 이진 분류기인 SVM을 여러 클래스 분류에 사용하기 위해 **One-vs-All** 또는 **One-vs-One** 전략을 사용합니다.

## 세부 내용 📚
### 1️⃣ SVM (Support Vector Machine)의 기본 개념
SVM은 두 개의 그룹으로 데이터를 분류하는 강력한 지도학습 모델입니다.
- **결정 경계 (Decision Boundary)**: 두 데이터 클러스터를 나누는 선(2D), 평면(3D), 또는 초평면(Hyperplane, n-D)을 의미합니다.
- **마진 (Margin)**: 결정 경계와 가장 가까운 데이터 포인트(서포트 벡터) 사이의 거리를 의미합니다. SVM은 이 마진을 최대화하는 것을 목표로 합니다.
- **서포트 벡터 (Support Vectors)**: 마진의 경계에 위치하는 데이터 포인트들로, 이 벡터들이 결정 경계를 정의하는 데 핵심적인 역할을 합니다.

### 2️⃣ 선형 분리가 불가능한 데이터 해결 방안
현실의 데이터는 완벽하게 선형으로 분리되지 않는 경우가 많습니다. SVM은 이를 해결하기 위해 두 가지 방법을 제공합니다.

#### 🔹 Soft Margin
- **Hard Margin**: 모든 데이터를 완벽하게 분류하고 마진 안에 어떤 데이터 포인트도 허용하지 않는 엄격한 방식입니다. 데이터에 노이즈가 있거나 겹치는 경우 사용할 수 없습니다.
- **Soft Margin**: 일부 데이터 포인트가 마진을 침범하거나 잘못 분류되는 것을 허용하여 더 유연한 결정 경계를 찾는 방식입니다.
- **C (Penalty) 파라미터**:
    - **높은 C 값**: 분류 오류에 대한 페널티가 큽니다. 모델이 훈련 데이터에 있는 오류를 최소화하려고 하므로 마진이 좁아지고, 과적합(Overfitting)의 위험이 있습니다.
    - **낮은 C 값**: 오류에 대한 페널티가 작습니다. 더 넓은 마진을 허용하지만, 훈련 데이터에 대한 오류가 늘어날 수 있습니다.
    - 최적의 C 값은 **교차 검증(Cross Validation)**을 통해 찾습니다.

#### 🔹 Kernel Trick
- **핵심 아이디어**: 데이터를 더 높은 차원의 공간으로 "투영(Project)"하여 선형 분리가 가능하게 만드는 기법입니다.
- **장점**: 실제로 데이터를 고차원으로 변환하지 않고도 내적(dot product) 계산을 통해 고차원에서의 분류 효과를 얻을 수 있어 계산적으로 매우 효율적입니다.
- **주요 커널 함수**:
    - **Linear**: 기본 커널로, 선형 분리가 가능한 경우에 사용됩니다.
    - **Polynomial**: 다항식 형태로 데이터를 변환하여 곡선 형태의 결정 경계를 만듭니다.
    - **Gaussian (RBF)**: 방사 기저 함수(Radial Basis Function)로, 복잡한 형태의 결정 경계를 만들 수 있어 널리 사용됩니다.
- **gamma (γ) 파라미터 (RBF 커널에서 사용)**:
    - **높은 gamma 값**: 하나의 훈련 데이터가 미치는 영향의 범위가 좁아져 모델이 매우 복잡해지고 과적합될 수 있습니다.
    - **낮은 gamma 값**: 영향의 범위가 넓어져 결정 경계가 부드러워집니다.

### 3️⃣ 다중 클래스 분류 (Multi-Class Classification)
SVM은 기본적으로 이진 분류기이므로, 3개 이상의 클래스를 분류하기 위해서는 특별한 전략이 필요합니다.
- **One-vs-All (One-vs-Rest)**:
    - N개의 클래스가 있을 때, 각 클래스마다 "해당 클래스 vs 나머지 모든 클래스"를 구분하는 N개의 이진 분류기를 학습시킵니다.
    - 새로운 데이터가 들어오면 N개의 분류기 모두를 통과시켜 가장 높은 신뢰도(확률) 점수를 내는 클래스로 예측합니다.
- **One-vs-One**:
    - N개의 클래스에 대해 가능한 모든 클래스 쌍(pair)에 대한 이진 분류기를 학습시킵니다. 총 `N * (N-1) / 2`개의 분류기가 생성됩니다.
    - 새로운 데이터가 들어오면 모든 분류기가 예측을 수행하고, 가장 많은 표를 얻은 클래스를 최종 클래스로 선택합니다(다수결 원칙).
    - 데이터셋이 매우 클 때 더 효율적이며, **Scikit-learn의 SVM에서는 이 방식이 기본으로 사용됩니다.**

### 4️⃣ SVM의 장단점
- **장점**:
    - 클래스 간의 경계가 명확할 때 성능이 매우 좋습니다.
    - 고차원 데이터(특성이 많은 데이터)에서 효과적입니다.
    - 서포트 벡터만 사용하여 모델을 정의하므로 메모리가 효율적입니다.
- **단점**:
    - 대규모 데이터셋에서는 훈련 시간이 오래 걸릴 수 있습니다.
    - 데이터에 노이즈가 많거나 클래스가 많이 겹치는 경우 성능이 저하될 수 있습니다.
    - 직접적으로 확률 값을 제공하지 않습니다.

## 오늘의 연습문제 📝
- **문제 1**: SVM의 주요 목표는 무엇이며, '마진(Margin)'과 '서포트 벡터(Support Vectors)'의 역할은 무엇인지 설명해 보세요.
- **문제 2**: 선형으로 분리할 수 없는 데이터셋을 SVM으로 분류하려고 할 때 사용할 수 있는 두 가지 주요 방법과, 각 방법에서 조절해야 할 핵심 하이퍼파라미터(C, kernel, gamma 등)에 대해 설명해 보세요.
- **문제 3**: SVM은 본질적으로 이진 분류기입니다. 3개의 클래스(A, B, C)를 가진 데이터셋을 분류하기 위해 'One-vs-One' 전략을 사용할 때, 총 몇 개의 분류기가 필요한지 설명하고, 새로운 데이터가 들어왔을 때 최종 클래스를 어떻게 결정하는지 설명해 보세요.
