## 🔍 강의 요약
이번 강의에서는 머신러닝 모델의 과적합(Overfitting)을 방지하고 예측 성능을 향상시키기 위한 핵심 기법인 정규화(Regularization)에 대해 학습합니다. 기본 선형 회귀 모델의 한계를 알아보고, 이를 개선하기 위해 비용 함수(Cost Function)에 페널티 항을 추가하는 정규화의 원리를 배웁니다. 특히, 대표적인 정규화 기법인 Ridge Regression(L2), LASSO Regression(L1), 그리고 이 둘을 결합한 Elastic Net Regression의 특징과 수학적 원리, 실제 코드 구현 방법을 자세히 살펴봅니다.

## 💡 핵심 포인트
- **정규화 (Regularization)**: 모델이 훈련 데이터에 과도하게 적합되는 것을 막기 위해 회귀 계수(β)의 크기에 제약을 가하는 기법입니다. 이를 통해 모델의 복잡도를 낮추고 일반화 성능을 높입니다.
- **Ridge Regression (L2 정규화)**: 모든 변수의 회귀 계수를 0에 가깝게 축소(Shrink)하지만, 0으로 만들지는 않습니다. 변수가 많을 때 유용합니다.
- **LASSO Regression (L1 정규화)**: 중요하지 않은 변수의 회귀 계수를 완전히 0으로 만들어 해당 변수를 모델에서 제거하는 효과를 가집니다. 이를 통해 **특성 선택(Feature Selection)**이 자동으로 이루어집니다.
- **Elastic Net Regression**: Ridge와 LASSO의 장점을 결합한 방식으로, 서로 상관관계가 높은 변수들을 함께 선택하거나 제외하는 경향이 있습니다.
- **람다(λ) 또는 알파(α)**: 정규화의 강도를 조절하는 하이퍼파라미터로, 교차 검증(Cross-validation)을 통해 최적의 값을 찾습니다.

## 세부 내용 📚
### 1️⃣ 회귀 모델의 과적합 문제와 정규화의 필요성
- **단순 선형 회귀(Simple Linear Regression)**: `y = α + βx` 형태의 모델로, 최소 제곱법(Least Squares)을 사용하여 오차를 최소화하는 계수를 찾습니다.
- **R²(결정 계수)의 한계**: 모델에 예측 변수(Feature)를 추가할수록 R² 값은 항상 증가하거나 유지되므로, 불필요한 변수가 포함된 복잡한 모델을 좋다고 판단할 수 있습니다.
- **Adjusted R²(수정된 결정 계수)**: 변수의 개수를 고려하여 R² 값을 보정하지만, 여전히 불필요한 변수를 포함할 수 있습니다.
- **정규화(Regularization)의 등장**: 모델의 복잡도에 페널티를 부여하여 과적합을 방지하고, 테스트 데이터에 대한 예측 정확도를 높이는 더 근본적인 해결책입니다. 이는 **편향-분산 트레이드오프(Bias-Variance Tradeoff)**를 조절하는 과정입니다.

### 2️⃣ Ridge Regression (L2 정규화)
- **목표**: 기존의 비용 함수(RSS, 잔차 제곱합)에 **L2 페널티 항**을 추가하여 최소화합니다.
- **수식**: `RSS + λ * Σ(β_j)²`
- **특징**:
    - **λ (람다)**: 정규화 강도 조절 파라미터.
        - **λ = 0**: 일반적인 선형 회귀(OLS)와 동일합니다.
        - **λ가 커질수록**: 회귀 계수(β)가 0에 가까워지지만, 완전히 0이 되지는 않습니다.
    - 모든 변수를 모델에 유지하면서 영향력만 줄이는 방식입니다.
    - **데이터 스케일링(Standardizing)**이 중요합니다. 변수들의 스케일이 다르면 페널티가 불공정하게 적용될 수 있습니다.

### 3️⃣ LASSO Regression (L1 정규화)
- **목표**: 비용 함수(RSS)에 **L1 페널티 항**을 추가하여 최소화합니다.
- **수식**: `RSS + λ * Σ|β_j|`
- **특징**:
    - **Least Absolute Shrinkable Selector Operator**의 약자입니다.
    - **λ가 충분히 커지면**: 중요도가 낮은 변수의 회귀 계수를 **완전히 0으로** 만듭니다.
    - 이 특성 덕분에 모델의 해석이 용이해지고, 자동으로 **특성 선택(Feature Selection)**이 이루어지는 강력한 장점이 있습니다.
    - Ridge에 비해 더 단순하고 해석 가능한 모델을 생성하는 경향이 있습니다.

### 4️⃣ Elastic Net Regression
- **목표**: Ridge와 LASSO의 페널티 항을 결합하여 사용합니다.
- **수식**: `RSS + λ * (αΣ|β_j| + (1-α)/2 * Σβ_j²)`
- **특징**:
    - L1과 L2 페널티를 모두 사용하여 두 기법의 장점을 취합니다.
    - 특히, 서로 상관관계가 높은 변수들이 있을 때 LASSO는 그중 하나만 선택하는 경향이 있지만, Elastic Net은 이들을 그룹으로 묶어 함께 선택하거나 제외할 수 있습니다.

## 오늘의 연습문제 📝
- **문제 1**: Ridge Regression과 LASSO Regression의 가장 큰 차이점은 무엇이며, 이 차이점으로 인해 LASSO가 갖는 특별한 장점은 무엇인지 설명해 보세요.
- **문제 2**: 정규화에서 튜닝 파라미터인 '람다(λ)' 또는 '알파(α)'의 역할은 무엇인가요? 만약 이 값을 매우 크게 설정하면 모델의 회귀 계수들은 어떻게 변할까요?
- **문제 3**: TV, 라디오, 신문 광고비 데이터를 사용하여 판매량을 예측하는 모델을 만든다고 가정해 봅시다. 산점도 분석 결과, '신문' 광고비는 판매량과 거의 상관관계가 없는 것으로 나타났습니다. 이 '신문' 변수를 모델에서 자동으로 제거하고 싶을 때, Ridge와 LASSO 중 어떤 기법을 사용하는 것이 더 적절할까요? 그 이유를 설명해 보세요.
