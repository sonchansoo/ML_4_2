## 🔍 강의 요약
이번 강의에서는 머신러닝 모델의 파라미터를 최적화하는 핵심 알고리즘인 Gradient Descent(경사 하강법)에 대해 학습합니다. 눈을 가리고 산을 내려가는 비유를 통해 Gradient Descent의 직관적인 개념을 이해하고, 비용 함수(Cost Function)를 최소화하기 위해 경사(Gradient)의 반대 방향으로 점진적으로 이동하는 수학적 원리를 배웁니다. 또한, 학습률(Learning Rate)의 중요성과 그에 따른 변화를 시각적으로 확인하고, 알고리즘이 직면하는 문제점들과 이를 해결하기 위한 다양한 변형(Batch, Stochastic, Mini-batch) 및 고급 옵티마이저에 대해 알아봅니다.

## 💡 핵심 포인트
- **Gradient Descent (경사 하강법)**: 비용 함수(Cost Function)의 최솟값을 찾기 위해 파라미터(가중치)를 반복적으로 조정하는 최적화 알고리즘입니다.
- **핵심 원리**: 현재 위치에서 기울기(Gradient)가 가장 가파른 방향의 **반대쪽**으로 조금씩 이동하여 점진적으로 최저점에 도달합니다.
- **학습률 (Learning Rate, α)**: 경사를 따라 이동하는 보폭(step)의 크기를 결정하는 중요한 하이퍼파라미터입니다. 너무 크면 발산할 수 있고, 너무 작으면 학습이 느려집니다.
- **목표**: 비용 함수 `J(W)`를 최소화하는 파라미터(가중치) `W`의 집합을 찾는 것입니다.
- **변형**: 계산 효율성을 높이기 위해 전체 데이터셋 대신 일부 데이터만 사용하는 **Stochastic Gradient Descent (SGD)**와 **Mini-batch Gradient Descent** 방식이 널리 사용됩니다.

## 세부 내용 📚
### 1️⃣ Gradient Descent의 개념과 비유
- **비유**: 눈을 가린 채 거친 지형(Cost Function)의 가장 낮은 지점(Global Minimum)으로 내려가는 과정과 같습니다. 현재 위치에서 모든 방향으로 땅을 더듬어 보고(Gradient 계산), 가장 빠르게 내려가는 방향으로 한 걸음씩(Learning Rate) 내딛는 것과 유사합니다.
- **역할**: Linear Regression, Logistic Regression, Neural Networks 등 다양한 머신러닝 모델의 파라미터를 학습시키는 데 사용되는 핵심 최적화 알고리즘입니다.

### 2️⃣ Gradient Descent의 수학적 원리
- **비용 함수 (Cost Function)**: 모델의 예측값(`ŷ`)과 실제값(`y`) 사이의 오차를 나타내는 함수입니다. 예를 들어, 선형 회귀에서는 평균 제곱 오차(Mean Squared Error) `J(θ) = (1/2m) * Σ(hθ(x) - y)²`를 사용합니다.
- **경사 (Gradient)**: 특정 지점에서 비용 함수의 기울기(미분값)를 의미합니다. 경사는 함수 값이 가장 가파르게 증가하는 방향을 나타냅니다.
- **업데이트 규칙**:
    - `θ_j := θ_j - α * (∂/∂θ_j)J(θ)`
    - 현재 파라미터(`θ_j`)에서 **학습률(α)과 경사(Gradient)를 곱한 값**을 빼서 새로운 파라미터로 업데이트합니다.
    - 경사의 반대 방향으로 이동해야 비용 함수가 감소하므로 뺄셈(-)을 사용합니다.
- **학습률 (Learning Rate)**:
    - 너무 작으면 최저점까지 도달하는 데 시간이 오래 걸립니다.
    - 너무 크면 최저점을 지나쳐 버리거나(Overshooting), 값이 발산하여 수렴하지 못할 수 있습니다.
    - 일반적으로 0.1, 0.01, 0.001과 같은 작은 값으로 시작하여 수동으로 조정하거나, Adam, AdaGrad, RMSProp과 같은 자동화된 옵티마이저를 사용합니다.

### 3️⃣ Gradient Descent의 과제 (Challenges)
1.  **계산 시간**: 전체 데이터를 사용하여 경사를 계산하는 방식은 매우 느릴 수 있습니다.
2.  **Local/Global Optima**: 볼록하지 않은(Non-convex) 비용 함수에서는 전역 최저점(Global Minimum)이 아닌 지역 최저점(Local Minimum)에 빠질 위험이 있습니다.
3.  **Saddle Points (안장점)**: 경사가 0이지만 최저점이 아닌 지점에 멈출 수 있습니다.
4.  **Vanishing Gradients**: Sigmoid나 tanh 같은 활성화 함수에서 경사가 0에 가까워져 학습이 거의 불가능해지는 문제입니다.

### 4️⃣ 과제 해결을 위한 솔루션
- **Gradient Descent의 변형**:
    - **Batch Gradient Descent**: 전체 훈련 데이터를 사용하여 한 번의 파라미터 업데이트를 수행합니다. 가장 느리지만 안정적입니다.
    - **Stochastic Gradient Descent (SGD)**: 무작위로 선택된 **하나의** 데이터 샘플을 사용하여 파라미터를 업데이트합니다. 매우 빠르지만 불안정할 수 있습니다.
    - **Mini-batch Gradient Descent**: 전체 데이터와 SGD의 절충안으로, 작은 묶음(mini-batch)의 데이터를 사용하여 업데이트합니다. 계산 효율성과 안정성 면에서 가장 널리 사용됩니다.
- **고급 옵티마이저 (Optimizers for Deep Learning)**:
    - Vanishing Gradient 문제를 해결하기 위해 **ReLU** 활성화 함수를 사용합니다.
    - 학습률을 동적으로 조절하고 Local Minima 문제를 완화하기 위해 **SGD with momentum**, **AdaGrad**, **ADAM** 등의 옵티마이저를 사용합니다.

## 오늘의 연습문제 📝
- **문제 1**: Gradient Descent에서 학습률(Learning Rate)이 너무 클 때와 너무 작을 때 각각 어떤 문제가 발생할 수 있는지 설명해 보세요.
- **문제 2**: Batch, Stochastic, Mini-batch Gradient Descent의 차이점을 '한 번의 파라미터 업데이트에 사용되는 데이터의 양' 관점에서 설명하고, 각각의 장단점을 간략히 서술해 보세요.
- **문제 3**: 아래는 주택 크기(X)에 따른 가격(Y)을 예측하는 단순 선형 회귀 모델의 Gradient Descent 업데이트 과정입니다. 현재 가중치가 α=0.42, β=0.73이고 학습률(r)이 0.01일 때, 다음 스텝의 새로운 α와 β 값을 계산해 보세요. (아래 표의 Sum 값을 사용)
| a | b | X | Y | YP=a+bX | SSE | ∂SSE/∂α | ∂SSE/∂β |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 0.42 | 0.73 | ... | ... | ... | ... | ... | ... |
| **Sum** | | | | | **0.553** | **2.900** | **1.350** |
