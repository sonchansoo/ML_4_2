## 🔍 강의 요약
이번 강의에서는 머신러닝의 대표적인 분류 알고리즘 중 하나인 **Decision Tree(의사결정 트리)**에 대해 심도 있게 다룹니다. Decision Tree의 기본 용어부터 시작하여, 노드를 분기하는 기준(Splitting Criteria)과 최적의 분기점을 찾는 방법, 특히 **Gini Index**와 **Gini Gain**의 개념을 학습합니다. 또한, 분류 문제뿐만 아니라 연속적인 값을 예측하는 **Decision Tree Regression**의 원리를 **표준편차 감소(Standard Deviation Reduction)**를 통해 이해하고, 모델의 과적합(Overfitting)을 방지하고 효율성을 높이기 위한 **Pruning(가지치기)** 기법(Pre-pruning, Post-pruning)까지 학습합니다.

## 💡 핵심 포인트
-   🌳 **Decision Tree**: 데이터를 특정 기준에 따라 반복적으로 분할하여 예측 또는 분류 모델을 만드는 지도 학습 알고리즘입니다.
-   📊 **분류 vs. 회귀**: Decision Tree는 범주형 데이터를 예측하는 **분류(Classification)**와 연속형 수치 데이터를 예측하는 **회귀(Regression)** 문제에 모두 사용될 수 있습니다.
-   📉 **Gini Index**: 분류 트리에서 노드의 불순도(Impurity)를 측정하는 지표로, 값이 낮을수록 순도가 높다는 의미입니다. 최적의 분기점은 **Gini Gain**을 최대화하는 지점을 선택하여 찾습니다.
-   📈 **Standard Deviation Reduction (SDR)**: 회귀 트리에서 분기 기준으로 사용되며, 분기 후 자식 노드들의 표준편차 감소량이 가장 큰 속성을 선택합니다.
-   ✂️ **Pruning (가지치기)**: 모델이 훈련 데이터에 과도하게 최적화되는 과적합(Overfitting)을 방지하고 모델을 단순화하기 위해 불필요한 가지를 제거하는 과정입니다.

## 세부 내용 📚
### 1️⃣ Decision Tree 기본 용어 🔖
Decision Tree를 이해하기 위해 사용되는 다양한 용어들은 서로 혼용될 수 있습니다.
-   **Record (행)**: `sample`, `observation`, `instance`와 동일한 의미로, 데이터의 한 단위를 나타냅니다.
-   **Attribute (속성)**: `feature`, `independent variable`, `predictor variable`과 동일하며, 예측에 사용되는 변수(독립 변수)를 의미합니다.
-   **Target (목표)**: `class`, `dependent variable`과 동일하며, 우리가 예측하고자 하는 목표 변수(종속 변수)를 의미합니다.

### 2️⃣ Decision Tree 분기(Splitting) 조건 🌿
트리의 노드는 특정 기준에 따라 자식 노드로 나뉩니다.
-   **속성 유형(Attribute Types)에 따른 분기**:
    -   **Categorical Attribute (범주형 속성)**: 성별(남/여), 사이즈(S/M/L)처럼 순서가 없거나(Nominal) 있는(Ordinal) 속성.
    -   **Continuous Attribute (연속형 속성)**: 나이, 소득처럼 연속적인 수치 값을 갖는 속성.
-   **분기 방식(Ways to Split)에 따른 분기**:
    -   **2-way split (이진 분기)**: 하나의 노드를 두 개의 자식 노드로 나눕니다. (예: 40세 미만 vs. 40세 이상)
    -   **Multi-way split (다중 분기)**: 하나의 노드를 여러 개의 자식 노드로 나눕니다. (예: 자동차 타입 - Family, Sports, Luxury)

### 3️⃣ 최적의 분기(Best Split) 결정 🎯
-   **Node Impurity (노드 불순도)**: 노드에 여러 클래스가 얼마나 섞여 있는지를 나타내는 척도입니다. Decision Tree는 각 분기 단계에서 불순도를 가장 많이 감소시키는 방향으로 학습합니다.
    -   **Homogeneous/Pure (동질/순수)**: 노드 내의 모든 데이터가 동일한 클래스에 속하는 상태. (불순도 낮음)
    -   **Non-homogeneous (비동질)**: 노드 내에 여러 클래스가 섞여 있는 상태. (불순도 높음)
-   **Gini Index (지니 계수)**:
    -   노드의 불순도를 측정하는 대표적인 지표입니다.
    -   **공식**: `GINI(t) = 1 - Σ [p(j|t)]²`
        -   `p(j|t)`: 노드 t에서 클래스 j에 속할 확률
    -   **특징**:
        -   **최솟값 (0.0)**: 모든 데이터가 하나의 클래스에 속할 때 (가장 순수).
        -   **최댓값 (1 - 1/nc)**: 데이터가 모든 클래스에 균등하게 분포할 때 (가장 불순).
-   **Gini Gain**:
    -   분기 전 부모 노드의 Gini Index에서 분기 후 자식 노드들의 가중 평균 Gini Index를 뺀 값입니다.
    -   **`Gain = Gini(Parent) - Gini(Children)`**
    -   Decision Tree는 **Gini Gain이 최대가 되는 속성과 분기점을 선택**하여 트리를 성장시킵니다.

### 4️⃣ Decision Tree Regression 📈
-   **목표**: 연속적인 수치 값을 예측합니다. (예: 날씨 데이터 기반 테니스 친 시간 예측)
-   **분기 기준**: Gini Index 대신 **Standard Deviation Reduction (SDR, 표준편차 감소)**을 사용합니다.
    -   **SDR(T, X) = S(T) - S(T, X)**
        -   `S(T)`: 목표 변수(Target)의 전체 표준편차
        -   `S(T, X)`: 특정 속성 X로 분기했을 때의 가중 평균 표준편차
    -   **SDR이 가장 큰 속성**을 분기 기준으로 선택합니다.
-   **종료 조건 및 예측값**:
    -   **종료 조건**: 노드의 변동 계수(CV = 표준편차 / 평균)가 특정 임계값(예: 10%)보다 작아지거나, 노드 내 데이터 개수가 특정 임계값(예: 4개) 미만이 되면 분기를 멈춥니다.
    -   **예측값**: 최종적으로 분기가 멈춘 Leaf Node에 속한 데이터들의 **평균값**을 해당 노드의 예측값으로 사용합니다.

### 5️⃣ Decision Tree Pruning (가지치기) ✂️
-   **목적**: 불필요한 가지를 제거하여 모델을 단순화하고, 훈련 데이터에 대한 과적합(Overfitting)을 방지합니다.
-   **Pre-pruning (사전 가지치기)**:
    -   트리를 생성하는 과정에서 특정 조건을 만족하면 미리 성장을 멈추는 방식입니다.
    -   **조건**: 노드의 데이터 개수가 특정 임계값 미만이거나, 분기를 해도 불순도가 개선되지 않을 경우.
    -   **단점**: 너무 일찍 성장을 멈춰 중요한 패턴을 놓칠 위험이 있습니다.
-   **Post-pruning (사후 가지치기)**:
    -   일단 트리를 최대한 성장시킨 후, 불필요한 노드를 제거하는 방식입니다.
    -   **Reduced Error Pruning**: 가장 대표적인 방법으로, **Validation Set(검증 데이터)**을 사용하여 특정 노드를 제거했을 때 모델의 성능이 저하되지 않거나 오히려 향상되면 해당 노드를 제거합니다.

## 오늘의 연습문제 📝
-   **문제 1**: 어떤 노드에 클래스 A 데이터가 2개, 클래스 B 데이터가 8개 있습니다. 이 노드의 Gini Index를 계산해 보세요.
-   **문제 2**: Decision Tree에서 분류(Classification)와 회귀(Regression)의 가장 큰 차이점은 무엇이며, 각각 어떤 기준으로 노드를 분기하는지 설명해 보세요.
-   **문제 3**: Decision Tree에서 Pruning(가지치기)을 하는 주된 이유 두 가지는 무엇인가요?
