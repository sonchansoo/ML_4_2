## 🔍 강의 요약
이번 강의에서는 Decision Tree 알고리즘의 심화 내용을 다룹니다. 연속형 데이터(Continuous Attribute)를 분할하는 효율적인 방법, Gini Index와 Classification Error와 같은 불순도 측정 기준, 그리고 연속적인 값을 예측하는 Decision Tree Regression에 대해 학습합니다. 마지막으로, 모델의 과적합(Overfitting)을 방지하고 일반화 성능을 높이기 위한 Pruning(가지치기) 기법, 특히 Pre-pruning과 Post-pruning의 개념과 방법에 대해 자세히 알아봅니다.

## 💡 핵심 포인트
- **연속형 속성(Continuous Attribute) 분할**: 정렬 후 인접한 값들의 중간 지점을 기준으로 최적의 분할 지점을 효율적으로 찾습니다.
- **Gini Index**: 클래스 불순도를 측정하는 지표로, 값이 낮을수록 순도가 높다는 의미입니다.
- **Decision Tree Regression**: 분류가 아닌 연속적인 숫자 값을 예측하는 모델로, 불순도 대신 **표준편차 감소량(Standard Deviation Reduction, SDR)**을 기준으로 트리를 분할합니다.
- **Pruning (가지치기)**: 과적합을 방지하기 위해 트리의 복잡도를 줄이는 과정으로, **Pre-pruning(사전 가지치기)**과 **Post-pruning(사후 가지치기)** 방법이 있습니다.

## 세부 내용 📚
### 1️⃣ 연속형 속성(Continuous Attribute) 분할 방법
연속형 속성을 분할할 때는 모든 가능한 지점을 테스트하는 **Naïve Method**와 더 효율적인 **Efficient Method**가 있습니다.
- **Naïve Method**: 모든 데이터 포인트를 분할 기준으로 삼아 Gini Index를 계산하는 방식. 계산량이 많아 비효율적입니다.
- **Efficient Method**:
    1.  **정렬 (Sort)**: 해당 속성의 모든 값을 오름차순으로 정렬합니다.
    2.  **분할 후보 선정**: 정렬된 값들 사이의 중간 지점들을 분할 후보로 정합니다.
    3.  **Gini Index 계산**: 각 후보 지점에 대해 Gini Index를 계산합니다.
    4.  **최적 지점 선택**: Gini Index를 가장 낮게 만드는(Gini Gain을 가장 높게 만드는) 지점을 최적의 분할 기준으로 선택합니다.

### 2️⃣ 불순도 측정: Gini Index와 Classification Error
- **Gini Index**:
    - 노드의 불순도를 측정하는 지표로, CART 알고리즘에서 주로 사용됩니다.
    - 계산이 Entropy보다 간단하며(로그 계산 불필요), 더 선호되는 경향이 있습니다.
    - **Gini Gain**: 부모 노드의 Gini Index에서 자식 노드들의 가중 Gini Index 합을 뺀 값으로, 이 값이 클수록 좋은 분할입니다.
- **Classification Error**:
    - `Error = 1 - max(p)` (p는 해당 노드에서 가장 빈번한 클래스의 비율)
    - 계산이 가장 간단하지만, Gini나 Entropy에 비해 변화에 덜 민감하여 분할 기준으로 잘 사용되지 않습니다.

### 3️⃣ Decision Tree Regression
분류(Classification)가 아닌 회귀(Regression) 문제에 Decision Tree를 적용하는 방법입니다.
- **목표**: 범주가 아닌 연속적인 숫자(e.g., 시간, 가격)를 예측합니다.
- **분할 기준**: Gini Index나 Entropy 대신 **표준편차 감소량(Standard Deviation Reduction, SDR)**을 사용합니다.
    - `SDR = S(부모 노드) - Σ(가중치 * S(자식 노드))`
    - SDR이 가장 큰 속성을 분할 기준으로 선택합니다.
- **종료 조건 (Termination Criteria)**:
    1.  **변동 계수(Coefficient of Variation, CV)**: `CV = 표준편차 / 평균`. 이 값이 특정 임계값(e.g., 10%)보다 작아지면 분할을 멈춥니다.
    2.  **데이터 개수**: 노드에 속한 데이터 개수가 특정 임계값(e.g., 4개)보다 적어지면 분할을 멈춥니다.
- **최종 예측값**: 리프 노드(Leaf Node)에 도달했을 때, 해당 노드에 속한 데이터들의 **평균값**을 최종 예측값으로 사용합니다.

### 4️⃣ Decision Tree Pruning (가지치기)
과적합(Overfitting)을 방지하고 모델의 일반화 성능을 높이기 위해 불필요한 가지를 제거하는 과정입니다.
- **Pre-pruning (사전 가지치기)**:
    - 트리를 생성하는 과정에서 특정 조건을 만족하면 미리 성장을 멈추는 방식입니다.
    - 예: 최대 깊이(max_depth) 제한, 리프 노드의 최소 샘플 수 지정 등.
    - 너무 일찍 멈춰 모델이 과소적합(Underfitting)될 위험이 있습니다.
- **Post-pruning (사후 가지치기)**:
    - 일단 트리를 최대한 성장시킨 후, 불필요한 가지를 제거하는 방식입니다.
    - **Reduced Error Pruning**:
        1.  훈련 데이터와는 별개의 **검증 데이터셋(Validation Set)**을 사용합니다.
        2.  가장 아래 리프 노드부터 시작하여, 특정 서브트리를 제거(하나의 리프 노드로 대체)했을 때 검증 데이터셋에 대한 에러가 증가하지 않으면 해당 가지를 제거합니다.
        3.  더 이상 에러가 개선되지 않을 때까지 반복합니다.

## 오늘의 연습문제 📝
- **문제 1**: 아래 데이터셋에서 '소득'을 기준으로 데이터를 분할하려고 합니다. Gini Gain이 가장 높은 분할 지점(e.g., 75K, 110K)은 어디일까요? (단, 분할은 '이하'와 '초과'로 나뉩니다.)
| 환급 여부 | 결혼 상태 | 소득 | 탈세 여부 |
| :--- | :--- | :--- | :--- |
| Yes | Single | 125K | No |
| No | Married | 100K | No |
| No | Single | 70K | No |
| Yes | Married | 120K | No |
| No | Divorced | 95K | Yes |

- **문제 2**: Decision Tree Regression에서 특정 노드의 데이터가 아래와 같을 때, 이 노드를 '온도' 속성으로 분할하는 것이 타당한지 SDR(표준편차 감소량)을 계산하여 판단하세요. (부모 노드의 표준편차는 9.32)
| 날씨 | 온도 | 습도 | 바람 | 플레이 시간(시간) |
| :--- | :--- | :--- | :--- | :--- |
| Rainy | Hot | High | False | 25 |
| Rainy | Hot | High | True | 30 |
| Rainy | Mild | High | False | 35 |
| Rainy | Cool | Normal | False | 38 |
| Rainy | Mild | Normal | True | 48 |

- **문제 3**: Pre-pruning과 Post-pruning의 장단점을 비교하고, 어떤 상황에서 각각의 방법이 더 유용할지 설명해 보세요.
