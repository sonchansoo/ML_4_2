## 🔍 강의 요약
이번 강의에서는 분류 알고리즘 중 하나인 Logistic Regression에 대해 학습합니다. 먼저, 기본이 되는 Linear Regression의 개념을 복습하고, 이를 분류 문제에 적용할 때 발생하는 한계점을 알아봅니다. 그 후, 이 한계를 극복하기 위해 등장한 Logistic Regression의 핵심 원리인 Sigmoid 함수와 Logit 변환(확률 → Odds → Logit)에 대해 심도 있게 다룹니다. 마지막으로, 모델의 계수를 해석하는 방법과 Scikit-learn을 활용한 실제 구현 과정을 살펴봅니다.

## 💡 핵심 포인트
- **Linear Regression vs. Logistic Regression**: Linear Regression은 연속적인 값을 예측하는 반면, Logistic Regression은 이진 분류(0 또는 1) 문제에 사용됩니다.
- **Sigmoid 함수**: Linear Regression의 결과를 0과 1 사이의 확률 값으로 변환해주는 S자 형태의 함수입니다.
- **Logit 변환**: 확률(p)을 Odds(p/(1-p))로, 다시 Odds를 Log-odds(ln(p/(1-p)))로 변환하여 독립 변수와 선형적인 관계를 갖도록 만드는 핵심 과정입니다.
- **계수(β)의 해석**: Logistic Regression 모델의 계수(β)에 지수(exponential)를 취한 값(e^β)은 **Odds Ratio(승산비)**를 의미하며, 독립 변수가 1단위 증가할 때 성공 확률의 Odds가 몇 배 증가하는지를 나타냅니다.

## 세부 내용 📚
### 1️⃣ Linear Regression 복습
- **Simple Linear Regression**: `y = α + β₁x₁`
  - 두 연속 변수 간의 관계를 직선으로 모델링합니다.
  - 계수(β₁)는 x가 1단위 변할 때 y의 평균적인 변화량을 의미합니다.
- **Multiple Linear Regression**: `y = α + β₁x₁ + β₂x₂ + ...`
  - 여러 독립 변수가 하나의 종속 변수에 미치는 영향을 모델링합니다.
- **용어 정리**:
  - **종속 변수 (y)**: Target, Class, Predicted, Dependent, Response, Outcome Variable
  - **독립 변수 (x)**: Predictor, Independent, Explanatory Variable, Feature

### 2️⃣ Logistic Regression의 개념과 필요성
- **정의**: 이진(Binary) 종속 변수(1/0, Yes/No)를 예측하기 위해 데이터에 곡선을 적합시키는 회귀 분석의 한 종류입니다.
- **필요성**: Linear Regression을 이진 분류에 직접 적용하면 예측값이 범위를 벗어날 수 있어 확률로 해석하기 어렵습니다.
- **해결책**: **Sigmoid (Logistic) 함수**를 사용하여 모델의 출력값을 0과 1 사이의 확률로 변환합니다.
  - `Φ(z) = 1 / (1 + e⁻ᶻ)`
  - 보통 0.5를 임계값(Threshold)으로 하여 0.5 이상이면 1(Yes), 0.5 미만이면 0(No)으로 분류합니다.

### 3️⃣ 확률에서 Logit으로의 변환 과정
Logistic Regression은 확률을 직접 모델링하는 대신, 선형성을 확보하기 위해 다음과 같은 변환을 거칩니다.
1.  **Probability (확률)**: 특정 사건이 일어날 가능성 (0 ~ 1).
2.  **Odds (승산)**: 사건이 일어날 확률을 일어나지 않을 확률로 나눈 값. `Odds = p / (1-p)`
3.  **Logit (로짓)**: Odds에 자연로그(ln)를 취한 값. `Logit(p) = ln(p / (1-p))`
    - Logit 값은 선형적인 척도를 가지며, 이 값을 Linear Regression 모델에 적합시킵니다.
    - **`logit(p) = α + βx`**

### 4️⃣ 모델 파라미터(β)의 해석과 학습
- **계수(β)의 해석**:
  - `e^β`는 **Odds Ratio(승산비)**를 의미합니다.
  - 이는 독립 변수(x)가 1단위 증가할 때, 성공(Y=1)의 Odds가 몇 배로 변하는지를 나타냅니다.
  - 예: 쿠폰 할인액(x)에 대한 계수 β가 0.097일 때, Odds Ratio는 e^0.097 ≈ 1.102입니다. 이는 할인액이 1달러 증가할 때마다 쿠폰 사용 Odds가 약 1.102배(10.2%) 증가함을 의미합니다.
- **파라미터 추정 방법**:
  - 모델의 최적 파라미터(α, β)를 찾기 위해 **Gradient Descent**, **Newton's method**, **Maximum-likelihood estimation** 등의 방법이 사용됩니다.
  - **Scikit-learn Solver**: `newton-cg`, `lbfgs`, `liblinear`, `sag`, `saga` 등 다양한 최적화 알고리즘을 제공합니다.

### 5️⃣ Scikit-learn을 이용한 Logistic Regression 구현
Python의 Scikit-learn 라이브러리를 사용하면 간단하게 Logistic Regression 모델을 구현할 수 있습니다.
1.  **데이터 준비**: `fetch_openml` 등으로 데이터를 불러오고, `train_test_split`으로 훈련/테스트 데이터셋을 분리합니다.
2.  **모델 생성**: `from sklearn.linear_model import LogisticRegression`을 통해 모델을 불러오고, `logisticRegr = LogisticRegression()`과 같이 인스턴스를 생성합니다.
3.  **모델 학습**: `logisticRegr.fit(train_img, train_lbl)`을 통해 모델을 학습시킵니다.
4.  **예측 및 평가**: `logisticRegr.predict()`로 예측을 수행하고, `logisticRegr.score()`로 모델의 정확도를 평가합니다.

## 오늘의 연습문제 📝
- **문제 1**: 어떤 학생이 시험에 합격할 확률(p)이 0.8이라고 할 때, 이 학생의 합격에 대한 (1) Odds와 (2) Logit 값을 각각 계산해 보세요.
- **문제 2**: 특정 질병(CD)에 대한 Logistic Regression 모델을 학습시킨 결과, '나이(Age)' 변수에 대한 계수(β)가 0.5535로 나타났습니다. 이 결과를 바탕으로, 나이가 한 살 증가할 때 질병에 걸릴 Odds가 약 몇 배 증가하는지 설명해 보세요.
- **문제 3**: Linear Regression을 이진 분류 문제에 사용하면 안 되는 주된 이유를 설명하고, Logistic Regression이 Sigmoid 함수를 통해 이 문제를 어떻게 해결하는지 간략히 서술해 보세요.
