네, 시험공부에 도움이 되도록 제공해주신 모든 내용을 바탕으로 전체적인 흐름을 시각화하고, 중요한 개념들을 비교하며, 시험에 나올 만한 포인트들을 짚어 상세하게 정리해 드리겠습니다.

### **마인드맵: 머신러닝 모델 및 통계적 검증**

```
머신러닝 모델 및 통계적 검증
├── 1. 결정 트리 (Decision Tree)
│   ├── 목표: 데이터를 잘게 쪼개 예측 모델 생성
│   ├── 분할 기준 결정
│   │   ├── 불순도(Impurity) 측정: 낮을수록 좋음
│   │   │   ├── 지니 지수 (Gini Index)
│   │   │   └── 엔트로피 (Entropy) -> 정보 이득 (Information Gain)
│   │   └── 연속형 변수 처리: 최적의 분할 지점 찾기
│   ├── 회귀 트리 (Regression Tree)
│   │   └── 분할 기준: 표준편차 감소 (SDR)
│   └── 과적합 방지: 가지치기 (Pruning)
│       ├── 사전 가지치기 (Pre-pruning): 미리 멈춤
│       └── 사후 가지치기 (Post-pruning): 다 만든 후 잘라냄
│
├── 2. 회귀 모델 (Regression Models)
│   ├── 선형 회귀 (Linear Regression): 연속된 값 예측
│   └── 로지스틱 회귀 (Logistic Regression): 이진 분류 (Yes/No)
│       ├── 핵심 개념: 확률 -> 오즈(Odds) -> 로짓(Logit) 변환
│       └── 목표: 로짓(Logit) 값을 선형적으로 예측
│
├── 3. 서포트 벡터 머신 (SVM)
│   ├── 목표: 두 클래스 간의 마진(Margin)을 최대화하는 경계선 찾기
│   ├── 주요 개념
│   │   ├── 결정 경계 (Decision Boundary)
│   │   ├── 서포트 벡터 (Support Vectors): 경계선에 가장 가까운 데이터
│   │   └── 마진 (Margin): 두 서포트 벡터 사이의 거리
│   ├── 마진 종류
│   │   ├── 하드 마진 (Hard Margin): 오류를 허용하지 않음
│   │   └── 소프트 마진 (Soft Margin): 일부 오류 허용 (C 파라미터로 조절)
│   └── 비선형 데이터 처리: 커널 트릭 (Kernel Trick)
│       └── 데이터를 고차원으로 보내 선형 분리가 가능하게 만듦
│
├── 4. 모델 최적화 및 일반화 (Optimization & Generalization)
│   ├── 경사 하강법 (Gradient Descent)
│   │   ├── 목표: 비용 함수(Cost Function)를 최소화하는 파라미터 찾기
│   │   ├── 핵심 요소: 학습률 (Learning Rate)
│   │   └── 종류: 배치, 확률적(Stochastic), 미니배치
│   └── 정규화 (Regularization): 과적합 방지 기법
│       ├── 목표: 회귀 계수(β)를 0에 가깝게 만들어 모델을 단순화
│       ├── 릿지 (Ridge, L2): 계수를 0에 가깝게 만들지만 0으로 만들진 않음
│       ├── 라쏘 (LASSO, L1): 일부 계수를 완전히 0으로 만들어 변수 선택 효과
│       └── 엘라스틱 넷 (Elastic Net): 릿지와 라쏘를 결합
│
└── 5. 추론 통계 및 가설 검정 (Inferential Statistics)
    ├── 목표: 표본(Sample)을 통해 모집단(Population)의 특성을 추론
    ├── 가설 검정 5단계
    │   ├── 1. 가설 설정 (귀무가설 H₀, 대립가설 H₁)
    │   ├── 2. 유의 수준(α) 설정 (보통 0.05)
    │   ├── 3. 통계량 계산
    │   ├── 4. 임계값 찾기
    │   └── 5. 가설 기각/채택 결정
    └── 주요 통계 검정
        ├── 카이제곱 검정 (Chi-Square Test): 두 '범주형' 변수 간의 독립성/관련성 검정
        └── t-검정 (t-Test): 두 그룹 간 '평균'의 차이가 유의미한지 검정
```

---

### **핵심 내용 상세 정리 및 비교**

#### **1. 결정 트리: 어떻게 나눌 것인가?**

*   **핵심 아이디어**: 질문을 통해 데이터를 계속 나눠 순수한(homogeneous) 그룹을 만드는 것.
*   **불순도 측정 (분류 트리)**
    *   **지니 지수(Gini Index)**: `1 - Σ(클래스별 비율)²`. 계산이 간단하고 빠름. CART 알고리즘에서 사용.
    *   **엔트로피(Entropy)**: `-Σ(클래스별 비율) * log₂(클래스별 비율)`. 불확실성을 측정. 엔트로피를 줄이는 방향으로 **정보 이득(Information Gain)**을 계산하여 분할.
    *   **비교**: 둘 다 비슷한 결과를 내지만, 지니 지수가 계산적으로 조금 더 효율적이라 기본값으로 많이 사용됩니다. 엔트로피가 지니보다 조금 더 균형 잡힌 트리를 만드는 경향이 있습니다.
*   **표준편차 감소 (회귀 트리)**
    *   목표 변수가 연속형일 때 사용. 분할 전 그룹의 표준편차와 분할 후 자식 그룹들의 가중 표준편차 합의 차이를 계산. 이 **SDR(Standard Deviation Reduction)**이 가장 큰 속성으로 분할.
*   **가지치기 (Pruning)**
    *   **사전 가지치기**: 트리가 다 자라기 전에 특정 조건(예: 노드의 데이터 수, 깊이)에 도달하면 성장을 멈춤. 너무 일찍 멈출 위험이 있음.
    *   **사후 가지치기**: 트리를 끝까지 만든 후, 검증 데이터셋을 사용해 성능 향상이 없는 가지를 잘라냄. 더 정교하지만 계산 비용이 큼.

#### **2. SVM vs. 로지스틱 회귀**

두 모델 모두 이진 분류에 사용될 수 있지만, 접근 방식이 다릅니다.

| 구분 | **서포트 벡터 머신 (SVM)** | **로지스틱 회귀 (Logistic Regression)** |
| :--- | :--- | :--- |
| **목표** | 두 클래스 간의 **마진(거리)을 최대화**하는 결정 경계를 찾음 | 데이터가 특정 클래스에 속할 **확률**을 모델링 |
| **결정 경계** | 마진의 경계에 있는 **서포트 벡터**에 의해서만 결정됨 | **모든 데이터 포인트**가 결정 경계에 영향을 줌 |
| **데이터 특성** | 고차원 데이터, 특징(feature)이 샘플 수보다 많을 때 효과적 | 데이터가 선형적으로 구분될 때 잘 작동하며, 확률적 해석이 중요할 때 유용 |
| **핵심 파라미터** | **C (비용)**: 마진 폭과 오류 허용 수준 조절<br>**Kernel**: 비선형 데이터를 다루기 위한 함수 | **회귀 계수 (β)**: 각 변수의 영향력을 나타냄 |

#### **3. 정규화: 릿지(L2) vs. 라쏘(L1)**

과적합을 막기 위해 비용 함수에 페널티를 추가하는 기법입니다.

| 구분 | **릿지 회귀 (Ridge, L2 Penalty)** | **라쏘 회귀 (LASSO, L1 Penalty)** |
| :--- | :--- | :--- |
| **페널티 항** | `λ * Σ(β²) ` (계수 제곱의 합) | `λ * Σ|β|` (계수 절대값의 합) |
| **계수 축소** | 계수들을 0에 **가깝게** 만들지만, 완전히 0으로 만들지는 않음 | 중요하지 않은 변수의 계수를 **완전히 0으로** 만듦 |
| **주요 특징** | 상관관계가 높은 변수들을 함께 유지하는 경향이 있음 | **변수 선택(Feature Selection)** 효과가 있음. 모델이 단순해지고 해석이 용이해짐 |
| **단점** | 불필요한 변수들이 모델에 계속 남아있어 해석이 복잡할 수 있음 | 상관관계 높은 변수 그룹 중 하나만 선택하고 나머지는 버려 정보 손실이 발생할 수 있음 |

*   **엘라스틱 넷(Elastic Net)**: 릿지와 라쏘를 섞은 형태로, 두 기법의 장점을 모두 취하려 할 때 사용됩니다.

#### **4. 경사 하강법의 3가지 변형**

비용 함수를 최소화하기 위해 파라미터를 업데이트하는 방식의 차이입니다.

| 구분 | **배치 경사 하강법** | **확률적 경사 하강법 (SGD)** | **미니배치 경사 하강법** |
| :--- | :--- | :--- | :--- |
| **업데이트 당 데이터** | **전체** 데이터셋 | **1개**의 데이터 샘플 | **소규모 배치(batch)** |
| **속도** | 매우 느림 | 매우 빠름 | 빠름 |
| **안정성** | 안정적으로 최솟값으로 수렴 | 불안정하고 변동이 심함 | 안정성과 속도의 절충안 |
| **메모리** | 많이 필요 | 적게 필요 | 적당히 필요 |
| **특징** | 가장 정확한 기울기 계산 | 노이즈가 심하지만 지역 최솟값(local minima)을 탈출할 수 있음 | **가장 널리 사용되는 방식** |

#### **5. 통계 검정: 카이제곱 vs. t-검정**

| 구분 | **카이제곱 검정 (Chi-Square Test)** | **t-검정 (t-Test)** |
| :--- | :--- | :--- |
| **목표** | 두 **범주형** 변수 간에 연관성이 있는지 (독립적인지) 검정 | 두 그룹 간 **평균**에 통계적으로 유의미한 차이가 있는지 검정 |
| **데이터 예시** | 성별(남/여)과 선호 휴가지(바다/산) | 신약 투여 그룹과 위약 투여 그룹의 평균 수명 |
| **핵심 통계량** | 카이제곱(χ²) 값 | t-점수 (t-score) |
| **가설** | H₀: 두 변수는 독립이다. | H₀: 두 그룹의 평균은 같다. |

---

### **헷갈릴 수 있는 내용 및 시험 출제 예상 포인트**

1.  **헷갈리기 쉬운 개념**
    *   **독립표본 t-검정 vs. 대응표본 t-검정**: **독립표본**은 서로 다른 두 집단(예: 남학생 그룹 vs. 여학생 그룹)의 평균을 비교하는 것이고, **대응표본**은 동일한 집단을 대상으로 사전/사후(예: 약물 투여 전/후)의 평균 변화를 비교하는 것입니다. 데이터 수집 방식에 따라 결정됩니다.
    *   **SVM의 C와 Gamma 하이퍼파라미터**:
        *   **C**: 오류 허용치. **C가 크면** 오류를 적게 허용하므로 마진이 좁아지고 과적합 위험이 커집니다. **C가 작으면** 오류를 많이 허용하여 마진이 넓어지고 과소적합 위험이 있습니다.
        *   **Gamma (RBF 커널)**: 하나의 데이터가 미치는 영향의 범위. **Gamma가 크면** 영향이 작고 결정 경계가 복잡해져 과적합 위험이 커집니다.
    *   **p-값(p-value)의 해석**: p-값은 **'귀무가설(H₀)이 사실일 때, 현재와 같은 결과가 우연히 관찰될 확률'**입니다. p-값이 유의수준(보통 0.05)보다 작으면 '우연히 일어났다고 보기에는 너무 드문 일이니, 귀무가설이 틀렸을 것이다'라고 결론 내리고 귀무가설을 기각합니다. (대립가설을 증명하는 것이 아님에 주의!)

2.  **계산이 단순하여 문제로 내기 쉬운 내용**
    *   **지니 지수 계산**: 간단한 2x2 데이터 테이블을 주고 특정 노드의 지니 지수를 계산하라는 문제가 나올 수 있습니다. 공식 `1 - Σ(p²) `만 알면 쉽게 풀 수 있습니다.
        *   예: 클래스 A가 4개, 클래스 B가 6개인 노드의 지니 지수 = `1 - ( (4/10)² + (6/10)² ) = 1 - (0.16 + 0.36) = 0.48`
    *   **카이제곱 검정의 기대 빈도 계산**: 분할표를 주고 특정 셀의 기대 빈도를 계산하는 문제입니다.
        *   공식: `(해당 셀의 행 합계 * 해당 셀의 열 합계) / 전체 데이터 수`
    *   **자유도(Degrees of Freedom) 계산**:
        *   **카이제곱 검정**: `(행의 수 - 1) * (열의 수 - 1)`
        *   **t-검정**: 일표본/대응표본은 `N-1`, 독립표본은 `N₁ + N₂ - 2`
    *   **로지스틱 회귀의 확률, 오즈, 로짓 변환**: 확률(p)이 주어졌을 때 오즈(`p / (1-p)`)와 로짓(`ln(오즈)`)을 계산하는 간단한 문제입니다.

이 자료가 시험을 준비하는 데 큰 도움이 되기를 바랍니다. 각 개념의 핵심 아이디어와 서로 다른 모델들의 장단점을 비교하며 공부하시면 좋은 결과를 얻으실 수 있을 겁니다.
