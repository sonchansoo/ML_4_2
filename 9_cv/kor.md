다음은 PDF의 모든 페이지에 대한 전체 한국어 번역입니다. 고유명사는 원문 그대로 유지했습니다.

***

### 1페이지
**컴퓨터 비전의 기초**
2025년 2학기
- Kwang-Hyun Uhm –
인공지능학과

***

### 2페이지
**강의 1:**
**컴퓨터 비전 소개**
*   컴퓨터 비전이란 무엇인가?
    *   컴퓨터 비전 적용 사례
    *   컴퓨터 비전 작업 개요
*   왜 중요한가?
*   어떻게 공부하는가?

***

### 3페이지
**컴퓨터 비전이란 무엇인가?**
*   **비전(Vision)**
    *   인간에게 비전이란 눈으로 세상을 보고 뇌로 해석하는 것을 의미합니다.
    *   이는 단순히 빛을 감지하는 것뿐만 아니라, 객체를 인식하고, 거리를 추정하며, 움직임을 추적하는 것에 관한 것입니다.
    *   예시: 우리가 사과를 볼 때, 단지 색과 모양만 보는 것이 아니라 "이것은 사과다"라고 이해합니다.

***

### 4페이지
**컴퓨터 비전이란 무엇인가?**
*   **컴퓨터(Computer)**
    *   컴퓨터는 논리와 수치 연산을 사용하여 정보를 처리하는 기계입니다.
    *   핵심적으로, 컴퓨터는 0과 1만을 이해합니다.
    *   하지만 카메라의 이미지는 픽셀(밝기와 색을 나타내는 숫자)로 변환될 수 있으며, 이는 컴퓨터가 처리할 수 있습니다.

***

### 5페이지
**컴퓨터 비전이란 무엇인가?**
*   **컴퓨터 비전(Computer Vision)**
    *   컴퓨터 비전은 컴퓨터가 세상을 "보게" 만드는 분야입니다.
    *   다른 말로, 컴퓨터가 이미지와 비디오를 해석하고 이해할 수 있도록 하는 것입니다.
    *   적용 사례:
        *   자율주행차가 보행자와 신호등을 감지하는 것
        *   AI 모델이 의료 영상에서 종양을 식별하는 것
        *   스마트폰이 얼굴 인식으로 잠금을 해제하는 것

***

### 6페이지
**CV 적용 사례 (일상생활)**
*   **자율주행차의 보행자 및 신호등 감지**
    *   자율주행차는 안전한 주행을 위해 도로 위의 보행자를 인식하고 신호등 상태를 실시간으로 감지해야 합니다.
    *   예를 들어, 사람이 건너거나 신호등이 빨간불로 바뀌면 차는 멈춰야 합니다.
    *   이것은 기계가 카메라 이미지를 해석하여 주행 환경을 이해하고 대응하는 컴퓨터 비전의 실제 사례입니다.

***

### 7페이지
**CV 적용 사례 (일상생활)**
*   **스마트폰 잠금 해제를 위한 얼굴 인식**
    *   최신 스마트폰은 얼굴 인식을 사용하여 빠르고 안전하게 기기를 잠금 해제합니다.
    *   카메라는 사용자 얼굴의 이미지를 캡처하여 저장된 프로필과 비교합니다.
    *   이것은 기계가 얼굴 특징을 분석하여 신원을 확인하는 컴퓨터 비전의 일상적인 예입니다.

***

### 8페이지
**CV 적용 사례 (일상생활)**
*   **Instagram 및 TikTok의 AR 필터**
    *   소셜 미디어 앱은 얼굴을 추적하고 실시간으로 효과를 덧씌우는 증강 현실(AR) 필터를 적용합니다.
    *   예시로는 가상 선글라스 추가, 머리 색 변경, 또는 얼굴을 만화 캐릭터로 바꾸는 것 등이 있습니다.
    *   이것은 컴퓨터 비전이 일상적인 비디오에 상호작용적이고 재미있는 효과를 직접 가져오는 방식입니다.

***

### 9페이지
**CV 적용 사례 (산업)**
*   **제조 – 자동 결함 감지**
    *   공장에서는 카메라가 조립 라인 위의 제품을 검사하여 긁힘, 균열 또는 누락된 부품을 감지합니다.
    *   비전 시스템은 인간 검사원보다 더 빠르고 일관성 있게 이 품질 검사를 수행할 수 있습니다.
    *   이 응용 프로그램은 컴퓨터 비전이 산업 생산의 효율성과 신뢰성을 어떻게 향상시키는지를 보여줍니다.

***

### 10페이지
**CV 적용 사례 (산업)**
*   **소매 – 매장 내 자동 결제 (예: Amazon Go)**
    *   계산원이 없는 매장에서는 카메라가 고객이 어떤 상품을 집거나 다시 선반에 놓는지 추적합니다.
    *   고객이 매장을 떠날 때, 시스템은 계산대 없이 자동으로 청구서를 계산하고 계정에 요금을 청구합니다.
    *   이 응용 프로그램은 컴퓨터 비전이 현대 소매업에서 어떻게 원활한 쇼핑 경험을 가능하게 하는지를 강조합니다.

***

### 11페이지
**CV 적용 사례 (산업)**
*   **보안 – 자동 CCTV 모니터링**
    *   현대 CCTV 시스템은 비전 알고리즘을 사용하여 무단 침입, 싸움 또는 버려진 물체와 같은 비정상적인 이벤트를 감지합니다.
    *   인간 경비원에게만 의존하는 대신, 카메라는 의심스러운 활동이 인식될 때 실시간 경고를 발생시킬 수 있습니다.
    *   이 응용 프로그램은 컴퓨터 비전이 공공 및 사설 공간의 안전과 감시 효율성을 어떻게 향상시키는지를 보여줍니다.

***

### 12페이지
**CV 응용 사례 (의료)**
*   **의료 영상 – CT/MRI에서의 종양 탐지**
    *   병원에서 의사들은 종양과 같은 이상을 찾기 위해 CT 또는 MRI 스캔을 검사합니다.
    *   컴퓨터 비전은 암을 나타낼 수 있는 의심스러운 영역을 자동으로 강조 표시하여 도움을 줄 수 있습니다.
    *   예를 들어:
        *   폐 CT에서 알고리즘은 사람의 눈으로 보기 힘든 작은 폐 결절을 감지할 수 있습니다.
        *   뇌 MRI에서 비전 시스템은 종양의 정확한 경계를 분할하여 수술 계획을 지원할 수 있습니다.
    *   이러한 컴퓨터 비전의 사용은 조기 진단, 치료 계획 및 의사 효율성을 개선하여 궁극적으로 더 나은 환자 결과를 지원합니다.

***

### 13페이지
**CV 응용 사례 (의료)**
*   **병리학 – 현미경 이미지에서 암세포 식별**
    *   병리 실험실에서는 암과 같은 질병을 진단하기 위해 조직 샘플을 현미경으로 검사합니다.
    *   컴퓨터 비전은 이러한 디지털 슬라이드를 분석하여 암세포를 식별하거나 비정상적인 세포 패턴을 계산할 수 있습니다.
    *   이는 진단 속도를 높이고 인적 오류를 줄임으로써 병리학자를 지원합니다.

***

### 14페이지
**CV 응용 사례 (의료)**
*   **수술 보조 – 3D 시뮬레이션 및 로봇 수술 지원**
    *   외과의는 CT 또는 MRI 스캔으로 만든 3D 재구성을 사용하여 수술실에 들어가기 전에 수술을 시뮬레이션할 수 있습니다.
    *   수술 중에는 비전이 장착된 로봇 시스템이 정밀한 안내와 확대된 수술 시야를 제공하여 보조할 수 있습니다.
    *   이러한 컴퓨터 비전 응용 프로그램은 복잡한 의료 절차에서 정확성, 안전성 및 계획을 개선하는 데 도움이 됩니다.

***

### 15페이지
**최신 동향**
*   **시각-언어-행동 모델 (VLAs)**
    *   VLA는 컴퓨터 비전, 자연어 이해, 행동 제어를 단일 시스템에 결합합니다.
    *   카메라의 시각적 입력과 자연어로 된 인간의 지시를 함께 해석하여 실제 세계의 행동을 수행합니다.
    *   예시: "빨간 블록을 집어서 테이블 위에 놓아줘"라고 말하면, 로봇은 비전을 사용하여 블록을 찾고, 언어를 사용하여 명령을 이해하며, 행동을 실행합니다.

***

### 16페이지
**최신 동향**
*   **AI 생성 비디오 – Veo 3**
    *   컴퓨터 비전의 새로운 트렌드는 텍스트나 이미지로부터 비디오를 생성하는 것입니다.
    *   Google의 Veo 3는 영화, 광고 또는 창의적인 미디어에 적합한 현실적이고 역동적인 짧은 비디오를 만들 수 있습니다.
    *   이는 컴퓨터 비전이 생성형 AI 시대에 분석에서 창작으로 어떻게 확장되었는지를 나타냅니다.

***

### 17페이지
**최신 동향**
*   **Genie 3 (Google DeepMind) - https://www.youtube.com/watch?v=PDKhUknuQDg**
    *   Genie 3는 단일 텍스트 프롬프트로부터 초당 24프레임, 720p 해상도의 상호작용 가능한 3D 가상 세계를 생성할 수 있습니다. 사용자는 이러한 동적 환경 내에서 탐색하고 상호작용할 수 있습니다.
    *   이는 상호작용 가능하고 탐색 가능한 세계를 생성하고 시뮬레이션하는 데 있어 최근의 발전을 보여줍니다.

***

### 18페이지
**최신 동향**
*   **Genie 3 (Google DeepMind) - https://www.youtube.com/watch?v=PDKhUknuQDg**
    *   Genie 3는 단일 텍스트 프롬프트로부터 초당 24프레임, 720p 해상도의 상호작용 가능한 3D 가상 세계를 생성할 수 있습니다. 사용자는 이러한 동적 환경 내에서 탐색하고 상호작용할 수 있습니다.
    *   이는 상호작용 가능하고 탐색 가능한 세계를 생성하고 시뮬레이션하는 데 있어 최근의 발전을 보여줍니다.

***

### 19페이지
**최신 동향**
*   **3D 및 디지털 트윈**
    *   카메라와 센서를 사용하여 컴퓨터 비전은 실제 객체와 환경을 3D로 재구성하고 디지털 트윈을 만들 수 있습니다.
    *   디지털 트윈은 물리적 시스템의 가상 복제품으로, 두 번째 물리적 복사본이 아니며 시뮬레이션, 예측 및 최적화를 가능하게 합니다.
    *   예시로는 안전 검사를 위한 건물의 3D 모델 생성 또는 고장이 발생하기 전에 예측하기 위한 공장 장비 모델링이 있습니다.
    *   이는 컴퓨터 비전이 인식을 넘어 가상 공간에서 실제 세계를 시뮬레이션하고 실험하는 단계로 나아가는 것을 보여줍니다.

***

### 20페이지
**컴퓨터 비전 작업 개요**

***

### 21페이지
**이미지 분류**
*   **이미지 분류란 무엇인가?**
    *   이미지 분류는 입력 이미지에 단일 클래스 레이블을 할당하는 작업입니다.
    *   시스템은 전체 이미지를 보고 "이 이미지는 무엇인가?"를 결정합니다.
*   **예시**
    *   고양이 대 개 분류: 모델은 이미지가 고양이를 포함하면 "고양이"를, 개를 포함하면 "개"를 출력합니다.
    *   의료 영상: 흉부 X-레이가 주어지면 "정상" 또는 "폐렴"으로 분류합니다.
    *   산업용: 결함이 있는 제품과 없는 제품을 분류합니다.

***

### 22페이지
**분류에서 지역화로**
*   **동기**
    *   분류는 단지 "이 이미지는 고양이를 포함한다"고 말합니다.
    *   하지만 이미지의 어디에 고양이가 있는지는 알려주지 않습니다.
*   **지역화의 개념**
    *   지역화는 클래스 레이블을 예측할 뿐만 아니라 객체의 위치도 예측하여 분류를 확장합니다.
    *   일반적으로 객체 주위의 경계 상자(bounding box)로 표현됩니다.
*   **예시**
    *   지역화 → "이 좌표(x, y, 너비, 높이)에 있는 고양이"

***

### 23페이지
**단일 레이블 대 다중 레이블 분류**
*   **단일 레이블 분류**
    *   각 이미지는 오직 하나의 레이블만 가집니다.
    *   모델은 가장 대표적인 클래스를 선택합니다.
*   **다중 레이블 분류**
    *   하나의 이미지가 동시에 여러 레이블을 가질 수 있습니다.
    *   모델은 존재하는 모든 클래스를 예측합니다.
    *   예시:
        *   고양이와 개가 모두 있는 사진 → 출력은 ["고양이", "개"]입니다.
        *   의료 영상에는 "폐렴"과 "삼출"이 모두 있을 수 있습니다.

***

### 24페이지
**객체 탐지**
*   **개념**
    *   객체 탐지는 이미지에 어떤 객체가 있고 어디에 위치하는지를 찾는 작업입니다.
    *   모델은 클래스 레이블과 경계 상자(객체 주위 사각형의 좌표)를 모두 출력합니다.
*   **예시**
    *   고양이와 개가 있는 사진:
        *   고양이 → 고양이 주위의 상자 / 개 → 개 주위의 상자
    *   자율 주행:
        *   자동차, 보행자, 신호등을 각각 경계 상자로 탐지합니다.
    *   보안 카메라:
        *   건물에 들어오거나 나가는 사람들을 탐지합니다.

***

### 25페이지
**이미지 분할 (의미론적 분할)**
*   **개념**
    *   이미지 분할은 이미지를 픽셀 수준에서 의미 있는 영역으로 나누는 작업입니다.
    *   각 픽셀은 클래스(예: 고양이, 개, 배경)에 할당됩니다.
    *   경계 상자를 사용하는 탐지와 달리, 분할은 객체의 세밀한 윤곽을 제공합니다.
*   **예시**
    *   고양이와 개가 있는 사진:
        *   고양이의 모든 픽셀은 "고양이"로 레이블링되고, 개의 모든 픽셀은 "개"로 레이블링됩니다. 나머지 픽셀은 "배경"입니다.
    *   자율 주행:
        *   픽셀이 도로, 보행자, 자동차, 건물, 하늘로 레이블링됩니다.
    *   위성 이미지:
        *   픽셀이 숲, 물, 도시 지역, 농지로 분류됩니다.
*   하지만 같은 이미지에 여러 마리의 고양이나 많은 사람들이 있다면 어떻게 분리할 수 있을까요?

***

### 26페이지
**인스턴스 분할**
*   **개념**
    *   인스턴스 분할은 의미론적 분할에서 한 단계 더 나아갑니다.
    *   각 픽셀에 클래스 레이블을 할당할 뿐만 아니라, 동일한 클래스의 다른 인스턴스들을 구별합니다.
    *   다른 말로: 그것이 무엇이고, 어디에 있으며, 어느 것인가?
*   **예시**
    *   여러 사람이 있는 거리 장면:
        *   의미론적 분할 → 모든 사람이 "사람"으로 레이블링됩니다.
        *   인스턴스 분할 → 사람 #1, 사람 #2, 사람 #3, 각각 명확하게 분리됩니다.

***

### 27페이지
**요약**
*   분류 → 다중 레이블 → 탐지 → 분할 → 인스턴스 분할

| 작업 | 답변된 질문 | 출력 | 예시 |
| :--- | :--- | :--- | :--- |
| **이미지 분류** | 이미지에 무엇이 있는가? | 전체 이미지에 대한 하나의 클래스 레이블 | 사진 → "고양이" |
| **다중 레이블 분류** | 이미지에 어떤 클래스들이 있는가? | 다중 레이블 | 사진 → "고양이, 개" |
| **객체 탐지** | 객체는 무엇이고 어디에 있는가? | 클래스 + 경계 상자 | "고양이" + 상자, "개" + 상자 |
| **의미론적 분할** | 각 클래스에 속하는 픽셀은? | 픽셀 단위 클래스 맵 | 모든 고양이 픽셀 = "고양이", 모든 개 픽셀 = "개" |
| **인스턴스 분할** | 각 개별 객체에 속하는 픽셀은? | 픽셀 단위 클래스 맵 + 인스턴스 ID | 고양이 #1 마스크, 고양이 #2 마스크, 사람 #1 마스크, 사람 #2 마스크… |

***

### 28페이지
**자세 추정**
*   **개념**
    *   객체(주로 인간)의 키포인트(예: 관절, 랜드마크)를 예측합니다.
    *   연결된 키포인트의 골격으로 객체의 자세나 구조를 나타냅니다.
*   **예시**
    *   스포츠 분석: 선수들의 움직임(예: 축구, 농구)을 추적합니다.
    *   AR/VR: 현실적인 상호작용을 위해 사용자의 신체 자세를 추정합니다.
    *   피트니스 앱: 사용자가 올바른 자세로 운동하는지 확인합니다.

***

### 29페이지
**비디오 이해**
*   **개념**
    *   비디오 이해는 단일 이미지가 아닌, 시간의 흐름에 따른 이미지 시퀀스를 분석하는 것입니다.
    *   목표는 비디오에서 일어나는 행동, 이벤트 또는 상호작용을 이해하는 것입니다.
*   **예시**
    *   **행동 인식**: 사람이 무엇을 하고 있는지(예: 달리기, 점프, 손 흔들기) 식별합니다.
    *   **객체 추적**: 여러 프레임에 걸쳐 동일한 객체를 따라갑니다.
    *   **감시**: CCTV 영상에서 비정상적인 행동을 감지합니다.
    *   **스포츠 분석**: 선수 움직임을 추적하고 플레이를 인식합니다.

***

### 30페이지
**이미지 캡셔닝**
*   **개념**
    *   이미지 캡셔닝은 이미지에 대한 자연어 설명을 생성하는 작업입니다.
    *   컴퓨터 비전(이미지 내용 이해)과 자연어 처리(말로 표현)를 결합합니다.
*   **예시**
    *   입력: 소파에 앉아 있는 고양이 사진. 출력: "고양이가 소파에 누워 있습니다."
    *   의료 영상: 흉부 X-레이나 CT 스캔으로부터 방사선 보고서(소견 + 인상)를 자동으로 생성합니다.
    *   스포츠 비디오: 선수들의 행동으로부터 경기 요약을 생성합니다.

***

### 31페이지
**시각적 질의응답 (VQA)**
*   **개념**
    *   VQA는 이미지에 대한 자연어 질문에 답하는 작업입니다.
    *   비전(이미지 이해)과 언어(질문 이해 및 답변 생성)를 결합합니다.
    *   시스템은 두 양식(modality)을 함께 추론해야 합니다.
*   **통찰**
    *   수동적 이해(캡셔닝/보고)에서 능동적 추론(임의의 질문에 답변)으로의 전환을 보여줍니다.
    *   모델이 시각적 콘텐츠를 언어로 표현된 특정 인간의 의도와 연결하도록 강제합니다.
    *   현재의 대규모 시각-언어 모델(예: BLIP-2, GPT-4V, LLaVA)은 VQA를 매우 효과적으로 만들고 있습니다.

***

### 32페이지
**이미지 생성**
*   **개념**
    *   이미지 생성은 주어진 입력(노이즈, 텍스트 또는 다른 이미지)을 바탕으로 사실적으로 보이는 새로운 이미지를 만드는 작업입니다.
    *   이미 있는 것을 인식하는 분류나 탐지와 달리, 생성은 창의성과 합성에 중점을 둡니다.
*   **예시 / 모델**
    *   **텍스트-이미지 변환**: 설명과 일치하는 생성된 이미지를 출력합니다.
    *   **이미지-이미지 변환**: 사진을 Studio Ghibli 스타일로 변환합니다.
    *   **GANs (생성적 적대 신경망)** → 사실적인 이미지 합성.
    *   **확산 모델 (Stable Diffusion, DALL·E, Imagen, MidJourney)** → 현재 최첨단 기술.

***

### 33페이지
**컴퓨터 비전의 역사**

***

### 34페이지
**컴퓨터 비전의 역사**
*   **컴퓨터 비전 역사의 간결한 타임라인, AI에서의 기원부터 딥러닝 혁명까지의 주요 이정표를 강조:**
    *   **1950년대–1960년대**: 인공지능(1956) 개념과 MIT에서의 첫 컴퓨터 비전 프로젝트(1960년대 여름 프로젝트)가 이 분야의 탄생을 알립니다. 동시에, Hubel과 Wiesel의 신경생리학 연구(1959)는 시각 정보가 뇌에서 계층적으로 처리된다는 것을 밝혀내어 후대의 컴퓨터 비전 모델에 큰 영향을 미쳤습니다.
    *   **1970년대**: 소위 AI 겨울 동안 발전이 둔화되었습니다. 높은 기대가 제한된 계산 능력과 데이터에 부딪혔기 때문입니다.
    *   **1990년대**: 초보적인 신경망에 대한 연구가 재개되어 이후의 돌파구를 위한 기초를 마련했습니다.

***

### 35페이지
**컴퓨터 비전의 역사**
*   **컴퓨터 비전 역사의 간결한 타임라인, AI에서의 기원부터 딥러닝 혁명까지의 주요 이정표를 강조:**
    *   **2001년**: Viola & Jones의 얼굴 탐지는 실용적인 이정표가 되었으며, 널리 배포된 최초의 실제 컴퓨터 비전 기술 중 하나가 되었습니다(예: 디지털 카메라).
    *   **2012년**: AlexNet (Krizhevsky et al.)으로 분수령을 맞이했습니다. 이 모델은 ImageNet 대회에서 압도적인 성과를 거두며 딥러닝 시대를 열었습니다.
    *   **2012년 이후**: 연속적인 아키텍처(VGG, GoogLeNet, ResNet, SENet 등)가 지속적으로 정확도와 효율성을 개선하며 컴퓨터 비전을 광범위하게 사용하도록 이끌었습니다.

***

### 36페이지
**최초의 컴퓨터 비전 프로젝트 (1960년대)**
*   가장 초기의 공식적인 컴퓨터 비전 연구는 종종 MIT의 여름 비전 프로젝트(1966)로 거슬러 올라갑니다.
*   목표는 야심 찼지만 순진했습니다: 컴퓨터가 세상을 "보고" 해석하도록 프로그래밍하는 것 — 예를 들어, 방 안의 물체를 인식하는 것입니다.
*   당시 연구자들은 이것이 학생들이 몇 달 안에 해결할 수 있을 것이라고 생각했습니다. 실제로는 이 프로젝트가 시각적 인식이 얼마나 엄청나게 어려운지를 드러냈고, 수십 년간의 연구를 촉발시켰습니다.
*   **핵심 교훈**: 비전은 단지 모양을 인식하는 것이 아닙니다. 그것은 계층적 해석, 맥락, 그리고 실제 세계의 엄청난 가변성을 포함합니다.

***

### 37페이지
**Hubel & Wiesel의 발견 (1959–1962)**
*   신경생리학자 David Hubel과 Torsten Wiesel은 고양이와 원숭이의 시각 피질이 정보를 어떻게 처리하는지 연구했습니다.
*   그들은 시각이 계층적이라는 것을 발견했습니다:
    *   **단순 세포**는 특정 방향(예: 수직 또는 수평선)의 가장자리에 반응합니다.
    *   **복합 세포**는 이러한 반응들을 결합하여 패턴, 움직임, 모양을 감지합니다.
*   그들의 연구는 생물학적 비전이 낮은 수준의 특징(가장자리, 대비)에서 높은 수준의 구조(모양, 객체)로 구축된다는 것을 보여주었습니다.
*   이 원리는 인공 비전 연구에 강력한 영향을 미쳤으며 — 수십 년 후, 합성곱 신경망(CNNs)의 핵심 영감이 되었습니다.

***

### 38페이지
**컴퓨터 비전의 초기 단계 (1960년대–1980년대)**
*   컴퓨터 비전 연구는 1960년대 후반, 컴퓨터가 인간처럼 세상을 "보게" 만들려는 야심 찬 목표를 가진 인공지능의 한 분야로 시작되었습니다. 이 시기에는 컴퓨팅 파워와 이미징 장치가 매우 제한적이어서, 연구는 기본적이고 비교적 간단한 작업에 집중되었습니다.
    *   **패턴 인식**: 초기 시스템들은 기본적인 모양이나 객체(예: 원과 사각형 구분)를 인식하려고 시도했습니다. 이들은 주로 규칙 기반이었고 제한된 환경에서만 작동했습니다.
    *   **David Marr의 이론 (1970년대)**: Marr는 다단계 시각 이론을 제안했습니다 — 원시 이미지에서 시작하여 가장자리와 질감의 "원시 스케치"를 형성하고, 그 다음 표면의 2.5D 스케치로, 최종적으로 완전한 3D 표현으로 나아가는 것입니다. 비록 개념적이었지만, 이는 미래 연구를 위한 강력한 기반을 제공했습니다.

***

### 39페이지
**컴퓨터 비전의 초기 단계 (1960년대–1980년대)**
*   **David Marr의 이론 (1970년대)**: Marr는 다단계 시각 이론을 제안했습니다 — 원시 이미지에서 시작하여 가장자리와 질감의 "원시 스케치"를 형성하고, 그 다음 표면의 2.5D 스케치로, 최종적으로 완전한 3D 표현으로 나아가는 것입니다. 비록 개념적이었지만, 이는 미래 연구를 위한 강력한 기반을 제공했습니다.

***

### 40페이지
**가장자리부터 시작하기**
*   **왜 가장자리부터 시작하는가?**
    *   초기 객체 인식 연구에서, 가장자리는 가장 기본적인 단서로 간주되었습니다.
    *   색상과 강도는 조명이나 그림자에 따라 많이 달라질 수 있지만, 가장자리는 객체의 구조적 경계를 드러냅니다.
    *   따라서 초기 컴퓨터 비전 시스템은 일반적으로 이미지를 가장자리 맵으로 변환하는 것으로 시작했습니다.
*   **주요 가장자리 탐지 방법**
    *   기울기 기반 방법 (1960년대–1970년대) - Prewitt 및 Sobel 필터
    *   영점 교차 / 라플라시안 기반 방법 (1970년대–1980년대) – 2차 미분 사용
    *   Canny 가장자리 탐지기 (1986) - 가장 영향력 있으며, 오늘날에도 여전히 교육됨.

***

### 41페이지
**가장자리부터 시작하기**
*   **가장자리에서 객체 인식으로**
    *   가장자리를 추출한 후, 연구자들은 더 높은 수준의 구조를 재구성하려고 시도했습니다:
    *   선, 모서리, 윤곽선 → 모양으로 결합.
    *   예시: 자동차는 직선과 곡선의 집합으로, 사람은 특정 실루엣으로 인식될 수 있습니다.
*   **실제로는, 노이즈, 조명 변화, 그리고 복잡한 배경 때문에 가장자리 기반 인식이 신뢰할 수 없었습니다.**
*   **그럼에도 불구하고, 이 접근 방식은 Marr의 이론적 프레임워크를 처음으로 구체적으로 구현한 것으로, 신경과학에서 영감을 받은 아이디어와 컴퓨터 비전 알고리즘을 연결했습니다.**

***

### 42페이지
**초보적인 신경망 (1980년대–2000년대)**
*   이 용어는 오늘날의 딥러닝 시대 훨씬 이전인 1980년대-1990년대에 주로 개발된 인공 신경망(ANNs)의 매우 초기 형태를 가리킵니다. 이들은 현대 아키텍처에 비해 훨씬 단순하고 제한적이었습니다.
*   **역사적 배경**
    *   **퍼셉트론 (1958, Rosenblatt)**
        *   단일 계층 신경망: 입력 → 가중치 → 출력.
        *   선형 문제만 해결할 수 있었고, XOR 문제에서는 실패한 것으로 유명합니다 (Minsky & Papert, 1969).
        *   이 한계는 1970년대의 첫 AI 겨울에 기여했습니다.
    *   **역전파 (1986, Rumelhart, Hinton, Williams)**
        *   오차 역전파를 사용하여 다층 퍼셉트론(MLPs)을 훈련하는 방법을 도입했습니다.
        *   더 깊은 모델이 원칙적으로 학습할 수 있음을 보여줌으로써 신경망에 대한 관심을 다시 불러일으켰습니다.

***

### 43페이지
**초보적인 신경망 (1980년대–2000년대)**
*   이 용어는 오늘날의 딥러닝 시대 훨씬 이전인 1980년대-1990년대에 주로 개발된 인공 신경망(ANNs)의 매우 초기 형태를 가리킵니다. 이들은 현대 아키텍처에 비해 훨씬 단순하고 제한적이었습니다.
*   **역사적 배경**
    *   **1990년대 응용**
        *   여전히 제한된 데이터와 계산 능력으로 인해 소규모 네트워크만 실용적이었습니다.
        *   손글씨 숫자 인식(MNIST) 및 초기 음성 인식과 같은 작업에 적용되었습니다.
        *   현대의 심층 CNN이나 트랜스포머에 비해 단순성 때문에 "초보적"으로 간주됩니다.
    *   **초기 CNNs (LeNet-5, 1998, Yann LeCun)**
        *   최초의 성공적인 합성곱 신경망 중 하나로, 숫자 인식을 위해 사용되었습니다.
        *   오늘날의 기준으로는 여전히 얕지만, 딥러닝의 기초를 마련했습니다.

***

### 44페이지
**Viola–Jones 얼굴 탐지 (2001)**
*   이미지에서 얼굴을 빠르고 신뢰성 있게 탐지하는 방법을 도입했습니다.
*   2000년대 초반의 컴퓨터와 카메라에서도 실시간으로 작동하도록 설계되었습니다.
*   픽셀 단위로 검색하는 대신, 영리한 패턴 집합과 단계별 필터링 과정을 사용하여 얼굴이 있을 만한 위치를 신속하게 결정했습니다.
*   **의의**
    *   실시간 얼굴 탐지를 실용적으로 만든 최초의 알고리즘.
    *   웹캠, 디지털 카메라, 감시 시스템에 널리 채택되었습니다.
    *   전환점을 표시했습니다: 컴퓨터 비전은 더 이상 학문적인 것만이 아니라, 일상 소비자 기술의 일부가 되었습니다.

***

### 45페이지
**인터넷과 데이터의 폭발 (2000년대)**
*   **인터넷 이전**
    *   컴퓨터 비전 연구자들은 작은, 실험실에서 수집된 데이터셋(수백 개의 이미지)에만 접근할 수 있었습니다.
    *   알고리즘은 종종 제한된 일반화 능력을 가진 통제된 환경에서만 작동했습니다.
*   **인터넷과 함께**
    *   웹과 디지털 카메라의 부상은 이미지와 비디오의 폭발을 일으켰습니다.
    *   Google 이미지(2001)와 Flickr(2004) 같은 플랫폼은 수백만 개의 실제 이미지를 접근 가능하게 만들었습니다.
    *   처음으로 연구자들은 대규모의 다양한 데이터셋을 구축할 수 있었습니다.
*   **컴퓨터 비전에 미친 영향**
    *   벤치마크 데이터셋이 탄생했습니다: Caltech101 (2003), Pascal VOC (2005), ImageNet (2009).
    *   연구의 초점이 바뀌었습니다: 성공은 알고리즘뿐만 아니라 데이터의 규모와 다양성에도 달려있게 되었습니다.
*   **인터넷은 데이터 혁명을 촉발하여, 컴퓨터 비전을 소규모 실험실 데모에서 빅데이터 기반 분야로 변모시켰고, 2010년대의 딥러닝 돌파구를 위한 길을 열었습니다.**

***

### 46페이지
**인터넷과 데이터의 폭발 (2000년대)**
*   **Caltech101 (2003)**
    *   **클래스**: 101개의 객체 카테고리 + 1개의 배경 클래스.
    *   **이미지**: 총 약 9,000개 (카테고리당 40–800개, 대부분 50개 내외).
    *   **작업**: 객체 분류 (이미지가 주어지면 올바른 카테고리를 할당).
*   **Pascal VOC (Visual Object Classes Challenge)**
    *   처음에는 분류만 → 나중에 탐지 및 분할로 확장됨.

***

### 47페이지
**ImageNet (2009, Fei-Fei Li와 팀)**
*   **규모**
    *   WordNet 계층 구조 기반: ILSVRC 챌린지를 위한 약 1,000개의 객체 카테고리.
    *   총 1,200만 개 이상의 이미지, 연간 대회를 위해 120만 개의 훈련, 5만 개의 검증, 10만 개의 테스트 데이터.
*   **ILSVRC (ImageNet Large Scale Visual Recognition Challenge)**
    *   2010년부터 2017년까지 연례 대회.
    *   전 세계 비전 알고리즘을 비교하는 벤치마크 플랫폼이 됨.
*   **ImageNet이 전환점이었던 이유**
    *   **규모**: Caltech101 (약 9천)이나 Pascal VOC (약 1만)보다 훨씬 컸습니다. 이는 대규모 비전 데이터셋의 시작을 알렸습니다.
    *   **벤치마크 문화**: ILSVRC를 통해 컴퓨터 비전은 표준화된 글로벌 벤치마크 경쟁을 얻게 되었습니다.
    *   **딥러닝 돌파구**: AlexNet (2012)은 GPU에서 CNN을 사용하여 ILSVRC에서 큰 차이로 우승했습니다.

***

### 48페이지
**ImageNet에서의 딥러닝 돌파구**
*   2012년, AlexNet(8계층 심층 신경망)은 ImageNet 분류 오류를 26%에서 16%로 줄여, 전통적인 컴퓨터 비전에 비해 극적인 도약을 이루었습니다.
*   그 후, 이 분야는 더 깊고 정교한 아키텍처(VGG, ResNet 등)와 함께 급속한 발전의 시기에 들어섰습니다.
*   2015년까지 오류율은 인간 수준의 성능(약 5%) 아래로 떨어졌고, 2017년에는 딥 모델 앙상블이 약 2.25%의 오류율을 달성했습니다.

***

### 49페이지
**딥러닝 시대 (2012 → 현재)**
*   **컴퓨터 비전**
    *   심층 CNN은 거의 모든 비전 작업의 기본 선택이 되었습니다:
        *   이미지 분류 / 객체 탐지 / 분할 / …
*   **비전을 넘어서**
    *   딥러닝은 음성 인식, NLP, 강화 학습, 로보틱스, 추천 시스템, 의료 AI로 확산되었습니다.
    *   각 분야에서 딥 모델은 수십 년간의 수작업 파이프라인을 대체했습니다.
*   **생태계 변화**
    *   GPU(그리고 나중에는 TPU)가 필수적인 연구 도구가 되었습니다.
    *   빅데이터 + 대형 모델 → 더 많은 계산 집약적 연구.
    *   기술 산업은 자율 주행, 얼굴 인식, 의료 영상, AR/VR, 보안을 위해 딥러닝을 빠르게 채택했습니다.
*   **인간 수준 및 그 이상**
    *   모델들은 많은 벤치마크에서 인간의 성능을 따라잡았을 뿐만 아니라 능가했습니다.
    *   오늘날의 파운데이션 모델(예: CLIP, SAM, GPT-4V, 다중 모드 트랜스포머)로 가는 길을 열었습니다.
*   **2012년 이후, 컴퓨터 비전(그리고 더 넓게는 AI)은 "어디에나 딥러닝" 단계에 들어섰습니다.**

***

### 50페이지
**컴퓨터 비전의 기초**
2025년 2학기
- Kwang-Hyun Uhm –
인공지능학과

***

### 51페이지
**강의 2:**
**컴퓨터 비전에서의 이미지 소개**
*   컴퓨터는 세상을 어떻게 보는가?
*   디지털 이미지란 무엇인가?
*   이미지 데이터는 어떻게 저장되는가?
*   이미지에 대한 기본 연산

***

### 52페이지
**컴퓨터는 어떻게 보는가?**
*   **인간의 시각 vs. 컴퓨터 비전**
    *   **인간**: 빛이 눈으로 들어와 수정체에 의해 초점이 맞춰지고, 망막에 의해 포착된 후, 뇌로 전달되는 전기 신호로 변환됩니다.
    *   \* 우리는 실제로 객체를 직접 '보는' 것이 아닙니다. 우리는 객체에서 반사되어 우리 눈으로 들어오는 빛을 봅니다."
    *   **컴퓨터**: 카메라가 눈의 역할을 합니다. 빛이 렌즈를 통해 들어와 이미지 센서(CCD 또는 CMOS)에 의해 포착된 후, 전기 신호로 변환됩니다. 이 신호들은 디지털화되어 숫자로 저장됩니다.
*   **컴퓨터의 눈으로서의 카메라**
    *   **렌즈**: 들어오는 빛의 초점을 맞춥니다.
    *   **센서**: 빛을 전기 신호로 변환합니다.

***

### 53페이지
**카메라 기초**
*   **핀홀 카메라 모델**
    *   가장 간단한 카메라 모델은 핀홀 카메라로, 빛이 작은 구멍을 통과하여 반대편 평면(필름 또는 센서)에 상을 형성합니다. 이 모델은 카메라가 이미지를 캡처하는 방식을 이해하기 위한 기초로 자주 사용됩니다.
*   **빛과 이미지 형성**
    *   카메라는 빛을 모아 센서에 투사하여 작동합니다. 객체에서 반사된 광선이 구멍이나 렌즈를 통과하여 방향이 바뀌고, 결국 3차원 세계의 2차원 이미지를 형성합니다.
*   **렌즈와 초점**
    *   실제 카메라는 핀홀 대신 렌즈를 사용합니다. 렌즈는 빛을 굴절시켜 더 선명한 이미지를 만듭니다. 객체가 다른 거리(가깝거나 먼)에 있을 때, 초점을 맞추기 위해 렌즈 위치를 조정해야 합니다.
*   **현대 카메라**
    *   현대 카메라는 단순한 이미지 형성을 훨씬 뛰어넘습니다. 여러 렌즈, 정밀한 광학 부품, 디지털 센서를 결합하여 고급 이미지 처리를 가능하게 합니다. 이는 자동 초점, 줌, 피사계 심도 제어와 같은 기능을 허용합니다.

***

### 54페이지
**카메라 기초**
*   **핀홀 카메라 모델**
    *   가장 간단한 카메라 모델은 핀홀 카메라로, 빛이 작은 구멍을 통과하여 반대편 평면(필름 또는 센서)에 상을 형성합니다. 이 모델은 카메라가 이미지를 캡처하는 방식을 이해하기 위한 기초로 자주 사용됩니다.
*   **빛과 이미지 형성**
    *   카메라는 빛을 모아 센서에 투사하여 작동합니다. 객체에서 반사된 광선이 구멍이나 렌즈를 통과하여 방향이 바뀌고, 결국 3차원 세계의 2차원 이미지를 형성합니다.
*   **렌즈와 초점**
    *   실제 카메라는 핀홀 대신 렌즈를 사용합니다. 렌즈는 빛을 굴절시켜 더 선명한 이미지를 만듭니다. 객체가 다른 거리(가깝거나 먼)에 있을 때, 초점을 맞추기 위해 렌즈 위치를 조정해야 합니다.
*   **현대 카메라**
    *   현대 카메라는 단순한 이미지 형성을 훨씬 뛰어넘습니다. 여러 렌즈, 정밀한 광학 부품, 디지털 센서를 결합하여 고급 이미지 처리를 가능하게 합니다. 이는 자동 초점, 줌, 피사계 심도 제어와 같은 기능을 허용합니다.

***

### 55페이지
**현대 이미징 장치의 다양성**
*   **360° 카메라**
    *   여러 렌즈나 광각 광학 장치를 사용하여 전체 파노라마 뷰를 캡처합니다. 가상 현실(VR), 증강 현실(AR), 몰입형 미디어에서 흔히 사용됩니다.
*   **적외선(IR) 카메라**
    *   가시광선 대신 적외선 복사(열)를 감지합니다. 야간 투시, 감시, 산업 검사, 의료 체온 측정에 사용됩니다.
*   **LiDAR 센서**
    *   레이저 펄스를 방출하고 반사된 신호를 측정하여 정밀한 3D 지도를 구성합니다. 자율 주행 차량, 로보틱스, 지형 매핑에 필수적입니다.
*   **의료 영상 장치**
    *   **CT (컴퓨터 단층 촬영)**: X-레이를 사용하여 내부 구조의 3D 뷰를 재구성합니다.
    *   **MRI (자기 공명 영상)**: 자기장과 전파를 사용하여 상세한 연조직 이미지를 캡처합니다.
    *   **초음파**: 고주파 음파를 사용하여 장기, 혈류, 태아 발달을 시각화합니다.
*   **드론 카메라**
    *   사진, 영화 촬영, 농업 모니터링, 재난 관리를 위한 항공 시점을 제공합니다. 종종 자율 항법을 위해 GPS 및 컴퓨터 비전과 결합됩니다.

***

### 56페이지
**디지털 이미지란 무엇인가? 아날로그 신호 vs. 디지털 신호**
*   **아날로그 신호 (가시광선 스펙트럼)**
    *   **연속적**: 값이 단계 없이 부드럽게 변합니다.
    *   실제 세계의 빛은 아날로그입니다 — 밝기와 색이 연속적으로 변합니다.
*   **디지털 신호**
    *   **이산적**: 값이 단계적으로 샘플링되어 숫자로 표현됩니다.
    *   카메라 센서는 수백만 개의 지점(픽셀)에서 들어오는 빛을 샘플링하여 각각을 숫자로 변환합니다.
    *   유사 예시:
        *   12:01, 12:02, … 를 보여주는 디지털 시계 (이산적인 점프).
        *   디지털 카메라: 밝기를 0–255와 같은 숫자로 기록합니다.
*   **아날로그 = 부드럽고, 자연스럽고, 연속적 vs 디지털 = 샘플링되고, 양자화되고, 숫자로 저장됨**
    *   아날로그에서 디지털로 변환 = 연속적인 빛 신호를 샘플링하고 이산적인 픽셀 값으로 인코딩하는 것.

***

### 57페이지
**디지털 이미지란 무엇인가? 아날로그 신호 vs. 디지털 신호**
*   **아날로그 신호 (가시광선 스펙트럼)**
    *   **연속적**: 값이 단계 없이 부드럽게 변합니다.
    *   실제 세계의 빛은 아날로그입니다 — 밝기와 색이 연속적으로 변합니다.
*   **디지털 신호**
    *   **이산적**: 값이 단계적으로 샘플링되어 숫자로 표현됩니다.
    *   카메라 센서는 수백만 개의 지점(픽셀)에서 들어오는 빛을 샘플링하여 각각을 숫자로 변환합니다.
    *   유사 예시:
        *   12:01, 12:02, … 를 보여주는 디지털 시계 (이산적인 점프).
        *   디지털 카메라: 밝기를 0–255와 같은 숫자로 기록합니다.
*   **아날로그 = 부드럽고, 자연스럽고, 연속적 vs 디지털 = 샘플링되고, 양자화되고, 숫자로 저장됨**
    *   아날로그에서 디지털로 변환 = 연속적인 빛 신호를 샘플링하고 이산적인 픽셀 값으로 인코딩하는 것.

***

### 58페이지
**디지털 이미지란 무엇인가?**
*   실제 세계에서 빛은 연속적입니다 (아날로그).
*   디지털 이미지는 우리가 눈으로 보는 것과 같은 연속적인 그림이 아닙니다. 대신, **픽셀**이라고 불리는 작은 요소들의 **격자**입니다. 각 픽셀은 밝기나 색을 나타내는 숫자 값을 저장합니다.
    *   **그레이스케일 이미지**에서 각 픽셀은 단일 숫자를 가지며, 보통 0(검은색)과 255(흰색) 사이의 값으로 다양한 회색 음영을 나타냅니다.
    *   **컬러 이미지**에서 각 픽셀은 빨강(Red), 초록(Green), 파랑(Blue) (RGB)에 대한 세 가지 값을 저장합니다. 이들을 결합하여 컴퓨터는 수백만 가지 색상을 표현할 수 있습니다.
*   디지털 이미지는 단순히 숫자의 배열 — 픽셀 값의 행렬입니다. 컴퓨터에게 이미지는 "그림"이 아니라 데이터입니다.

***

### 59페이지
**디지털 이미지: 샘플링, 양자화, 해상도**
*   **샘플링**
    *   디지털 카메라는 이 연속적인 장면을 픽셀의 격자로 나눕니다.
    *   격자가 너무 거칠면, 우리는 픽셀화(계단 현상)를 보게 됩니다.
*   **양자화**
    *   픽셀에 도달하는 빛의 양은 여전히 연속적인 값입니다.
    *   카메라는 이를 이산적인 수준으로 변환합니다 → 이 과정을 양자화라고 합니다.
*   **공간 해상도**
    *   가로 × 세로의 픽셀 수로 정의됩니다.
    *   고해상도 → 더 많은 디테일, 더 선명한 이미지.
    *   저해상도 → 더 적은 픽셀, 더 큰 블록 → 눈에 보이는 픽셀화.

***

### 60페이지
**디지털 이미지의 색상**
*   **RGB 표현 (세 채널: 빨강, 초록, 파랑)**
    *   이 세 신호를 다른 강도로 혼합함으로써 거의 모든 가시 색상을 표현할 수 있습니다.
    *   예시: (255, 0, 0) → 빨강 / (0, 255, 0) → 초록 / (0, 0, 255) → 파랑 / (255, 255, 255) → 흰색
*   **왜 8비트(0–255)인가? -> 품질과 저장 효율성 사이의 최상의 절충안.**
    *   **효율성**:
        *   채널당 8비트 = 256 단계 × 3 채널 = 약 1,670만 가지 가능한 색상.
        *   이는 인간의 눈이 일상적인 사용에서 구별할 수 있는 범위를 충분히 커버합니다.
    *   **비교**:
        *   4비트 (채널당 16 단계) → 4,096가지 색상만 → 눈에 띄는 밴딩 현상.
        *   12비트 또는 16비트 → 수십억 가지 색상, 더 높은 정밀도 → 전문 사진, 의료 영상 또는 과학 데이터에 유용하지만 대부분의 디스플레이에는 불필요합니다. (HDR, High Dynamic Range)
    *   **인간의 시각**: 인간의 눈은 일반적인 시청 조건에서 8비트와 더 높은 비트 깊이의 차이를 쉽게 인지하지 못합니다.

***

### 61페이지
**디지털 컬러 이미지**
*   디지털 컬러 이미지는 각 색상 채널(빨강, 초록, 파랑 - RGB)에 대해 하나씩, 세 개의 개별 격자(배열)로 구성됩니다.
    *   각 픽셀 위치에서 세 가지 값이 결합되어 최종 색상을 나타냅니다.
    *   그레이스케일 이미지는 단 하나의 채널만 가집니다. 단일 밝기 값(예: 0 = 검정, 255 = 흰색).

***

### 62페이지
**디지털 이미지: 기본 구성 요소 (요약)**
*   **픽셀**
    *   디지털 이미지의 가장 작은 단위.
    *   각 픽셀은 숫자 값을 저장합니다.
    *   더 많은 픽셀 → 더 세밀한 디테일.
*   **채널**
    *   **그레이스케일 이미지**: 1 채널 (밝기만).
    *   **컬러 이미지**: 3 채널 (빨강, 초록, 파랑).
    *   컬러 픽셀 = R, G, B 값의 조합.
*   **해상도**
    *   가로 × 세로 픽셀 수로 정의됩니다.
    *   고해상도 → 더 선명하고, 더 많은 디테일.
    *   저해상도 → 블록 모양 (픽셀화).
*   디지털 이미지는 픽셀의 격자입니다. 각 픽셀은 하나의 밝기 값(그레이스케일) 또는 세 개의 값(RGB 색상)을 가집니다. 픽셀의 수는 해상도를 정의하고, 채널 값은 이미지가 어떻게 보이는지를 정의합니다.

***

### 63페이지
**기본 이미지 연산**

***

### 64페이지
**기본 이미지 연산**
*   **그레이스케일 변환**
    *   그레이스케일로 변환하는 것은 색상 정보를 제거하고 각 픽셀의 밝기만 유지하는 것을 의미합니다.
*   **직관**
    *   인간의 눈은 이미 밝기에 매우 민감합니다.
    *   색이 없어도 우리는 여전히 모양, 가장자리, 객체를 인식할 수 있습니다.
    *   그레이스케일 이미지는 더 단순합니다 → 데이터가 적고, 처리 속도가 빠릅니다.
*   **예시**
    *   고양이의 컬러 사진 → 그레이스케일 사진: 여전히 명확하게 인식 가능.
    *   문서 스캔 (색상 불필요).
    *   가장자리 탐지 (강도 변화에 집중).
    *   얼굴 탐지 (그레이스케일에서 잘 작동).
*   필수 정보를 보존하면서

***

### 65페이지
**그레이스케일 변환**
*   **단순 평균**
    *   `Gray = (R + G + B) / 3`
    *   세 채널을 모두 동등하게 처리합니다.
    *   계산하기 쉽지만 인간의 눈이 밝기를 인식하는 방식과 일치하지 않습니다.
*   **가중 평균 (지각적 휘도)**
    *   인간의 눈은 초록색에 가장 민감하고, 빨간색에 덜 민감하며, 파란색에 가장 덜 민감합니다.
    *   따라서 가중 공식이 사용됩니다 (ITU-R BT.601 표준):
    *   `Gray = 0.299R + 0.587G + 0.144B`
    *   이는 인지된 밝기를 더 잘 반영하는 그레이스케일 이미지를 생성합니다.

***

### 66페이지
**이진 이미지 (임계처리)**
*   **이진 이미지**는 각 픽셀에 대해 두 가지 가능한 값만 가집니다:
    *   **0** = 검은색
    *   **1 (또는 255)** = 흰색
    *   그레이스케일 이미지에 **임계값**을 적용하여 생성됩니다.
*   **임계처리**
    *   픽셀 강도 ≥ 임계값 → 흰색
    *   픽셀 강도 < 임계값 → 검은색
*   **예시**:
    *   임계값 (T) = 128
    *   픽셀 150 → 흰색 / 픽셀 80 → 검은색

***

### 67페이지
**이진 이미지 (임계처리)**
*   **직관**
    *   이진 이미지는 그레이스케일보다 복잡성을 더욱 줄입니다.
    *   객체의 필수적인 구조만 유지합니다.
    *   전경과 배경을 분리하는 데 유용합니다.
*   **응용**
    *   문서 스캔: 텍스트 대 배경.
    *   바코드 및 QR 코드 인식.
    *   모양 탐지 및 객체 계수.
*   **컬러 이미지 -> 그레이스케일 우선**
    *   대부분의 경우, 컬러 이미지는 먼저 그레이스케일로 변환된 다음 임계처리가 적용됩니다.
    *   이유: 임계처리는 강도 값에 대해 작동하며, 그레이스케일은 단일 밝기 채널을 제공합니다.

***

### 68페이지
**크기 조정 (이미지 스케일링)**
*   크기 조정은 이미지의 **너비 × 높이**를 변경합니다.
    *   **다운샘플링** → 이미지를 더 작게 만듭니다.
    *   **업샘플링** → 이미지를 더 크게 만듭니다.
*   이미지를 더 작게 만들기: 픽셀이 제거되거나 병합됩니다.
    *   결과 → 처리 속도는 빨라지지만 디테일이 손실됩니다.
*   이미지를 더 크게 만들기: 픽셀이 보간법에 의해 "채워져야" 합니다.
    *   결과 → 이미지는 커지지만 흐릿하거나 각져 보일 수 있습니다.
*   **응용**
    *   **디스플레이**: 화면에 맞게 이미지 크기를 조정합니다.
    *   **전처리**: 머신러닝 모델을 위해 이미지를 고정된 입력 크기로 조정합니다.
    *   **압축**: 더 작은 크기 → 저장 공간 감소 및 전송 속도 향상.

***

### 69페이지
**크기 조정 (이미지 스케일링)**
*   **최근접 이웃 스케일링**
    *   크기 조정을 위한 가장 간단한 방법.
    *   원본 이미지에서 가장 가까운 픽셀의 값을 그대로 복사합니다.
    *   빠르지만, 확대할 때 종종 각지거나 픽셀화되어 보입니다.

***

### 70페이지
**크기 조정 (이미지 스케일링)**
*   **최근접 이웃 스케일링 (다운샘플링)**
    *   **다운샘플링**(크기 축소) 시, 원본 픽셀 블록 각각이 더 작은 이미지의 단일 픽셀에 매핑됩니다.
    *   값은 구현에 따라 다른 방식으로 선택될 수 있습니다:
        *   블록의 **중심 픽셀** (가장 일반적).
        *   블록의 **왼쪽 상단 픽셀** (더 간단하지만 덜 정확함).
    *   두 방법 모두 최근접 이웃으로 간주되지만, 선택된 참조점이 다릅니다.

***

### 71페이지
**크기 조정 (이미지 스케일링)**
*   **쌍선형 업샘플링**
    *   단순히 가장 가까운 픽셀을 복사하는 대신, 쌍선형 보간법은 새 픽셀 위치 주변의 **네 개의 가장 가까운 이웃**을 살펴봅니다.
    *   새 픽셀 값은 이 네 값의 **가중 평균**입니다.
*   **직관**
    *   최근접 이웃 → "어떤 픽셀이 가장 가까운가? 복사해."
    *   쌍선형 → "가장 가까운 네 픽셀을 부드럽게 섞어."
*   **속성**
    *   최근접 이웃보다 **더 부드럽지만**, 계산 비용이 약간 더 높습니다.
    *   사진 및 자연 이미지에 잘 작동합니다.
    *   여전히 완벽하지 않음: 평균화 때문에 흐림 현상이 발생할 수 있습니다.

***

### 72페이지
**크기 조정 (이미지 스케일링)**
*   **쌍선형 업샘플링**
    *   단순히 가장 가까운 픽셀을 복사하는 대신, 쌍선형 보간법은 새 픽셀 위치 주변의 **네 개의 가장 가까운 이웃**을 살펴봅니다.
    *   새 픽셀 값은 이 네 값의 **가중 평균**입니다.
*   **직관**
    *   최근접 이웃 → "어떤 픽셀이 가장 가까운가? 복사해."
    *   쌍선형 → "가장 가까운 네 픽셀을 부드럽게 섞어."
*   **속성**
    *   최근접 이웃보다 **더 부드럽지만**, 계산 비용이 약간 더 높습니다.
    *   사진 및 자연 이미지에 잘 작동합니다.
    *   여전히 완벽하지 않음: 평균화 때문에 흐림 현상이 발생할 수 있습니다.

***

### 73페이지
**크기 조정 (이미지 스케일링)**
*   **쌍선형 업샘플링**
    *   단순히 가장 가까운 픽셀을 복사하는 대신, 쌍선형 보간법은 새 픽셀 위치 주변의 **네 개의 가장 가까운 이웃**을 살펴봅니다.
    *   새 픽셀 값은 이 네 값의 **가중 평균**입니다.
*   **직관**
    *   최근접 이웃 → "어떤 픽셀이 가장 가까운가? 복사해."
    *   쌍선형 → "가장 가까운 네 픽셀을 부드럽게 섞어."
*   **속성**
    *   최근접 이웃보다 **더 부드럽지만**, 계산 비용이 약간 더 높습니다.
    *   사진 및 자연 이미지에 잘 작동합니다.
    *   여전히 완벽하지 않음: 평균화 때문에 흐림 현상이 발생할 수 있습니다.

***

### 74페이지
**크기 조정 (이미지 스케일링)**
*   **평균 풀링 (쌍선형 다운샘플링)**
    *   **평균 풀링**은 다운샘플링 방법입니다.
    *   이미지는 작은 블록(예: 2×2, 3×3)으로 나뉩니다.
    *   각 블록은 해당 픽셀들의 **평균값**으로 대체됩니다.
*   **직관**
    *   (최근접 이웃처럼) 단 하나의 픽셀만 선택하는 대신, 블록 안의 **모든 픽셀의 평균**을 취합니다.
    *   이는 전반적인 밝기를 유지하고 변동을 부드럽게 합니다.
    *   노이즈를 줄이고 최근접 이웃보다 전역 구조를 더 잘 보존합니다.

***

### 75페이지
**최근접 이웃 vs. 평균 풀링 (다운샘플링)**
*   **최근접 이웃**
    *   각 픽셀 블록에 대해, 단지 **하나의 픽셀 값**을 선택합니다 (예: 왼쪽 상단 또는 중앙).
    *   매우 빠르고 간단합니다.
    *   대부분의 정보를 무시합니다. 각지거나 들쭉날쭉해 보일 수 있습니다.
*   **평균 풀링**
    *   각 픽셀 블록에 대해, 블록 안의 **모든 픽셀의 평균값**을 계산합니다.
    *   전반적인 밝기(강도 균형)를 보존합니다. 더 부드럽고 덜 각진 결과를 낳습니다.
    *   세밀한 디테일이 흐려집니다.

***

### 76페이지
**이미지 크기 조정을 위한 보간 방법**
*   최근접 → 선형 → 3차 (Bicubic)
*   **3차 (Bicubic)**
    *   16개의 이웃 픽셀(4×4 블록)을 사용합니다.
    *   그들을 통과하는 부드러운 곡선(3차 다항식)을 맞춥니다.
    *   쌍선형에 비해 더 선명한 결과를 생성하고 더 많은 디테일을 보존합니다.
    *   더 많은 계산이 필요합니다.

***

### 77페이지
**이미지 크기 조정을 위한 보간 방법**
*   최근접 → 선형 → 3차 (Bicubic)
*   **3차 (Bicubic)**
    *   16개의 이웃 픽셀(4×4 블록)을 사용합니다.
    *   그들을 통과하는 부드러운 곡선(3차 다항식)을 맞춥니다.
    *   쌍선형에 비해 더 선명한 결과를 생성하고 더 많은 디테일을 보존합니다.
    *   더 많은 계산이 필요합니다.

***

### 78페이지
**기하학적 변환: 이미지 뒤집기**
*   **개념**
    *   **뒤집기** = 축을 따라 이미지를 반전시키는 것
    *   **수평 뒤집기**: 왼쪽 ↔ 오른쪽
    *   **수직 뒤집기**: 위 ↔ 아래
*   **응용**
    *   딥러닝에서의 **데이터 증강**
        *   훈련 데이터의 다양성을 증가시킵니다.
        *   과적합을 줄이는 데 도움이 됩니다.
    *   컴퓨터 비전 작업
        *   객체의 방향이 고정되지 않았을 때 유용합니다 (예: 동물, 얼굴, 자연 이미지).
*   **뒤집기는 가장 간단한 기하학적 변환입니다 — 새로운 픽셀 값이 생성되지 않고, 단지 미러링됩니다.**

***

### 79페이지
**이미지 회전 (기본 각도)**
*   **회전** = 이미지의 중심(또는 원점)을 기준으로 이미지를 돌리는 것.
*   특수 경우(90°의 배수)는 픽셀이 정확히 정렬되므로 구현하기 쉽습니다 — 보간이 필요 없습니다.
*   **일반적인 회전**
    *   90° 회전 (반시계 방향)
    *   180° 회전 (수평 + 수직 뒤집기와 동일)
    *   270° 회전 (반시계 방향)

***

### 80페이지
**이미지 회전 (기본 각도)**
*   **회전** = 이미지의 중심(또는 원점)을 기준으로 이미지를 돌리는 것.
*   특수 경우(90°의 배수)는 픽셀이 정확히 정렬되므로 구현하기 쉽습니다 — 보간이 필요 없습니다.
*   **일반적인 회전**
    *   90° 회전 (반시계 방향)
    *   180° 회전 (수평 + 수직 뒤집기와 동일)
    *   270° 회전 (반시계 방향)

***

### 81페이지
**이미지 회전 (기본 각도)**
*   **회전** = 이미지의 중심(또는 원점)을 기준으로 이미지를 돌리는 것.
*   특수 경우(90°의 배수)는 픽셀이 정확히 정렬되므로 구현하기 쉽습니다 — 보간이 필요 없습니다.
*   **일반적인 회전**
    *   90° 회전 (반시계 방향)
    *   180° 회전 (수평 + 수직 뒤집기와 동일)
    *   270° 회전 (반시계 방향)

***

### 82페이지
**이미지 회전 (기본 각도)**
*   **회전** = 이미지의 중심(또는 원점)을 기준으로 이미지를 돌리는 것.
*   특수 경우(90°의 배수)는 픽셀이 정확히 정렬되므로 구현하기 쉽습니다 — 보간이 필요 없습니다.
*   **일반적인 회전**
    *   90° 회전 (반시계 방향)
    *   180° 회전 (수평 + 수직 뒤집기와 동일)
    *   270° 회전 (반시계 방향)

***

### 83페이지
**이미지 회전 (임의의 각도)**
*   **특수 각도 (90°, 180°, 270°)**는 픽셀 좌표가 정수 격자에 깔끔하게 매핑되기 때문에 간단합니다. 예를 들어, 90° 회전은 행과 열을 바꾸기만 합니다 (예: (x,y) = (y, W−x)).
*   **임의의 각도 (예: 25°, 47°)**는 픽셀 좌표가 더 이상 격자와 완벽하게 정렬되지 않기 때문에 더 복잡합니다.
*   **좌표 변환**
    *   임의의 각도 θ로 회전할 때, 핵심은 각 출력 픽셀을 입력 이미지의 해당 위치로 다시 매핑하는 것입니다. 변환은 다음과 같이 쓸 수 있습니다:
    *   (x′,y′): 회전된 (출력) 이미지의 좌표
    *   (x,y): 원본 (입력) 이미지의 해당 좌표
    *   이 방법은 출력 픽셀에서 입력 픽셀로 추적하기 때문에 종종 **역방향 매핑**이라고 불립니다.
*   회전 후, (x,y)는 보통 **정수가 아닌 좌표**(예: (25.3, 47.7))가 됩니다.
    *   픽셀 강도를 결정하기 위해, 우리는 **보간법**이 필요합니다. 최근접 이웃 / 쌍선형 / 3차 보간법.

***

### 84페이지
**이미지 자르기**
*   **자르기란 무엇인가?**
    *   자르기는 이미지의 **하위 영역을 선택**하고 그 밖의 모든 것을 버리는 것을 의미합니다.
    *   예시: 512×512 이미지의 중앙에서 100×100 패치를 가져오는 것.
    *   픽셀 값은 변경하지 않고, 이미지의 공간적 범위만 변경합니다.
*   **개념적으로 어떻게 작동하는가?**
    *   이미지를 좌표 (x,y)를 가진 픽셀의 격자로 생각합니다.
    *   자르기는 단지 **경계 상자**(상, 하, 좌, 우 인덱스로 정의된 사각형 영역)를 선택하는 것입니다.
    *   그런 다음 그 상자 안의 픽셀만 유지합니다.

***

### 85페이지
**이미지 자르기**
*   **자르기란 무엇인가?**
    *   자르기는 이미지의 **하위 영역을 선택**하고 그 밖의 모든 것을 버리는 것을 의미합니다.
    *   예시: 512×512 이미지의 중앙에서 100×100 패치를 가져오는 것.
    *   픽셀 값은 변경하지 않고, 이미지의 공간적 범위만 변경합니다.
*   **개념적으로 어떻게 작동하는가?**
    *   이미지를 좌표 (x,y)를 가진 픽셀의 격자로 생각합니다.
    *   자르기는 단지 **경계 상자**(상, 하, 좌, 우 인덱스로 정의된 사각형 영역)를 선택하는 것입니다.
    *   그런 다음 그 상자 안의 픽셀만 유지합니다.

***

### 86페이지
**이미지 히스토그램**
*   **히스토그램이란 무엇인가?**
    *   **히스토그램**은 데이터의 분포를 보여주는 그래프입니다.
    *   x축은 값을 나타내고, y축은 그 값이 얼마나 자주(빈도) 발생하는지를 나타냅니다.
    *   예시: 시험 점수 히스토그램은 각 점수 범위에 얼마나 많은 학생이 있는지를 보여줍니다.
*   **이미지 히스토그램**
    *   이미지의 경우, 히스토그램은 **픽셀 강도 값**의 분포를 나타냅니다.
    *   **그레이스케일 이미지**:
        *   x축 = 픽셀 값 (0 = 검정, 255 = 흰색).
        *   y축 = 해당 값을 가진 픽셀의 수.
    *   이미지가 전반적으로 얼마나 어둡거나 밝은지를 알려줍니다.

***

### 87페이지
**이미지 히스토그램**
*   **히스토그램이란 무엇인가?**
    *   **히스토그램**은 데이터의 분포를 보여주는 그래프입니다.
    *   x축은 값을 나타내고, y축은 그 값이 얼마나 자주(빈도) 발생하는지를 나타냅니다.
    *   예시: 시험 점수 히스토그램은 각 점수 범위에 얼마나 많은 학생이 있는지를 보여줍니다.
*   **이미지 히스토그램**
    *   이미지의 경우, 히스토그램은 **픽셀 강도 값**의 분포를 나타냅니다.
    *   **그레이스케일 이미지**:
        *   x축 = 픽셀 값 (0 = 검정, 255 = 흰색).
        *   y축 = 해당 값을 가진 픽셀의 수.
    *   이미지가 전반적으로 얼마나 어둡거나 밝은지를 알려줍니다.
*   히스토그램 평활화

***

### 88페이지
**이미지 히스토그램**
*   **컬러 이미지**
    *   컬러 (RGB) 이미지의 경우, 각 채널에 대해 개별적으로 히스토그램을 그릴 수 있습니다:
        *   빨간색 채널 히스토그램
        *   초록색 채널 히스토그램
        *   파란색 채널 히스토그램
    *   이들은 종종 함께 표시되어 이미지의 색상 분포에 대한 개요를 제공합니다.
*   **왜 유용한가?**
    *   **밝기 및 대비 분석**: 이미지가 너무 어둡거나, 너무 밝거나, 균형이 잘 맞는지 보여줍니다.
    *   **이미지 향상**: 히스토그램 평활화와 같은 방법은 이 개념에 의존합니다.
    *   **컴퓨터 비전 특징**: 히스토그램은 종종 이미지 특성을 설명하는 데 사용됩니다.

***

### 89페이지
**컴퓨터 비전의 기초**
2025년 2학기
- Kwang-Hyun Uhm –
인공지능학과

***

### 90페이지
**강의 3:**
**이미지 필터링**
*   이미지 필터링이란 무엇인가?
*   스무딩 필터
*   에지 필터

***

### 91페이지
**고양이 vs. 개**
*   "어떻게 구별할 수 있나요?"
    *   아마도 귀, 눈, 털 패턴, 심지어 얼굴 모양을 볼 것입니다.

***

### 92페이지
**"숫자로 고양이를 알아볼 수 있나요?"**
*   **숫자로서의 픽셀**
    *   "하지만 이 고양이 이미지를 가져와서 픽셀 값만 보여준다면 — 큰 배열의 숫자들 — 여전히 고양이인지 알 수 있을까요?"
    *   그렇지 않습니다. 개별 픽셀 값은 우리에게 많은 것을 알려주지 않습니다.

***

### 93페이지
**"숫자로 고양이를 알아볼 수 있나요?"**
*   **숫자로서의 픽셀**
    *   "하지만 이 고양이 이미지를 가져와서 픽셀 값만 보여준다면 — 큰 배열의 숫자들 — 여전히 고양이인지 알 수 있을까요?"
    *   그렇지 않습니다. 개별 픽셀 값은 우리에게 많은 것을 알려주지 않습니다.
*   **"그렇다면 실제로 인식을 가능하게 하는 것은 무엇일까요?"**
    *   원시 픽셀 값이 아닙니다.

***

### 94페이지
**"숫자로 고양이를 알아볼 수 있나요?"**
*   **"그렇다면 실제로 인식을 가능하게 하는 것은 무엇일까요?"**
    *   원시 픽셀 값이 아닙니다.
    *   그것은 **픽셀들이 함께 변하는 패턴**입니다: 모양을 정의하는 가장자리, 털을 암시하는 질감, 그리고 객체를 배경과 분리하는 대비입니다."

***

### 95페이지
**"숫자로 고양이를 알아볼 수 있나요?"**
*   원시 이미지는 **정보 과부하** 상태입니다 → 수백만 개의 픽셀.
    *   개별 픽셀 값은 거의 의미가 없습니다.
    *   (예: 고양이와 개 이미지는 픽셀 값 수준에서는 매우 유사해 보입니다)
*   이러한 패턴을 포착하기 위해, 우리는 도구가 필요합니다.
    *   그 도구는 **필터링**이라고 불립니다.
    *   필터링은 픽셀과 그 이웃을 보고, 특정 구조를 강조하는 새로운 이미지를 생성합니다 — 노이즈를 부드럽게 하거나, 가장자리를 선명하게 하거나, 윤곽선을 감지하는 것처럼 말이죠.
*   **이것은 이미지를 단순한 원시 숫자에서 분류나 탐지와 같은 작업에 사용할 수 있는 의미 있는 특징으로 변환하는 첫 번째 단계입니다."**

***

### 96페이지
**비전은 픽셀이 아닌 패턴에 관한 것**
*   원시 이미지는 수백만 개의 픽셀 값에 불과합니다. 그 자체로는 의미를 전달하지 않습니다.
    *   이미지를 유익하게 만드는 것은 픽셀이 **이웃과 어떻게 관련되는지**입니다: 가장자리, 모서리, 질감, 모양.
    *   필터링은 근처 픽셀 값들을 결합하여 새롭고 더 의미 있는 신호로 만들어 이러한 관계를 포착합니다.
*   **인간 시각과의 연결**
    *   인간의 눈과 뇌는 단순히 픽셀을 보지 않습니다 — 그들은 **패턴과 대비**를 인식합니다.
    *   예시:
        *   텍스트 읽기 → 글자와 배경 사이의 **가장자리** 감지
        *   얼굴 인식 → 그 **윤곽선**을 따라감
        *   객체 구별 → 그들의 **질감**을 알아챔
    *   컴퓨터 비전에서의 필터링은 비슷한 역할을 합니다: 계산적인 방식으로 원시 픽셀에서 **의미 있는 시각적 특징**을 추출합니다.

***

### 97페이지
**노이즈 감소, 구조 강화**
*   필터링을 통해 우리는 관련 없는 세부 사항(노이즈, 배경 혼잡)을 **억제**하고 중요한 구조(가장자리, 윤곽, 패턴)를 **강조**할 수 있습니다.
*   **노이즈 감소**
    *   실제 이미지는 지저분합니다: 카메라 노이즈, 흐림, 조명 변화.
    *   스무딩 필터(Gaussian, Median)는 관련 없는 세부 사항을 억제하고 이미지를 단순화합니다.
*   **특징 강화**
    *   인식은 가장자리, 모서리, 질감에 의존합니다.
    *   선명화 또는 가장자리 필터(Sobel, Laplacian)는 중요한 구조와 경계를 강조합니다.
*   **다중 스케일 분석**
    *   객체는 미세한 세부 사항과 거친 패턴을 모두 포함합니다.
    *   다른 커널 크기의 필터를 적용하면 여러 스케일에서 특징을 드러냅니다.

***

### 98페이지
**노이즈 감소, 구조 강화**
*   필터링을 통해 우리는 관련 없는 세부 사항(노이즈, 배경 혼잡)을 **억제**하고 중요한 구조(가장자리, 윤곽, 패턴)를 **강조**할 수 있습니다.
*   **노이즈 감소**
    *   실제 이미지는 지저분합니다: 카메라 노이즈, 흐림, 조명 변화.
    *   스무딩 필터(Gaussian, Median)는 관련 없는 세부 사항을 억제하고 이미지를 단순화합니다.
*   **특징 강화**
    *   인식은 가장자리, 모서리, 질감에 의존합니다.
    *   선명화 또는 가장자리 필터(Sobel, Laplacian)는 중요한 구조와 경계를 강조합니다.
*   **다중 스케일 분석**
    *   객체는 미세한 세부 사항과 거친 패턴을 모두 포함합니다.
    *   다른 커널 크기의 필터를 적용하면 여러 스케일에서 특징을 드러냅니다.

***

### 99페이지
**현대 딥러닝으로의 다리**
*   필터링은 단순히 이미지를 "장식"하는 것이 아닙니다 — 그것은 유용한 정보를 선택적으로 강화하고 관련 없는 세부 사항을 억제하는 과정입니다.
*   분류, 탐지, 분할과 같은 고수준 비전 작업은 궁극적으로 이러한 저수준 특징들을 결합하는 데 의존합니다.
*   고전 컴퓨터 비전에서는 **수작업 필터**(HOG, SIFT, Gabor 필터)가 특징 추출의 중심이었지만, 현대 딥러닝에서는 CNN이 이 역할을 자동으로 수행하기 위해 합성곱 필터를 학습합니다.
*   따라서 필터링을 배우는 것은 우리에게 도움이 됩니다:
    *   컴퓨터 비전의 기본 철학을 이해하는 것 — 이미지를 단순한 픽셀 모음에서 패턴 모음으로 변환하는 것.
    *   합성곱 계층이 학습된 필터의 집합으로 볼 수 있는 CNN에 대한 직관을 구축하는 것.

***

### 100페이지
**에지 탐지 필터**

***

### 101페이지
**에지 탐지 필터**
*   **에지란 무엇인가?**
    *   에지는 이미지에서 강도가 급격하게 변하는 곳입니다 — 예를 들어, 객체와 배경 사이.
*   **에지가 왜 중요한가?**
    *   에지는 객체의 **모양**을 윤곽으로 나타냅니다.
    *   색상과 질감이 제거되더라도, 에지만으로도 종종 객체를 인식할 수 있습니다 (예: 고양이의 윤곽 대 개의 윤곽).
    *   많은 고전 컴퓨터 비전 방법(HOG, SIFT)은 에지와 그래디언트 정보 위에 구축됩니다.
    *   딥러닝에서, CNN의 **초기 계층** 또한 에지와 질감을 자동으로 감지하도록 학습합니다.

***

### 102페이지
**에지 탐지 필터**
*   **에지를 어떻게 탐지하는가?**
    *   **그래디언트** — 픽셀 값의 변화 — 를 측정하는 필터를 적용함으로써.
    *   예시: **Sobel, Prewitt, Laplacian** 필터.
    *   이 필터들은 균일한 영역을 억제하면서 경계를 강조합니다.
*   **컴퓨터 비전과의 연결**
    *   에지 탐지는 **원시 픽셀에서 의미 있는 구조로 가는 첫 단계**입니다.
    *   이는 고수준 작업(분류, 탐지, 분할)이 의존하는 저수준 특징을 제공합니다.

***

### 103페이지
**에지 탐지 필터**
*   **에지를 어떻게 탐지하는가?**
    *   **그래디언트** — 픽셀 값의 변화 — 를 측정하는 필터를 적용함으로써.
    *   예시: **Sobel, Prewitt, Laplacian** 필터.
    *   이 필터들은 균일한 영역을 억제하면서 경계를 강조합니다.
*   **컴퓨터 비전과의 연결**
    *   에지 탐지는 **원시 픽셀에서 의미 있는 구조로 가는 첫 단계**입니다.
    *   이는 고수준 작업(분류, 탐지, 분할)이 의존하는 저수준 특징을 제공합니다.

***

### 104페이지
**극단적인 수직 에지**
*   **간단한 이미지 구성**
    *   왼쪽 절반 = 강도 **50** (균일한 회색)
    *   오른쪽 절반 = 강도 **0** (검은색)
*   **질문: "에지는 어디에 있는가?"**
    *   이는 정확히 중앙에 **날카로운 수직 에지**를 만듭니다.

***

### 105페이지
**극단적인 수직 에지**
*   **차이의 관점에서 생각하기**
*   **"50에서 이웃 0을 빼고 싶지 않나요?"**
    *   → 그러면 **50**이라는 강한 신호가 나옵니다 → 이것이 에지입니다.

***

### 106페이지
**극단적인 수직 에지**
*   **차이의 관점에서 생각하기**
*   **"50에서 이웃 0을 빼고 싶지 않나요?"**
    *   → 그러면 **50**이라는 강한 신호가 나옵니다 → 이것이 에지입니다.
*   **"50과 50, 또는 0과 0은 어떤가요?"**
    *   → 차이 = **0** → 에지 없음.

***

### 107페이지
**극단적인 수직 에지**
*   **전체 이미지에 걸쳐 적용**
    *   행의 **모든 위치**에서 이 빼기를 반복합니다.

***

### 108페이지
**차이에서 "템플릿"으로**
*   방금 수행한 빼기는 작은 **템플릿: [1, -1]**로 작성할 수 있습니다.
    *   이 템플릿을 두 개의 이웃 픽셀 위에 놓습니다.
    *   곱하고 더합니다:
    *   1×50+(−1)×0=50 → 에지
    *   1×50+(−1)×50=0 → 평평한 영역

***

### 109페이지
**차이에서 "템플릿"으로**
*   **템플릿 슬라이딩**
    *   이 템플릿을 이미지 전체에 걸쳐 **이동(또는 슬라이드)**합니다.
    *   각 위치에서 곱셈과 합계를 계산합니다.
    *   모든 결과를 수집합니다 → 이것이 **에지 맵**을 제공합니다.

***

### 110페이지
**차이에서 "템플릿"으로**
*   **템플릿 슬라이딩**
    *   이 템플릿을 이미지 전체에 걸쳐 **이동(또는 슬라이드)**합니다.
    *   각 위치에서 곱셈과 합계를 계산합니다.
    *   모든 결과를 수집합니다 → 이것이 **에지 맵**을 제공합니다.

***

### 111페이지
**차이에서 "템플릿"으로**
*   **핵심 통찰**
    *   이 슬라이딩 연산이 나중에 우리가 **합성곱(convolution)**이라고 부르는 것입니다.
    *   합성곱 = 곱셈 + 합계 + 슬라이딩.
    *   템플릿 [1, -1]은 한 방향의 변화를 감지하는 가장 간단한 **필터(커널)**입니다.
    *   커널은 어떤 종류의 특징(에지, 흐림, 선명화 등)이 추출될지를 정의합니다.

***

### 112페이지
**[1, –1]의 한계**
*   **한 방향**(왼쪽에서 오른쪽으로의 차이)만 봅니다.
    *   에지를 감지하지만, **하나의 얇은 선**을 생성합니다.
    *   불완전하게 느껴집니다 — 대칭적인 차이를 놓칩니다.
*   **개선: 양쪽 고려**
    *   새로운 아이디어: **중심 픽셀(0)**을 포함하고 양쪽을 비교합니다.
    *   템플릿이 [1, 0, -1]이 됩니다.

***

### 113페이지
**더 균형 잡힌 에지 응답을 생성합니다.**
*   **한 방향**(왼쪽에서 오른쪽으로의 차이)만 봅니다.
    *   에지를 감지하지만, **하나의 얇은 선**을 생성합니다.
    *   불완전하게 느껴집니다 — 대칭적인 차이를 놓칩니다.
*   **개선: 양쪽 고려**
    *   새로운 아이디어: **중심 픽셀(0)**을 포함하고 양쪽을 비교합니다.
    *   템플릿이 [1, 0, -1]이 됩니다.

***

### 114페이지
**필터 디자인의 힘**
*   **연산**(합성곱)은 동일하게 유지됩니다.
    *   오직 **필터(커널) 디자인**만 변경됩니다.
    *   그러나 출력 이미지는 매우 다르게 보입니다.
*   필터 디자인은 **어떤 특징**이 강조될지를 제어합니다:
*   이것이 컴퓨터 비전에서 **필터 디자인이 중요한 이유**입니다.

***

### 115페이지
**수평 에지에 동일한 필터 적용하기**
*   이미지에 **수평 에지**(상단 절반 = 50, 하단 절반 = 0)가 있다고 가정합니다.
    *   동일한 [-1, 0, +1] **행 필터**를 적용합니다.

***

### 116페이지
**필터 디자인의 힘**
*   이미지에 **수평 에지**(상단 절반 = 50, 하단 절반 = 0)가 있다고 가정합니다.
    *   동일한 [-1, 0, +1] **행 필터**를 적용합니다.
    *   **결과: 모두 0** — 응답 없음.
    *   **예상 결과**: 이 필터는 **수직 변화**에만 반응합니다.

***

### 117페이지
**수평 에지를 어떻게 탐지하는가?**
*   **필터를 열로 회전:**
    *   이제 왼쪽과 오른쪽 대신 **위아래**의 차이를 측정합니다.

***

### 118페이지
**수평 에지를 어떻게 탐지하는가?**
*   **필터를 열로 회전:**
    *   이제 왼쪽과 오른쪽 대신 **위아래**의 차이를 측정합니다.
    *   이 필터를 적용하면 **수평 에지에서 강한 응답**을 얻습니다.
*   **커널의 방향**이 어떤 유형의 에지를 탐지할지 결정합니다.

***

### 119페이지
**왜 1D에서 2D 필터로 이동하는가?**
*   **비대칭 문제**
    *   1×3 또는 3×1 커널을 사용하면 이미지가 한 방향으로만 축소됩니다 (예: 6×6→4×6).
    *   불균형하게 느껴지고 결과가 덜 자연스러워 보입니다.
*   **노이즈 민감성**
    *   1D 커널은 **오직 한 행 또는 한 열**만 봅니다.
    *   너무 민감함: 단일 노이즈 픽셀이 거짓 에지를 만들 수 있습니다.

***

### 120페이지
**해결책: 2D 커널 (3×3)**
*   이웃 행/열을 함께 고려하는 **3×3 커널**로 확장합니다.
    *   예시: Prewitt 또는 Sobel 커널.
    *   **효과**:
        *   더 **견고함** (노이즈가 평균화됨).
        *   더 **균형 잡힘** (더 넓은 이웃에서 에지를 탐지).

***

### 121페이지
**해결책: 2D 커널 (3×3)**
*   이웃 행/열을 함께 고려하는 **3×3 커널**로 확장합니다.
    *   예시: Prewitt 또는 Sobel 커널.
    *   **효과**:
        *   더 **견고함** (노이즈가 평균화됨).
        *   더 **균형 잡힘** (더 넓은 이웃에서 에지를 탐지).

***

### 122페이지
**방향성 에지 맵**
*   **수평 에지 필터**를 적용하면 → **수평 에지**를 탐지합니다.
*   **수직 에지 필터**를 적용하면 → **수직 에지**를 탐지합니다.
*   각각은 특정 방향의 에지를 강조하는 **에지 맵**을 생성합니다.

***

### 123페이지
**해결책: 2D 커널 (3×3)**
*   **수평 에지 필터**를 적용하면 → **수평 에지**를 탐지합니다.
*   **수직 에지 필터**를 적용하면 → **수직 에지**를 탐지합니다.
*   각각은 특정 방향의 에지를 강조하는 **에지 맵**을 생성합니다.

***

### 124페이지
**수평 및 수직 에지를 넘어서**
*   지금까지: 우리는 순수한 **수평** 및 순수한 **수직** 에지만 보았습니다.
    *   하지만 실제 이미지에서는 에지가 **어떤 방향**으로든 나타날 수 있습니다 — 45°, 30°, 곡선 경계 등.
*   **예시: 45° 에지**
    *   **수평 필터(Gx) 적용**: 강도 변화가 부분적으로 x 방향으로 발생하기 때문에 응답이 강합니다.
    *   **수직 필터(Gy) 적용**: 강도 변화가 y 방향으로도 발생하기 때문에 응답이 또한 강합니다.

***

### 125페이지
**수평 및 수직 에지를 넘어서**
*   지금까지: 우리는 순수한 **수평** 및 순수한 **수직** 에지만 보았습니다.
    *   하지만 실제 이미지에서는 에지가 **어떤 방향**으로든 나타날 수 있습니다 — 45°, 30°, 곡선 경계 등.
*   **예시: 45° 에지**
    *   **수평 필터(Gx) 적용**: 강도 변화가 부분적으로 x 방향으로 발생하기 때문에 응답이 강합니다.
    *   **수직 필터(Gy) 적용**: 강도 변화가 y 방향으로도 발생하기 때문에 응답이 또한 강합니다.

***

### 126페이지
**방향 결합하기**
*   **어떤 방향**에서든 에지를 알고 싶다면, 둘 다 결합해야 합니다.
*   **두 가지 일반적인 방법**:
    *   절댓값의 합: `G = |Gx| + |Gy|`
    *   L2 노름 (그래디언트 크기): `G = sqrt(Gx^2 + Gy^2)`

***

### 127페이지
**방향 결합하기**
*   **어떤 방향**에서든 에지를 알고 싶다면, 둘 다 결합해야 합니다.
*   **두 가지 일반적인 방법**:
    *   절댓값의 합: `G = |Gx| + |Gy|`
    *   L2 노름 (그래디언트 크기): `G = sqrt(Gx^2 + Gy^2)`

***

### 128페이지
**에지 방향 (그래디언트 방향)**
*   에지의 **방향**을 알고 싶다면 어떻게 해야 할까요?
*   x 방향(Gx)과 y 방향(Gy)의 에지 강도를 취하면 이들을 결합할 수 있습니다.
*   비율로 표현함으로써, 에지 방향의 각도를 찾을 수 있습니다. `θ = tan⁻¹(Gy / Gx)`
*   **에지 방향 = 그래디언트에 수직**

***

### 129페이지
**에지 방향 (그래디언트 방향)**
*   에지의 **방향**을 알고 싶다면 어떻게 해야 할까요?
*   x 방향(Gx)과 y 방향(Gy)의 에지 강도를 취하면 이들을 결합할 수 있습니다.
*   비율로 표현함으로써, 에지 방향의 각도를 찾을 수 있습니다. `θ = tan⁻¹(Gy / Gx)`
*   **에지 방향 = 그래디언트에 수직**

***

### 130페이지
**미분으로서의 에지 탐지**
*   에지 필터(Sobel, Prewitt)는 실제로 **미분의 근사치**입니다.
    *   **X 방향 미분**: 왼쪽과 오른쪽 이웃 픽셀 간의 변화.
    *   **Y 방향 미분**: 위쪽과 아래쪽 이웃 픽셀 간의 변화.
*   에지는 강도가 **급격하게 변하는** 곳입니다 → 그래디언트가 큽니다.
*   따라서, 에지 탐지는 **높은 1차 미분**을 가진 곳을 찾는 것입니다.

***

### 131페이지
**Prewitt 필터**
*   수평 Prewitt (수직 에지 탐지)
*   수직 Prewitt (수평 에지 탐지)
*   **아이디어**: 노이즈를 줄이기 위해 **세 개의 행/열에 걸쳐 평균**을 냅니다.

***

### 132페이지
**Prewitt에서 Sobel로**
*   하지만: 중앙 행/열이 더 많은 가중치를 받아야 하지 않을까요?
*   이것이 **Sobel 필터**의 아이디어입니다.
*   Sobel은 중앙 행/열에 가중치(2)를 추가합니다 → 중앙 차이에 더 강한 강조를 둡니다.

***

### 133페이지
**Prewitt 에지 탐지 (결과)**
*   **에지 맵의 경향**:
    *   **더 날카롭고 얇은** 에지
    *   노이즈에 더 민감함
*   간단한 이미지에 좋음
*   자연 이미지에는 덜 견고함

***

### 134페이지
**Sobel 에지 탐지 (결과)**
*   **에지 맵의 경향**:
    *   **더 부드럽고 약간 더 두꺼운** 에지
    *   노이즈에 덜 민감함
*   더 견고하고, 실제 이미지에 더 적합함
*   기본적인 에지 탐지기로 널리 사용됨

***

### 135페이지
**Sobel 에지 탐지 (결과)**
*   **에지 맵의 경향**:
    *   **더 부드럽고 약간 더 두꺼운** 에지
    *   노이즈에 덜 민감함
*   더 견고하고, 실제 이미지에 더 적합함
*   기본적인 에지 탐지기로 널리 사용됨

***

### 136페이지
**Sobel 에지 탐지 (결과)**
*   **에지 맵의 경향**:
    *   **더 부드럽고 약간 더 두꺼운** 에지
    *   노이즈에 덜 민감함
*   더 견고하고, 실제 이미지에 더 적합함
*   기본적인 에지 탐지기로 널리 사용됨

***

### 137페이지
**Sobel 에지 탐지 (결과)**
*   **에지 맵의 경향**:
    *   **더 부드럽고 약간 더 두꺼운** 에지
    *   노이즈에 덜 민감함
*   더 견고하고, 실제 이미지에 더 적합함
*   기본적인 에지 탐지기로 널리 사용됨

***

### 138페이지
**Canny 에지 탐지기 (고급)**
*   **Canny 에지 탐지기**라는 고급 방법도 있습니다.
    *   Canny는 Sobel을 기반으로 하지만, 단순 필터에 대한 **단계별 개선**으로 설계되었습니다.
    *   Canny = 가우시안 스무딩 → Sobel 에지 탐지 → 비최대 억제(NMS) → 임계처리
    *   **목표**: **깨끗하고, 얇고, 연결된** 에지를 생성합니다.
*   **Sobel과 비교**
    *   **Sobel**: "에지가 존재할 수 있는 모든 곳을 보여줘."
        *   에지를 강하게 탐지하지만, 결과는 **두껍고 노이즈가 많습니다.**
    *   **Canny**: "깨끗하고 연결된 정밀한 단일 픽셀 에지를 줘."
        *   여러 개선 사항을 적용하여 → **얇고, 정밀한** 에지를 만듭니다.

***

### 139페이지
**라플라시안 필터란 무엇인가?**
*   Sobel이나 Prewitt 같은 필터가 **단일 방향**(수평 또는 수직)의 변화를 보는 반면, 라플라시안은 **모든 방향을 한 번에** 봅니다.
*   **중심 픽셀**을 주변의 모든 **이웃**과 비교합니다.
    *   중심 픽셀이 주변과 매우 다르면, 라플라시안은 **강한 응답**을 보입니다.
    *   이 때문에, 방향에 관계없이 — 수평, 수직 또는 대각선 — **에지를 탐지하는 데 특히 좋습니다.**
    *   중심 픽셀이 주변 영역과 비교하여 얼마나 **두드러지는지** 측정하는 방법으로 생각할 수 있습니다.

***

### 140페이지
**라플라시안 필터란 무엇인가?**
*   Sobel이나 Prewitt 같은 필터가 **단일 방향**(수평 또는 수직)의 변화를 보는 반면, 라플라시안은 **모든 방향을 한 번에** 봅니다.
*   **중심 픽셀**을 주변의 모든 **이웃**과 비교합니다.
    *   중심 픽셀이 주변과 매우 다르면, 라플라시안은 **강한 응답**을 보입니다.
    *   이 때문에, 방향에 관계없이 — 수평, 수직 또는 대각선 — **에지를 탐지하는 데 특히 좋습니다.**
    *   중심 픽셀이 주변 영역과 비교하여 얼마나 **두드러지는지** 측정하는 방법으로 생각할 수 있습니다.

***

### 141페이지
**라플라시안 필터란 무엇인가?**
*   **8-이웃 버전**은 중심을 **모든 주변 픽셀**과 비교합니다.
    *   이는 8-이웃 라플라시안이 중심과 그 **완전한 이웃** 사이의 차이를 확인한다는 것을 의미합니다.
    *   결과적으로, **에지에 더 민감**할 수 있지만, **노이즈에도 더 민감**할 수 있습니다.
*   **Sobel** = "이미지가 어느 방향으로 변하고 있는가?"
*   **라플라시안** = "이 픽셀 주변에 강한 변화가 있는가?"

***

### 142페이지
**확장: 2차 미분으로서의 라플라시안**
*   1D에서, **1차 미분**은 무언가가 얼마나 빨리 변하는지(기울기)를 알려줍니다.
*   **2차 미분**은 변화 자체가 가장 많이 변하는 곳 — 곡선이 급격하게 구부러지는 곳과 같이 — 을 알려줍니다.
    *   이미지의 에지는 바로 이러한 "급격한 변화"이므로, 라플라시안은 **2D 이미지에서 2차 미분을 근사**하는 방법으로 볼 수 있습니다.
*   **이것이 말이 되는 이유**
    *   Sobel ≈ 1차 미분 (한 방향의 변화율).
    *   라플라시안 ≈ 2차 미분 (변화율 자체가 모든 방향에서 얼마나 변하는지).
    *   이것이 라플라시안이 넓은 밴드에 걸쳐서가 아니라, **에지 바로 위에서 강하게 반응**하는 이유입니다.

***

### 143페이지
**확장: 2차 미분으로서의 라플라시안**
*   1D에서, **1차 미분**은 무언가가 얼마나 빨리 변하는지(기울기)를 알려줍니다.
*   **2차 미분**은 변화 자체가 가장 많이 변하는 곳 — 곡선이 급격하게 구부러지는 곳과 같이 — 을 알려줍니다.
    *   이미지의 에지는 바로 이러한 "급격한 변화"이므로, 라플라시안은 **2D 이미지에서 2차 미분을 근사**하는 방법으로 볼 수 있습니다.
*   **이것이 말이 되는 이유**
    *   Sobel ≈ 1차 미분 (한 방향의 변화율).
    *   라플라시안 ≈ 2차 미분 (변화율 자체가 모든 방향에서 얼마나 변하는지).
    *   이것이 라플라시안이 넓은 밴드에 걸쳐서가 아니라, **에지 바로 위에서 강하게 반응**하는 이유입니다.

***

### 144페이지
**Sobel vs 라플라시안**
*   **라플라시안**
    *   작은 변동에 민감합니다.

***

### 145페이지
**Sobel vs 라플라시안**
*   **영점 교차(Zero-crossing)**
    *   수학적으로, 이것은 강도 전환의 중심 — 실제 경계 — 에 해당합니다.
    *   따라서 이론적으로, 이것은 더 **정확한 에지 위치**를 제공합니다.

***

### 146페이지
**필터 비교**
| 원본 이미지 | Canny | 라플라시안 | Sobel | Prewitt |
| :--- | :--- | :--- | :--- | :--- |
| (이미지) | 얇고, 깨끗하며, 연속적인 에지 (다단계, 가장 정밀) | 모든 방향의 에지, 얇지만 노이즈에 민감 | 더 강한 방향성 에지 (1차 미분) | 단순한 방향성 에지 |
| (이미지) | | | | |
| (이미지) | | | | |

***

### 147페이지
**필터 비교**
| 원본 이미지 | Canny | 라플라시안 | Sobel | Prewitt |
| :--- | :--- | :--- | :--- | :--- |
| (이미지) | 얇고, 깨끗하며, 연속적인 에지 (다단계, 가장 정밀) | 모든 방향의 에지, 얇지만 노이즈에 민감 | 더 강한 방향성 에지 (1차 미분) | 단순한 방향성 에지 |
| (이미지) | | | | |
| (이미지) | | | | |

***

### 148페이지
**스무딩 필터**

***

### 149페이지
**이미지 노이즈란 무엇인가?**
*   **이미지 노이즈**는 픽셀의 밝기나 색상에 원치 않는 **무작위 변동**을 의미합니다.
*   매끄러운 표면이나 깨끗한 에지 대신, 작은 **얼룩, 입자, 또는 정적인 패턴**이 보입니다.
*   이미지를 **덜 선명하게** 보이게 하고 에지 탐지와 같은 알고리즘을 혼란스럽게 할 수 있습니다.

***

### 150페이지
**실제 이미지에 노이즈가 포함된 이유**
*   **센서 한계**: 카메라 센서는 빛(광자)을 전기 신호로 변환하지만, 이 과정에는 항상 무작위 변동이 있습니다.
    *   저조도에서는 신호가 약해져 **노이즈 비율이 높아지고** 더 잘 보이게 됩니다.
*   **전자 간섭**: 카메라나 저장 장치 내부의 회로는 **열 잡음**(열 관련) 및 기타 전자 간섭을 유발합니다.
    *   라디오에서 듣는 정적 잡음과 유사합니다.
*   **전송 및 압축**
    *   무선 전송, 저장 또는 **JPEG 압축** 중에 추가 노이즈나 인공적인 결함이 발생할 수 있습니다.
*   **환경**
    *   렌즈의 먼지, 안개 또는 초점 문제도 최종 이미지에서 노이즈처럼 보일 수 있습니다.

***

### 151페이지
**에지 탐지 전에 스무딩이 중요한 이유**
*   실제 이미지에는 **노이즈**(무작위 강도 변화)가 포함되어 있습니다.
    *   에지 필터(특히 Sobel, Laplacian과 같은 미분)는 노이즈를 마치 에지인 것처럼 **증폭**시킵니다.
    *   먼저 스무딩을 통해 노이즈를 줄이면 에지 탐지기가 **의미 있는 경계**에 집중할 수 있습니다.
*   **작은 디테일 / 질감 줄이기**
    *   실제 이미지에는 많은 미세한 질감(예: 피부 모공, 직물 패턴)이 있습니다.
    *   Sobel이나 Laplacian과 같은 에지 탐지기는 이 모든 것을 에지로 처리합니다.
    *   스무딩은 **주요 객체 경계만 남도록** 미세한 디테일을 제거합니다.

***

### 152페이지
**에지 탐지 전에 스무딩이 중요한 이유**
*   실제 이미지에는 **노이즈**(무작위 강도 변화)가 포함되어 있습니다.
    *   에지 필터(특히 Sobel, Laplacian과 같은 미분)는 노이즈를 마치 에지인 것처럼 **증폭**시킵니다.
    *   먼저 스무딩을 통해 노이즈를 줄이면 에지 탐지기가 **의미 있는 경계**에 집중할 수 있습니다.
*   **작은 디테일 / 질감 줄이기**
    *   실제 이미지에는 많은 미세한 질감(예: 피부 모공, 직물 패턴)이 있습니다.
    *   Sobel이나 Laplacian과 같은 에지 탐지기는 이 모든 것을 에지로 처리합니다.
    *   스무딩은 **주요 객체 경계만 남도록** 미세한 디테일을 제거합니다.

***

### 153페이지
**스무딩 필터란 무엇인가?**
*   스무딩 필터는 각 픽셀을 그 이웃으로부터 **평균(또는 다른 방식으로 결합된)** 값으로 대체합니다.
    *   효과는 급격한 변화를 줄여 이미지를 "흐릿하게" 또는 "부드럽게" 보이게 하는 것입니다.
    *   **목표**: 전반적인 구조를 유지하면서 작은 변동(노이즈, 질감)을 제거합니다.

***

### 154페이지
**스무딩 필터란 무엇인가?**
*   스무딩 필터는 각 픽셀을 그 이웃으로부터 **평균(또는 다른 방식으로 결합된)** 값으로 대체합니다.
    *   효과는 급격한 변화를 줄여 이미지를 "흐릿하게" 또는 "부드럽게" 보이게 하는 것입니다.
    *   **목표**: 전반적인 구조를 유지하면서 작은 변동(노이즈, 질감)을 제거합니다.

***

### 155페이지
**스무딩 필터란 무엇인가?**
*   스무딩 필터는 각 픽셀을 그 이웃으로부터 **평균(또는 다른 방식으로 결합된)** 값으로 대체합니다.
    *   효과는 급격한 변화를 줄여 이미지를 "흐릿하게" 또는 "부드럽게" 보이게 하는 것입니다.
    *   **목표**: 전반적인 구조를 유지하면서 작은 변동(노이즈, 질감)을 제거합니다.

***

### 156페이지
**스무딩을 위해 어떤 커널이 필요한가?**
*   이 스무딩 효과를 원한다면, 어떤 종류의 **필터(커널)**를 사용해야 할까요?
    *   특히, **가중치 값**은 어떻게 보여야 할까요?

***

### 157페이지
**평균 필터링**
*   **평균 필터**: 모든 이웃 픽셀이 동일한 가중치를 가집니다.
    *   모든 이웃 픽셀이 동일한 가중치를 가집니다.
*   **아이디어**: 각 픽셀을 그 이웃의 평균으로 대체합니다.
    *   **커널**: 모든 가중치는 동일하며, 그 합은 1입니다.
    *   **예시 (3×3 커널)**: 이는 3×3 이웃의 각 픽셀이 동등하게 (1/9) 기여한다는 것을 의미합니다.
    *   **효과**: 작은 변동(노이즈 또는 미세 질감)을 부드럽게 하지만, 에지가 흐려질 수 있습니다.

***

### 158페이지
**평균 필터링**
*   **평균 필터**: 모든 이웃 픽셀이 동일한 가중치를 가집니다.
    *   모든 이웃 픽셀이 동일한 가중치를 가집니다.
*   **아이디어**: 각 픽셀을 그 이웃의 평균으로 대체합니다.
    *   **커널**: 모든 가중치는 동일하며, 그 합은 1입니다.
    *   **예시 (3×3 커널)**: 이는 3×3 이웃의 각 픽셀이 동등하게 (1/9) 기여한다는 것을 의미합니다.
    *   **효과**: 작은 변동(노이즈 또는 미세 질감)을 부드럽게 하지만, 에지가 흐려질 수 있습니다.

***

### 159페이지
**평균 필터링**
*   **3×3 평균 필터**는 가장 간단한 형태입니다 (바로 이웃을 봅니다).
    *   하지만: "항상 한 픽셀 떨어진 곳만 보는 것이 적절한가?"
    *   실제로는 더 큰 커널(예: 5×5, 7×7, …)도 사용됩니다.
*   **더 큰 커널**은 더 넓은 이웃을 고려하여 더 강한 스무딩 효과를 줍니다.
    *   하지만 단점은 **더 많은 디테일과 에지가 흐려진다**는 것입니다.
    *   따라서 커널 크기의 선택은 원하는 스무딩 정도와 보존하고 싶은 디테일의 양에 따라 달라집니다.

***

### 160페이지
**평균 필터링**
*   **3×3 평균 필터**는 가장 간단한 형태입니다 (바로 이웃을 봅니다).
    *   하지만: "항상 한 픽셀 떨어진 곳만 보는 것이 적절한가?"
    *   실제로는 더 큰 커널(예: 5×5, 7×7, …)도 사용됩니다.
*   **더 큰 커널**은 더 넓은 이웃을 고려하여 더 강한 스무딩 효과를 줍니다.
    *   하지만 단점은 **더 많은 디테일과 에지가 흐려진다**는 것입니다.
    *   따라서 커널 크기의 선택은 원하는 스무딩 정도와 보존하고 싶은 디테일의 양에 따라 달라집니다.

***

### 161페이지
**평균 필터링**
*   **3×3 평균 필터**는 가장 간단한 형태입니다 (바로 이웃을 봅니다).
    *   하지만: "항상 한 픽셀 떨어진 곳만 보는 것이 적절한가?"
    *   실제로는 더 큰 커널(예: 5×5, 7×7, …)도 사용됩니다.
*   **더 큰 커널**은 더 넓은 이웃을 고려하여 더 강한 스무딩 효과를 줍니다.
    *   하지만 단점은 **더 많은 디테일과 에지가 흐려진다**는 것입니다.
    *   따라서 커널 크기의 선택은 원하는 스무딩 정도와 보존하고 싶은 디테일의 양에 따라 달라집니다.

***

### 162페이지
**평균 필터링**
*   **3×3 평균 필터**는 가장 간단한 형태입니다 (바로 이웃을 봅니다).
    *   하지만: "항상 한 픽셀 떨어진 곳만 보는 것이 적절한가?"
    *   실제로는 더 큰 커널(예: 5×5, 7×7, …)도 사용됩니다.
*   **더 큰 커널**은 더 넓은 이웃을 고려하여 더 강한 스무딩 효과를 줍니다.
    *   하지만 단점은 **더 많은 디테일과 에지가 흐려진다**는 것입니다.
    *   따라서 커널 크기의 선택은 원하는 스무딩 정도와 보존하고 싶은 디테일의 양에 따라 달라집니다.

***

### 163페이지
**왜 이미지 크기가 변해야 하는가?**
*   잠깐만요 — 우리는 방금 이미지를 흐리게 만들었습니다. 왜 이미지 자체가 줄어들었을까요?
    *   필터링은 픽셀 값을 변경하는 것이지, 이미지 크기를 변경하는 것이 아닙니다… 그런데 왜 출력이 더 작을까요?
    *   우리가 한 것이 이웃을 평균내는 것이었다면, 이미지는 같은 크기를 유지해야 하지 않을까요? 사라진 픽셀은 어디로 갔을까요?
*   **가장자리를 보세요**: 우리 필터는 거기서 무엇을 해야 할지 모릅니다. 그 픽셀들을 버려야 할까요, 아니면 다른 해결책을 찾아야 할까요?

***

### 164페이지
**왜 이미지 크기가 변해야 하는가?**
*   잠깐만요 — 우리는 방금 이미지를 흐리게 만들었습니다. 왜 이미지 자체가 줄어들었을까요?
    *   필터링은 픽셀 값을 변경하는 것이지, 이미지 크기를 변경하는 것이 아닙니다… 그런데 왜 출력이 더 작을까요?
    *   우리가 한 것이 이웃을 평균내는 것이었다면, 이미지는 같은 크기를 유지해야 하지 않을까요? 사라진 픽셀은 어디로 갔을까요?
*   **가장자리를 보세요**: 우리 필터는 거기서 무엇을 해야 할지 모릅니다. 그 픽셀들을 버려야 할까요, 아니면 다른 해결책을 찾아야 할까요?
    *   지금까지 우리 필터는 단지 그 **경계 픽셀들을 버렸기** 때문에 출력 이미지가 더 작아진 것입니다.

***

### 165페이지
**패딩 소개**
*   그렇다면 경계에서 픽셀을 잃는 것을 어떻게 피할 수 있을까요?
    *   우리는 **패딩**을 사용할 수 있습니다 — 필터링 전에 경계 주위에 추가 픽셀을 추가하는 것입니다.
*   **가장 간단한 방법: 제로 패딩**
    *   이미지 외부에 0을 추가합니다.
    *   이제 모서리 픽셀조차도 필터를 위한 충분한 이웃을 가집니다.
    *   출력은 입력과 **같은 크기**를 유지합니다.

***

### 166페이지
**패딩 소개**
*   그렇다면 경계에서 픽셀을 잃는 것을 어떻게 피할 수 있을까요?
    *   우리는 **패딩**을 사용할 수 있습니다 — 필터링 전에 경계 주위에 추가 픽셀을 추가하는 것입니다.
*   **가장 간단한 방법: 제로 패딩**
    *   이미지 외부에 0을 추가합니다.
    *   이제 모서리 픽셀조차도 필터를 위한 충분한 이웃을 가집니다.
    *   출력은 입력과 **같은 크기**를 유지합니다.

***

### 167페이지
**패딩 소개**
*   그렇다면 경계에서 픽셀을 잃는 것을 어떻게 피할 수 있을까요?
    *   우리는 **패딩**을 사용할 수 있습니다 — 필터링 전에 경계 주위에 추가 픽셀을 추가하는 것입니다.
*   **가장 간단한 방법: 제로 패딩**
    *   이미지 외부에 0을 추가합니다.
    *   이제 모서리 픽셀조차도 필터를 위한 충분한 이웃을 가집니다.
    *   출력은 입력과 **같은 크기**를 유지합니다.

***

### 168페이지
**패딩 소개**
*   그렇다면 경계에서 픽셀을 잃는 것을 어떻게 피할 수 있을까요?
    *   우리는 **패딩**을 사용할 수 있습니다 — 필터링 전에 경계 주위에 추가 픽셀을 추가하는 것입니다.
*   **가장 간단한 방법: 제로 패딩**
    *   이미지 외부에 0을 추가합니다.
    *   이제 모서리 픽셀조차도 필터를 위한 충분한 이웃을 가집니다.
    *   출력은 입력과 **같은 크기**를 유지합니다.

***

### 169페이지
**제로 패딩 크기 규칙**
*   **k × k 커널**의 경우, 각 면(상, 하, 좌, 우)에 **(k-1)/2** 픽셀만큼 패딩해야 합니다.
*   **예시**:
    *   **3×3 커널** → 각 면에 1 픽셀 패딩.
    *   **5×5 커널** → 각 면에 2 픽셀 패딩.
    *   **7×7 커널** → 각 면에 3 픽셀 패딩.
*   이는 필터가 크기 손실 없이 경계 픽셀을 포함한 모든 원본 픽셀에 중앙에 위치할 수 있도록 보장합니다.

***

### 170페이지
**제로 패딩 너머 설명하기**
*   "패딩에 항상 0을 사용해야 하나요? 이미지 경계에서 부자연스러워 보일 수 있습니다."
    *   제로 패딩은 가장 간단한 방법이지만, 유일한 방법은 아닙니다.
*   **일반적인 대안**:
    *   **복제 패딩** – 가장자리 픽셀 값을 바깥쪽으로 복사합니다.
        *   0보다 더 자연스러워 보이고, 어두운 경계를 피합니다.
    *   **반사 (미러) 패딩** – 경계에서 이미지 내용을 미러링합니다.
        *   경계에서 기존 이미지 내용을 그대로 반사합니다. 컴퓨터 비전 라이브러리에서 자주 사용됩니다.
        *   이웃을 실제 이미지와 더 일관성 있게 유지합니다.

***

### 171페이지
**제로 패딩 너머 설명하기**
*   **반사 (미러) 패딩** – 경계에서 이미지 내용을 미러링합니다.
    *   경계에서 기존 이미지 내용을 그대로 반사합니다. 컴퓨터 비전 라이브러리에서 자주 사용됩니다.
    *   이웃을 실제 이미지와 더 일관성 있게 유지합니다.

***

### 172페이지
**제로 패딩 너머 설명하기**
*   **반사 (미러) 패딩** – 경계에서 이미지 내용을 미러링합니다.
    *   경계에서 기존 이미지 내용을 그대로 반사합니다. 컴퓨터 비전 라이브러리에서 자주 사용됩니다.
    *   이웃을 실제 이미지와 더 일관성 있게 유지합니다.
*   원본 이미지는 어디에 있나요?

***

### 173페이지
**제로 패딩 너머 설명하기**
*   **반사 (미러) 패딩** – 경계에서 이미지 내용을 미러링합니다.
    *   경계에서 기존 이미지 내용을 그대로 반사합니다. 컴퓨터 비전 라이브러리에서 자주 사용됩니다.
    *   이웃을 실제 이미지와 더 일관성 있게 유지합니다.
*   원본 이미지는 어디에 있나요?

***

### 174페이지
**가우시안 필터링**
*   **중심이 가장 큰 가중치를 가지며, 멀어질수록 점차 가중치가 작아집니다.**
    *   **커널**: 가중치는 가우시안 분포(종 모양)를 따르며, 중심이 가장 큰 가중치를 가집니다.
    *   **예시 (3×3 커널)**: 이는 중심 픽셀이 가장 많이 (4/16) 기여하고, 인접 픽셀은 다소 적게 (2/16), 모서리는 가장 적게 (1/16) 기여한다는 것을 의미합니다.
    *   **효과**: 평균 필터링보다 더 부드러운 결과를 생성하면서 **에지를 더 잘 보존**하고 더 자연스러워 보입니다.

***

### 175페이지
**가우시안 필터링**
*   **중심이 가장 큰 가중치를 가지며, 멀어질수록 점차 가중치가 작아집니다.**
    *   **커널**: 가중치는 가우시안 분포(종 모양)를 따르며, 중심이 가장 큰 가중치를 가집니다.
    *   **예시 (3×3 커널)**: 이는 중심 픽셀이 가장 많이 (4/16) 기여하고, 인접 픽셀은 다소 적게 (2/16), 모서리는 가장 적게 (1/16) 기여한다는 것을 의미합니다.
    *   **효과**: 평균 필터링보다 더 부드러운 결과를 생성하면서 **에지를 더 잘 보존**하고 더 자연스러워 보입니다.

***

### 176페이지
**왜 가우시안 필터링을 사용하는가 (동일 가중치 대신)?**
*   **평균 필터 (동일 가중치)**:
    *   간단하고 계산하기 쉽습니다.
    *   하지만 멀리 떨어진 이웃까지도 **모든 이웃을 동일하게** 취급합니다.
    *   이는 종종 에지를 너무 많이 흐리게 하고, 결과가 "부자연스러워" 보일 수 있습니다.
*   **가우시안 필터 (중심 가중)**:
    *   **중심 픽셀에 더 높은 가중치**를, 더 먼 이웃에는 낮은 가중치를 부여합니다.
    *   자연에서 노이즈 감소가 작동하는 방식을 모방합니다 (가까운 픽셀이 보통 더 상관관계가 높습니다).
    *   균일한 평균화에 비해 에지를 더 잘 보존합니다.
    *   더 부드럽고 시각적으로 더 만족스러운 결과를 생성합니다.

***

### 177페이지
**더 큰 커널을 사용한 가우시안 필터**
*   **5×5 가우시안 커널**
    *   더 넓은 커널은 각 픽셀 주변의 더 큰 이웃을 고려합니다.
    *   이는 더 강한 스무딩과 더 나은 노이즈 억제로 이어집니다, 더 많은 픽셀이 가중 평균에 기여하기 때문입니다.
    *   그러나 에지와 미세한 디테일은 3×3에 비해 더 흐려질 수 있습니다.

***

### 178페이지
**더 큰 커널을 사용한 가우시안 필터**
*   **7×7 및 더 큰 커널**
    *   커널 크기가 더 커짐에 따라, 필터는 훨씬 더 넓은 맥락을 고려합니다.
    *   이는 고주파 노이즈를 줄이거나 매우 부드러운 이미지를 만드는 데 유용할 수 있지만, 미세한 질감과 에지 선명도를 잃는 대가가 따릅니다.
    *   계산 비용도 커널 크기와 함께 증가합니다.

***

### 179페이지
**시각적 비교**
*   **3×3 가우시안 필터**: 약간의 노이즈 감소. / 에지, 디테일은 대부분 보존됨.
*   **5×5 가우시안 필터**: 더 강한 노이즈 억제. / 미세 질감이 감소됨.
*   **7×7 가우시안 필터**: 매우 강한 스무딩. / 많은 에지와 디테일이 손실됨.

***

### 180페이지
**평균화에서 중간값 필터링으로**
*   **평균 / 가우시안 필터링**
    *   이웃 픽셀 값을 결합(혼합) → 가중 평균.
    *   노이즈 감소에 효과적.
    *   하지만: 필연적으로 **에지와 미세 디테일의 흐림**을 유발합니다.
*   **질문: 에지를 너무 많이 흐리지 않으면서 노이즈를 줄일 수 있을까?**

***

### 181페이지
**평균화에서 중간값 필터링으로**
*   **중간값 필터링 (비선형)**
    *   평균을 내는 대신, 이웃을 **정렬하고 중간값**을 취합니다.
    *   **소금-후추 노이즈**에 강하고 에지를 더 잘 보존합니다.
    *   평균 기반 필터의 대안을 제공합니다.

***

### 182페이지
**평균화에서 중간값 필터링으로**
*   **중간값 필터링 (비선형)**
    *   평균을 내는 대신, 이웃을 **정렬하고 중간값**을 취합니다.
    *   **소금-후추 노이즈**에 강하고 에지를 더 잘 보존합니다.
    *   평균 기반 필터의 대안을 제공합니다.

***

### 183페이지
**평균화에서 중간값 필터링으로**
*   **중간값 필터링 (비선형)**
    *   평균을 내는 대신, 이웃을 **정렬하고 중간값**을 취합니다.
    *   **소금-후추 노이즈**에 강하고 에지를 더 잘 보존합니다.
    *   평균 기반 필터의 대안을 제공합니다.

***

### 184페이지
**가우시안 / 평균 필터 vs 중간값 필터**
*   **평균 / 가우시안 필터링**
    *   커널의 각 위치에는 고정된 가중치가 있습니다.
    *   출력 픽셀은 입력 픽셀 값의 **가중 합**으로 계산됩니다.
    *   이것은 **선형 연산**입니다. → 합성곱으로 표현될 수 있습니다.
*   **중간값 필터**
    *   커널에 **가중치 개념이 없습니다.**
    *   커널의 픽셀 값은 단순히 **정렬**되고, **중간값이 선택**됩니다.
    *   이것은 **비선형 연산**입니다. → 가중 합이 아니므로 합성곱으로 작성할 수 없습니다.

***

### 185페이지
**시각적 비교**
*   평균 vs 가우시안 vs 중간값

***

### 186페이지
**요약**
*   평균 vs 가우시안 vs 중간값

| 필터 | 연산 유형 | 작동 방식 | 강점 | 약점 |
| :--- | :--- | :--- | :--- | :--- |
| **평균 (Box)** | 선형 | 모든 이웃에 동일한 가중치 (단순 평균) | 매우 간단하고 빠름 | 강한 흐림, 에지 보존 불량 |
| **가우시안** | 선형 | 가우시안 분포를 사용한 가중 평균 | 자연스러운 스무딩, 좋은 노이즈 감소 | 여전히 에지를 흐리게 함, 더 많은 계산 |
| **중간값** | 비선형 | 이웃을 정렬하고 중간값 선택 | 소금-후추 노이즈에 탁월, 에지 보존 | 평균보다 비용이 비쌈, 가우시안 노이즈에는 덜 효과적 |

***

### 187페이지
**스무딩에서 에지 탐지로**
*   우리는 **스무딩 필터**(평균, 가우시안, 중간값)를 배웠습니다.
    *   **목적**: 노이즈 감소, 작은 변동 제거.
*   우리는 또한 **에지 탐지 필터**(Sobel, Laplacian)를 공부했습니다.
    *   **목적**: 급격한 변화 강조, 구조 탐지.
*   **문제**: 에지 탐지기는 노이즈에 매우 민감합니다.

***

### 188페이지
**에지 탐지 전에 스무딩을 적용하세요.**
*   → 이는 더 깨끗한 에지 맵을 생성하고 주요 구조를 강조합니다.

***

### 189페이지
**컴퓨터 비전의 기초**
2025년 2학기
- Kwang-Hyun Uhm –
인공지능학과

***

### 190페이지
**강의 2:**
**OpenCV란 무엇인가?**
*   OpenCV란 무엇인가?
*   이미지 I/O
*   이미지 데이터 접근
*   이미지에 대한 기본 연산

***

### 191페이지
**OpenCV란 무엇인가?**
*   **OpenCV (Open Source Computer Vision Library)**는 컴퓨터 비전 및 이미지 처리 작업을 위해 설계된 오픈 소스 라이브러리입니다.
*   **주요 특징**
    *   **오픈 소스**: 누구나 무료로 사용할 수 있습니다.
    *   **다중 언어 지원**: C++, Python, Java, MATLAB (Python이 가장 인기 있음).
    *   **크로스 플랫폼**: Windows, Linux, macOS, Android, iOS에서 작동합니다.
    *   **고성능**: 최적화된 C/C++로 작성되어 실시간 비전 작업에 적합합니다.
*   **왜 OpenCV를 배우는가?**
    *   **기초**: 이미지를 처리하고 조작하는 방법을 가르쳐줍니다.
    *   **응용**: 의료 영상, 자율 주행 자동차, CCTV 감시, AR/VR, 로보틱스와 같은 분야에서 사용됩니다.
    *   **프로토타이핑**: 모든 것을 처음부터 코딩하지 않고도 비전 알고리즘을 신속하게 테스트하고 구현할 수 있습니다.

***

### 192페이지
**OpenCV란 무엇인가?**
*   **OpenCV (Open Source Computer Vision Library)**는 컴퓨터 비전 및 이미지 처리 작업을 위해 설계된 오픈 소스 라이브러리입니다.
*   **주요 기능**
    *   **이미지 처리**: 필터링, 블러링, 색 공간 변환, 히스토그램, 에지 탐지.
    *   **비디오 처리**: 카메라에서 읽기, 객체 추적, 비디오 스트림 분석.
    *   **형태학적 연산**: 침식, 팽창, 열림, 닫힘.
    *   **특징 탐지 및 매칭**: 코너 탐지, SIFT, SURF, ORB 등.
    *   **머신/딥러닝 모듈**: 얼굴 인식, 객체 탐지, 분류, DNN 추론.

***

### 193페이지
**OpenCV 설치하기**
*   **Python 환경**
    *   OpenCV는 Python 패키지로 제공됩니다.
    *   Python ≥ 3.7을 사용하고 있는지 확인하세요.
    *   Anaconda, venv 또는 Colab에서 직접 작동합니다.
*   **설치 명령어**
    *   가장 일반적인 설치 방법은 pip를 통하는 것입니다:
    *   `pip install opencv-python`
    *   이는 표준 패키지(cv2 모듈)를 설치합니다.
*   **설치 확인**
    *   설치 후, 버전을 확인하세요:
    *   `import cv2`
    *   `print(cv2.__version__)`

***

### 194페이지
**왜 OpenCV에 가상 환경(Anaconda / venv)을 사용하는가?**
*   **의존성 및 버전 관리**
    *   Python 라이브러리는 종종 버전 충돌이 발생합니다.
    *   예시: OpenCV 4.9는 최신 TensorFlow와 잘 작동하지만, 특정 PyTorch 버전과 충돌할 수 있습니다.
    *   가상 환경은 각 프로젝트의 패키지를 **격리**하여 충돌을 방지합니다.
*   **재현성**
    *   연구 및 과제에서 "내 컴퓨터에서는 되는데" 문제가 흔합니다.
    *   가상 환경을 사용하면 패키지 목록(requirements.txt 또는 environment.yml)을 내보낼 수 있습니다.
    *   다른 사람들은 **정확히 동일한 환경을 재현**할 수 있습니다.
*   **깨끗한 설정**
    *   환경이 없으면: 모든 패키지가 전역적으로 설치되어 → 지저분해집니다.
    *   환경이 있으면: 각 프로젝트는 자신만의 깨끗한 패키지 공간을 가집니다.

***

### 195페이지
**왜 OpenCV에 가상 환경(Anaconda / venv)을 사용하는가?**
*   **Anaconda**
    *   **장점**:
        *   많은 데이터 과학 및 ML 패키지가 사전 설치되어 제공됩니다.
        *   환경 관리가 쉽습니다 (conda create -n myenv python=3.10).
        *   GUI 도구(Anaconda Navigator)가 있습니다.
    *   **단점**: 크기가 크고 설치가 무겁습니다.
*   **Anaconda 설치하기**
    *   **다운로드**
        *   공식 사이트로 이동: https://www.anaconda.com/download
        *   OS(Windows, macOS, Linux)에 맞는 설치 프로그램을 선택합니다.
        *   Python 3.x 버전을 선택합니다 (최신 버전도 괜찮습니다).
    *   **설치 단계**
        *   **Windows/macOS**: 설치 프로그램을 실행하고 → 기본 설정을 따릅니다.
        *   **Linux**: 제공된 셸 스크립트를 실행합니다.
        *   설치 후, Anaconda Navigator(GUI)와 conda(명령줄 도구)를 사용할 수 있습니다.
    *   **설치 확인**
        *   터미널(또는 Windows의 Anaconda Prompt)을 열고 다음을 입력합니다:
        *   `conda --version`
        *   올바르게 설치되었다면 버전 번호가 표시됩니다.

***

### 196페이지
**왜 OpenCV에 가상 환경(Anaconda / venv)을 사용하는가?**
*   **Anaconda**
    *   **장점**:
        *   많은 데이터 과학 및 ML 패키지가 사전 설치되어 제공됩니다.
        *   환경 관리가 쉽습니다 (conda create -n myenv python=3.10).
        *   GUI 도구(Anaconda Navigator)가 있습니다.
    *   **단점**: 크기가 크고 설치가 무겁습니다.

(코드 스니펫)
`# Python 3.10으로 "cv_env"라는 새 환경 생성`
`conda create -n cv_env python=3.10`

`# 환경 활성화`
`conda activate cv_env`

`# OpenCV 설치`
`pip install opencv-python`

`conda activate cv_env`

`(cv_env) C:\Users\student>`

`conda deactivate`

`(base) C:\Users\student>`

***

### 197페이지
**왜 OpenCV에 가상 환경(Anaconda / venv)을 사용하는가?**
*   가상 환경에 OpenCV 설치하기
*   환경 생성 및 활성화 (Anaconda 예시)
*   OpenCV 설치
*   설치 확인

(코드 스니펫)
`(cv_env) C:\Users\student>`
`pip install opencv-python`
`python`
`import cv2`
`print(cv2.__version__)`
`4.9.0`
`exit()`

***

### 198페이지
**왜 OpenCV에 가상 환경(Anaconda / venv)을 사용하는가?**
*   **venv (Python ≥ 3.3에 내장)**
    *   **장점**:
        *   가볍고, 추가 설치가 필요 없습니다.
        *   서버 및 최소한의 설정에 좋습니다.
    *   **단점**: 대규모 ML 워크플로우에는 Anaconda보다 덜 편리합니다.
*   **설치 및 사용법**:

(코드 스니펫)
`# 새 가상 환경 생성`
`python -m venv cv_env`

`# 활성화`
`# Windows:`
`cv_env\Scripts\activate`
`# macOS/Linux:`
`source cv_env/bin/activate`

`# OpenCV 설치`
`pip install opencv-python`

***

### 199페이지
**왜 OpenCV에 가상 환경(Anaconda / venv)을 사용하는가?**
*   가상 환경을 사용하여 충돌을 피하고, 프로젝트를 체계적으로 유지하며, 재현성을 보장하세요. Anaconda는 초보자에게 사용자 친화적이며, venv는 가볍고 내장되어 있습니다.
*   **학생들을 위한 추천**
    *   **초보자 / 데이터 과학 학습자** → Anaconda (더 쉽고 신뢰성 있음).
    *   **가벼운 설정 / 서버** → venv.
    *   **Google Colab** → 가상 환경 필요 없음 (각 세션이 이미 격리되어 있음).

***

### 200페이지
**예제 1: 이미지 읽고 표시하기**
*   1. 이미지 불러오기

(코드 스니펫)
`import cv2`

`# 이미지 읽기 (기본값: 컬러, BGR 순서)`
`img = cv2.imread("example.jpg")`

`# 이미지가 성공적으로 로드되었는지 확인`
`if img is None:`
`    print("오류: 이미지를 읽을 수 없습니다.")`
`else:`
`    print("이미지 로드됨, 형태:", img.shape)`

***

### 201페이지
**예제 1: 이미지 읽고 표시하기**
*   2. 이미지 표시하기
    *   **로컬 환경 (GUI 지원)**
    *   이미지를 보여주는 팝업 창이 열립니다.

(코드 스니펫)
`cv2.imshow("My Image", img)`
`cv2.waitKey(0)          # 키가 눌릴 때까지 대기`
`cv2.destroyAllWindows() # 창 닫기`

*   **Colab 또는 서버 (GUI 지원 없음)**
    *   **Matplotlib**을 사용해야 합니다:
    *   이미지는 노트북 셀 내부에 표시됩니다.

(코드 스니펫)
`import matplotlib.pyplot as plt`

`# 올바른 색상 표시를 위해 BGR → RGB 변환`
`img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`

`plt.imshow(img_rgb)`
`plt.axis("off")`
`plt.show()`

***

### 202페이지
**예제 1: 이미지 읽고 표시하기**
*   2. 이미지 표시하기
    *   **로컬 환경 (GUI 지원)**
    *   이미지를 보여주는 팝업 창이 열립니다.

(코드 스니펫)
`cv2.imshow("My Image", img)`
`cv2.waitKey(0)          # 키가 눌릴 때까지 대기`
`cv2.destroyAllWindows() # 창 닫기`

(코드 스니펫)
`import matplotlib.pyplot as plt`

`# 올바른 색상 표시를 위해 BGR → RGB 변환`
`img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)`

`plt.imshow(img_rgb)`
`plt.axis("off")`
`plt.show()`

***

### 203페이지
**이미지 배열**
*   **NumPy 배열로서의 이미지 (3 또는 1 채널)**
    *   **높이** = 행의 수 (수직 픽셀)
    *   **너비** = 열의 수 (수평 픽셀)
    *   **채널** = 3 (컬러 이미지의 경우 B, G, R)

(코드 스니펫)
`import cv2`

`img = cv2.imread("example.jpg")`
`print(type(img))      # <class 'numpy.ndarray'>`
`print(img.shape)      # (높이, 너비, 채널), 예: (480, 640, 3)`

*   **인덱싱으로 픽셀 접근**
    *   OpenCV는 BGR 순서를 사용하므로, 인덱스 [행, 열, 0] = 파란색입니다.

(코드 스니펫)
`# 행=100, 열=200에서 픽셀 접근`
`pixel = img[100, 200]`
`print(pixel)          # 예: [B, G, R] - [123, 45, 67]`

`# 해당 픽셀의 파란색 채널에만 접근`
`blue_value = img[100, 200, 0]`
`green_value = img[100, 200, 1]`
`red_value = img[100, 200, 2]`

`print("파랑:", blue_value, "초록:", green_value, "빨강:", red_value)`

***

### 204페이지
**이미지 배열**
*   **픽셀 값 수정**
    *   이는 픽셀 값(BGR)을 직접 조작합니다.

(코드 스니펫)
`# 한 픽셀을 흰색으로 변경`
`img[100, 200] = [255, 255, 255]`

`# 한 영역(50x50 정사각형)을 빨간색으로 변경`
`img[50:100, 50:100] = [0, 0, 255]`

***

### 205페이지
**이미지 배열**
*   **슬라이싱을 이용한 관심 영역(ROI)**
    *   2D NumPy 배열을 슬라이싱하는 것과 같지만, 여기서는 모든 채널을 유지할 수도 있습니다.

(코드 스니펫)
`# 왼쪽 상단 모서리에서 100x100 패치 추출`
`roi = img[0:100, 0:100]`
`cv2.imshow("ROI", roi)`
`cv2.waitKey(0)`
`cv2.destroyAllWindows()`

*   **(ROI) 이미지 저장 - cv2.imwrite()**
    *   확장자를 변경하여 다른 형식으로 저장할 수 있습니다:
    *   "roi_output.png"
    *   "roi_output.bmp"

(코드 스니펫)
`# 잘라낸 이미지 저장`
`cv2.imwrite("roi_output.jpg", roi)`

***

### 206페이지
**OpenCV에서의 그레이스케일 변환**
*   **cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)**를 사용합니다.
    *   그레이스케일 이미지는 (높이, 너비) 형태를 가집니다 (채널 차원 없음).
    *   cv2.imshow는 컬러와 그레이스케일 이미지를 별도의 창에 모두 표시합니다.

(코드 스니펫)
`import cv2`

`# 컬러 이미지 불러오기`
`img = cv2.imread("example.jpg")`

`# BGR → 그레이스케일 변환`
`gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)`

`# 이미지 표시`
`cv2.imshow("Original", img)`
`cv2.imshow("Grayscale", gray)`

`cv2.waitKey(0)`
`cv2.destroyAllWindows()`

`# 그레이스케일 이미지 저장`
`cv2.imwrite("example_gray.jpg", gray)`

***

### 207페이지
**수동 그레이스케일 변환**
*   컬러 이미지는 채널 [B, G, R]을 가진 (H, W, 3)입니다.
    *   일반적인 공식을 사용하여 그레이스케일을 근사할 수 있습니다.
    *   `Gray=0.299R+0.587G+0.114B`

(코드 스니펫)
`import cv2`
`import numpy as np`

`# 컬러 이미지 불러오기 (BGR)`
`img = cv2.imread("example.jpg")`

`# B, G, R 채널 추출`
`B = img[:, :, 0]`
`G = img[:, :, 1]`
`R = img[:, :, 2]`

`# 그레이스케일을 위한 가중 합 (표준 휘도 공식)`
`gray = 0.114*B + 0.587*G + 0.299*R`
`gray = gray.astype(np.uint8)`

`# 그레이스케일 이미지 표시`
`cv2.imshow("Grayscale (manual)", gray)`
`cv2.waitKey(0)`
`cv2.destroyAllWindows()`

`# 형태 출력`
`print("컬러 이미지 형태:", img.shape)          # 예: (480, 640, 3)`
`print("그레이스케일 이미지 형태:", gray.shape)  # 예: (480, 640)`

`# 그레이스케일 이미지 저장`
`cv2.imwrite("manual_gray.jpg", gray)`

***

### 208페이지
**Colab에서의 옵션**
*   **Colab으로 직접 업로드**
    *   `from google.colab import files`
    *   `uploaded = files.upload()`
    *   이렇게 하면 파일이 `/content/`에 저장됩니다.
    *   그런 다음 직접 읽을 수 있습니다:
    *   `img = cv2.imread("example.jpg")`
*   **Google 드라이브 마운트**
    *   이미지가 Google 드라이브에 저장된 경우:
    *   `from google.colab import drive`
    *   `drive.mount('/content/drive')`
    *   그런 다음 전체 경로를 제공합니다:
    *   `img = cv2.imread("/content/drive/MyDrive/example.jpg")`

***

### 209페이지
**이진 이미지 (임계처리)**
*   **cv2.threshold(src, thresh, maxval, type)**
    *   **src**: 입력 그레이스케일 이미지 / **thresh**: 임계값 / **maxval**: 픽셀 > 임계값일 때 할당할 값 (보통 255)
    *   **type**: 임계처리 유형 (cv2.THRESH_BINARY, cv2.THRESH_BINARY_INV 등)
    *   **반환값**:
        *   _ (실제 사용된 임계값, 적응형 방법에 유용)
        *   binary (임계처리된 이미지)

(코드 스니펫)
`import cv2`

`# 그레이스케일로 이미지 불러오기`
`img = cv2.imread("example.jpg", cv2.IMREAD_GRAYSCALE)`

`# 이진 임계처리 적용`
`threshold_value = 128`
`_, binary = cv2.threshold(img, threshold_value, 255, cv2.THRESH_BINARY)`

`# 결과 표시`
`cv2.imshow("Grayscale", img)`
`cv2.imshow("Binary", binary)`

`cv2.waitKey(0)`
`cv2.destroyAllWindows()`

`# 이진 이미지 저장`
`cv2.imwrite("example_binary.jpg", binary)`

***

### 210페이지
**OpenCV에서의 이미지 크기 조정**
*   **cv2.resize(src, dsize, fx, fy, interpolation)**
    *   **dsize=(너비, 높이)** → 절대 크기
    *   **fx, fy** → 스케일링 팩터 (너비 스케일, 높이 스케일)
    *   dsize가 주어지면 → 스케일링 팩터는 무시됩니다.
    *   **일반적인 보간법**:
        *   `cv2.INTER_NEAREST` (빠름, 각짐)
        *   `cv2.INTER_LINEAR` (기본값, 확대에 좋음)
        *   `cv2.INTER_AREA` (축소에 좋음)

(코드 스니펫)
`# 고정된 크기(300x300)로 조정`
`resized_fixed = cv2.resize(img, (300, 300))`

`# 스케일링 팩터로 조정 (절반 크기)`
`resized_half = cv2.resize(img, None, fx=0.5, fy=0.5)`

`# 결과 표시`
`cv2.imshow("Original", img)`
`cv2.imshow("Resized (300x300)", resized_fixed)`
`cv2.imshow("Resized (0.5x)", resized_half)`

`cv2.waitKey(0)`
`cv2.destroyAllWindows()`

***

### 211페이지
**다른 보간법으로 크기 조정**
*   **다른 보간법으로 크기 조정**
    *   `INTER_NEAREST` (최근접 이웃)
    *   `INTER_LINEAR` (쌍선형 보간)

(코드 스니펫)
`# 확대 (2x)`
`up_nearest = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)`
`up_linear = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)`

`# 축소 (0.5x)`
`down_nearest = cv2.resize(img, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_NEAREST)`
`down_linear = cv2.resize(img, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)`

`# 결과 표시`
`cv2.imshow("Original", img)`

`cv2.imshow("Up Nearest", up_nearest)`
`cv2.imshow("Up Linear", up_linear)`

`cv2.imshow("Down Nearest", down_nearest)`
`cv2.imshow("Down Linear", down_linear)`

`cv2.waitKey(0)`
`cv2.destroyAllWindows()`

***

### 212페이지
**다른 보간법으로 크기 조정**
*   **NumPy 전용 크기 조정 (최근접 이웃)**

(코드 스니펫)
`import cv2`
`import numpy as np`

`# 이미지 불러오기`
`img = cv2.imread("example.jpg")`

`# 원본 크기`
`h, w, c = img.shape`

`# 새 크기 (2배 확대)`
`new_h, new_w = h * 2, w * 2`

`# 빈 배열 생성`
`resized = np.zeros((new_h, new_w, c), dtype=np.uint8)`

`# 최근접 이웃으로 채우기 (단순히 픽셀 복제)`
`for i in range(new_h):`
`    for j in range(new_w):`
`        resized[i, j] = img[i // 2, j // 2]`

`# 결과 표시`
`cv2.imshow("Original", img)`
`cv2.imshow("Resized 2x (NumPy Nearest)", resized)`
`cv2.waitKey(0)`
`cv2.destroyAllWindows()`

***

### 213페이지
**NumPy 크기 조정 예제 (2배 확대, 쌍선형 보간)**

(복잡한 NumPy 코드 스니펫이 표시됨)

***

### 214페이지
**임의의 (소수점) 스케일링 개념**
*   이미지를 **정수가 아닌 팩터**(예: 1.3배 확대 또는 0.7배 축소)로 크기 조정할 때, 새 픽셀 위치는 원본 이미지의 정수 픽셀 좌표와 정확히 일치하지 않습니다.
    *   새 픽셀이 크기 조정된 이미지에서 (x', y')에 있다고 가정합니다.
    *   이 픽셀이 원본 이미지에서 어디에서 왔는지 찾으려면 다음을 계산합니다:
    *   여기서 fx와 fy는 너비와 높이에 대한 스케일링 팩터입니다.
    *   좌표 (x, y)는 보통 정수가 아닙니다. 따라서 단 하나의 픽셀을 선택할 수 없으며, 주변 픽셀로부터 **보간**을 통해 값을 추정해야 합니다.

***

### 215페이지
**임의의 (소수점) 스케일링 개념**
*   이미지를 **정수가 아닌 팩터**(예: 1.3배 확대 또는 0.7배 축소)로 크기 조정할 때, 새 픽셀 위치는 원본 이미지의 정수 픽셀 좌표와 정확히 일치하지 않습니다.
    *   새 픽셀이 크기 조정된 이미지에서 (x', y')에 있다고 가정합니다.
    *   이 픽셀이 원본 이미지에서 어디에서 왔는지 찾으려면 다음을 계산합니다:
    *   여기서 fx와 fy는 너비와 높이에 대한 스케일링 팩터입니다.
    *   좌표 (x, y)는 보통 정수가 아닙니다. 따라서 단 하나의 픽셀을 선택할 수 없으며, 주변 픽셀로부터 **보간**을 통해 값을 추정해야 합니다.

***

### 216페이지
**임의의 (소수점) 스케일링 개념**
*   이미지를 **정수가 아닌 팩터**(예: 1.3배 확대 또는 0.7배 축소)로 크기 조정할 때, 새 픽셀 위치는 원본 이미지의 정수 픽셀 좌표와 정확히 일치하지 않습니다.
    *   새 픽셀이 크기 조정된 이미지에서 (x', y')에 있다고 가정합니다.
    *   이 픽셀이 원본 이미지에서 어디에서 왔는지 찾으려면 다음을 계산합니다:
    *   여기서 fx와 fy는 너비와 높이에 대한 스케일링 팩터입니다.
    *   좌표 (x, y)는 보통 정수가 아닙니다. 따라서 단 하나의 픽셀을 선택할 수 없으며, 주변 픽셀로부터 **보간**을 통해 값을 추정해야 합니다.

***

### 217페이지
**임의의 (소수점) 스케일링 개념**
*   이미지를 **정수가 아닌 팩터**(예: 1.3배 확대 또는 0.7배 축소)로 크기 조정할 때, 새 픽셀 위치는 원본 이미지의 정수 픽셀 좌표와 정확히 일치하지 않습니다.
    *   새 픽셀이 크기 조정된 이미지에서 (x', y')에 있다고 가정합니다.
    *   이 픽셀이 원본 이미지에서 어디에서 왔는지 찾으려면 다음을 계산합니다:
    *   여기서 fx와 fy는 너비와 높이에 대한 스케일링 팩터입니다.
    *   좌표 (x, y)는 보통 정수가 아닙니다. 따라서 단 하나의 픽셀을 선택할 수 없으며, 주변 픽셀로부터 **보간**을 통해 값을 추정해야 합니다.

***

### 218페이지
**임의의 (소수점) 스케일링 개념**
*   이미지를 **정수가 아닌 팩터**(예: 1.3배 확대 또는 0.7배 축소)로 크기 조정할 때, 새 픽셀 위치는 원본 이미지의 정수 픽셀 좌표와 정확히 일치하지 않습니다.
    *   새 픽셀이 크기 조정된 이미지에서 (x', y')에 있다고 가정합니다.
    *   이 픽셀이 원본 이미지에서 어디에서 왔는지 찾으려면 다음을 계산합니다:
    *   여기서 fx와 fy는 너비와 높이에 대한 스케일링 팩터입니다.
    *   좌표 (x, y)는 보통 정수가 아닙니다. 따라서 단 하나의 픽셀을 선택할 수 없으며, 주변 픽셀로부터 **보간**을 통해 값을 추정해야 합니다.

***

### 219페이지
**임의의 (소수점) 스케일링 개념**
*   이미지를 **정수가 아닌 팩터**(예: 1.3배 확대 또는 0.7배 축소)로 크기 조정할 때, 새 픽셀 위치는 원본 이미지의 정수 픽셀 좌표와 정확히 일치하지 않습니다.
    *   새 픽셀이 크기 조정된 이미지에서 (x', y')에 있다고 가정합니다.
    *   이 픽셀이 원본 이미지에서 어디에서 왔는지 찾으려면 다음을 계산합니다:
    *   여기서 fx와 fy는 너비와 높이에 대한 스케일링 팩터입니다.
    *   좌표 (x, y)는 보통 정수가 아닙니다. 따라서 단 하나의 픽셀을 선택할 수 없으며, 주변 픽셀로부터 **보간**을 통해 값을 추정해야 합니다.

***

### 220페이지
**임의의 (소수점) 스케일링 개념**
*   이미지를 **정수가 아닌 팩터**(예: 1.3배 확대 또는 0.7배 축소)로 크기 조정할 때, 새 픽셀 위치는 원본 이미지의 정수 픽셀 좌표와 정확히 일치하지 않습니다.
    *   새 픽셀이 크기 조정된 이미지에서 (x', y')에 있다고 가정합니다.
    *   이 픽셀이 원본 이미지에서 어디에서 왔는지 찾으려면 다음을 계산합니다:
    *   여기서 fx와 fy는 너비와 높이에 대한 스케일링 팩터입니다.
    *   좌표 (x, y)는 보통 정수가 아닙니다. 따라서 단 하나의 픽셀을 선택할 수 없으며, 주변 픽셀로부터 **보간**을 통해 값을 추정해야 합니다.

***

### 221페이지
**OpenCV에서의 이미지 뒤집기**
*   뒤집기는 축을 따라 이미지를 **미러링**하는 것을 의미합니다. OpenCV에서는 다음을 사용하여 수행됩니다:
*   `flipped = cv2.flip(img, flipCode)`
    *   `flipCode = 0` → 수직으로 뒤집기 (위아래, x축 기준).
    *   `flipCode > 0` (보통 1) → 수평으로 뒤집기 (좌우 미러, y축 기준).
    *   `flipCode < 0` (보통 -1) → 수직 및 수평으로 모두 뒤집기 (180° 회전).
*   **예제**
    *   `import cv2`
    *   `img = cv2.imread("example.jpg")`
    *   `flip_vertical = cv2.flip(img, 0) # 위아래 뒤집기`
    *   `flip_horizontal = cv2.flip(img, 1) # 좌우 미러`
    *   `flip_both = cv2.flip(img, -1) # 위아래 + 좌우 뒤집기`

***

### 222페이지
**OpenCV에서의 이미지 회전 (간단한 경우)**
*   이미지를 회전하는 것은 회전 각도에 따라 픽셀의 위치를 변경하는 것을 의미합니다. **90°, 180°, 270°**와 같은 특수 각도의 경우, 복잡한 수학 없이 쉽게 수행할 수 있습니다.
*   **OpenCV의 내장 함수 사용**
    *   `# 90° 시계 방향`
    *   `rot90 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)`
    *   `# 180°`
    *   `rot180 = cv2.rotate(img, cv2.ROTATE_180)`
    *   `# 90° 반시계 방향`
    *   `rot270 = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)`

***

### 223페이지
**OpenCV를 이용한 이미지 회전 (임의의 각도)**
*   **cv2.getRotationMatrix2D(center, angle, scale)**는 2×3 행렬 M을 반환합니다.
    *   **angle**은 도(degree) 단위이며, 양수는 반시계 방향입니다.
    *   **center=(cx, cy)**는 픽셀 단위의 회전 중심입니다.
*   **cv2.warpAffine(img, M, dsize, flags=..., borderMode=..., borderValue=...)**로 적용합니다.

(코드 스니펫)
`import cv2`

`img = cv2.imread("example.jpg")`
`h, w = img.shape[:2]`
`center = (w/2, h/2)`

`angle = 33.0      # 도, +CCW`
`scale = 1.0`

`M = cv2.getRotationMatrix2D(center, angle, scale)`
`rot_same = cv2.warpAffine(`
`    img, M, (w, h),`
`    flags=cv2.INTER_LINEAR,                 # 보간법`
`    borderMode=cv2.BORDER_CONSTANT,         # 또는 BORDER_REFLECT, BORDER_REPLICATE, ...`
`    borderValue=(0, 0, 0)                   # 덮이지 않은 영역을 채울 색상`
`)`

***

### 224페이지
**이미지 히스토그램**
*   히스토그램은 이미지의 **픽셀 강도 분포**를 보여줍니다.
*   **OpenCV를 이용한 그레이스케일 히스토그램**
*   **컬러 히스토그램**

(왼쪽 코드 스니펫)
`import cv2`
`import matplotlib.pyplot as plt`

`# 그레이스케일 이미지 불러오기`
`img = cv2.imread("example.jpg", cv2.IMREAD_GRAYSCALE)`

`# 히스토그램 계산: [이미지], [채널], 마스크, [빈 개수], [범위]`
`hist = cv2.calcHist([img], [0], None, [256], [0, 256])`

`# 플롯`
`plt.figure()`
`plt.title("Grayscale Histogram")`
`plt.xlabel("Pixel Intensity")`
`plt.ylabel("Count")`
`plt.plot(hist)`
`plt.xlim([0, 256])`
`plt.show()`

(오른쪽 코드 스니펫)
`# 컬러 이미지 불러오기`
`img = cv2.imread("example.jpg")`
`colors = ('b', 'g', 'r')`
`for i, col in enumerate(colors):`
`    hist = cv2.calcHist([img], [i], None, [256], [0, 256])`
`    plt.plot(hist, color=col)`
`    plt.xlim([0, 256])`

`plt.title("Color Histogram")`
`plt.xlabel("Pixel Intensity")`
`plt.ylabel("Count")`
`plt.show()`

***

### 225페이지
**과제**

***

### 226페이지
**이미지 처리 과제 — OpenCV & NumPy**
*   **학습 목표**
    *   OpenCV를 이용한 기본 이미지 I/O 및 시각화 학습.
    *   그레이스케일 변환 및 이미지 이진화 연습.
    *   다른 보간법을 사용하여 크기 조정(다운샘플링/업샘플링, 정수 및 소수점 스케일) 구현.
    *   이미지 뒤집기 및 회전(단순 및 임의 각도) 수행.
    *   히스토그램(그레이스케일 및 컬러) 계산 및 분석.
    *   **순수 Python + NumPy로 모든 연산을 재구현**하고(OpenCV의 기성 처리 함수 사용 없이) 결과가 OpenCV와 일치하는지 확인.
*   **데이터**
    *   제공된 샘플 이미지 한두 개.
    *   **직접 찍은 사진 최소 2장** (예: 스마트폰, 카메라).
        *   입력으로 컬러(BGR)를 사용하고, 필요에 따라 그레이스케일 사용.
        *   다양한 콘텐츠(다른 조명, 색상, 질감) 선택.

***

### 227페이지
**파트 A. OpenCV로 구현하기**
*   모든 연산에 OpenCV 함수를 사용하고, 결과(원본과 나란히)를 표시하며, 주요 속성(shape, dtype)을 출력합니다.
    *   **이미지 읽기 / 표시**
        *   `cv2.imread`, `cv2.imshow` 또는 `matplotlib.pyplot.imshow`
        *   shape 및 dtype 출력
    *   **그레이스케일 변환**
        *   `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)`
        *   출력 shape (H, W) 확인
    *   **이진화 (임계처리)**
        *   전역 임계값 (64, 128, 200, …)
    *   **크기 조정**
        *   다운샘플 및 업샘플
        *   정수 스케일(×2, ×3) 및 임의 스케일(예: fx=1.5, fy=0.75)
        *   `INTER_NEAREST` vs `INTER_LINEAR` 비교 (다운샘플링의 경우 `INTER_AREA`도)
    *   **뒤집기**
        *   `cv2.flip(img, 0/1/-1)`
    *   **회전**
        *   90°의 단순 배수: `cv2.rotate(img, ROTATE_90_CLOCKWISE/180/90_COUNTERCLOCKWISE)`
        *   임의 각도: `cv2.getRotationMatrix2D` + `cv2.warpAffine`
        *   동일 캔버스(잘림) vs 회전 경계(확장된 캔버스) 비교
    *   **히스토그램**
        *   그레이스케일: `cv2.calcHist([gray],[0],None,[256],[0,256])`
        *   컬러: 채널별(B/G/R) 히스토그램을 하나의 플롯에 겹쳐서 표시

***

### 228페이지
**파트 B. NumPy만으로 재구현하기**
*   OpenCV의 처리 함수를 사용하지 않고 모든 연산을 재현합니다. 오직 기본 **Python과 NumPy**만 사용합니다.
    *   **금지된 함수 (연산용)**
        *   그레이스케일: `cv2.cvtColor`
        *   임계처리: `cv2.threshold`
        *   크기 조정: `cv2.resize`, `cv2.INTER_*`
        *   뒤집기/회전: `cv2.flip`, `cv2.rotate`, `cv2.getRotationMatrix2D`, `cv2.warpAffine`
        *   히스토그램: `cv2.calcHist`
    *   (이미지 읽기/쓰기는 `cv2.imread`/`cv2.imwrite` 사용 허용.)
*   **허용됨**
    *   NumPy 연산 (슬라이싱, 브로드캐스팅, 배열 수학)
    *   시각화를 위한 Matplotlib
*   **반드시 구현해야 할 것**
    *   **그레이스케일**: 채널의 가중 합 (예: 평균 또는 BT.601: 0.114B + 0.587G + 0.299R)
    *   **이진화**: 임계값에 따라 픽셀을 0/255로 설정
    *   **크기 조정**:
        *   최근접 이웃 (역 매핑 + 반올림)
        *   쌍선형 (2×2 이웃의 가중 평균)
        *   정수 및 임의 스케일 팩터
    *   **뒤집기**: 슬라이싱 (`[::-1]`, `[:, ::-1]`)
    *   **회전**:
        *   90°의 배수: 전치 + 뒤집기
        *   임의 각도: 역 매핑 + 보간 (최소한 최근접, 쌍선형은 선택 사항)
    *   **히스토그램**: 강도 계산 (0–255)

***

### 229페이지
**파트 B. NumPy만으로 재구현하기**
*   **검증**
    *   OpenCV 출력과 결과 비교 (`np.allclose`, MSE, PSNR).
    *   차이점 논의 (예: 경계 처리, 반올림).
*   **파트 C. 자신의 사진에 적용하기**
    *   자신의 사진(최소 2장)으로 모든 연산(파트 A & 파트 B)을 반복합니다.
    *   결과, 비교, 분석을 보여줍니다.
*   **파트 D. 토론**
    *   보간, 회전, 히스토그램, OpenCV vs NumPy 차이점에 대한 관찰

***

### 230페이지
**제출물 / 제출**
*   학생들은 다음 세 가지 항목을 제출해야 합니다:
    *   **1. 소스 코드**
        *   Python .py 스크립트 또는 Jupyter Notebook .ipynb (Colab 허용).
        *   코드는 오류 없이 순차적으로 실행되어야 합니다.
    *   **2. 결과 이미지**
        *   모든 입력 이미지와 처리된 출력을 `cv2.imwrite`로 저장 (예: .jpg, .png).
        *   명확하게 정리 (예: input/, output/ 폴더).
    *   **3. 보고서 (단일 Word 파일)**
        *   하나의 .docx 파일만.
        *   **반드시 포함해야 할 내용**:
            *   실행 결과 (실행 중인 코드와 콘솔 출력의 스크린샷).
            *   저장된 결과 이미지 (보고서에 삽입).
            *   **모든 연산**에 대한 스크린샷/캡처 (그레이스케일, 이진화, 크기 조정, 뒤집기, 회전, 히스토그램).
            *   각 결과에 대한 설명 및 해석 (무슨 일이 일어났는지, 결과가 왜 그렇게 보이는지, OpenCV와 NumPy 구현 간의 차이점).
*   **보고서 가이드라인 (Assignment_Report.docx)**
    *   **표지**: 과제 제목, 이름, 학번, 날짜
    *   **파트 A (OpenCV)**: 코드 실행 스크린샷, 결과 이미지, 간단한 설명
    *   **파트 B (NumPy)**: 스크린샷, 결과 이미지, OpenCV와의 비교, 차이점 설명
    *   **파트 C (개인 사진)**: 결과 및 분석
    *   **파트 D (토론)**: 보간, 회전, 히스토그램, OpenCV vs NumPy 차이점에 대한 관찰

***

### 231페이지
**과제 제출 형식**
*   학생들은 다음 구조를 포함하는 **단일 압축 파일(.zip)**을 제출해야 합니다:

(파일 구조 이미지)
`Assignment_<StudentID>_<Name>.zip`
`├── src/`
`│   ├── assignment.ipynb    # Jupyter Notebook (또는 .py 파일)`
`│   └── utils.py            # (선택 사항, 헬퍼 함수)`
`├── results/`
`│   ├── input/              # 원본 이미지 (제공된 것 + 개인 사진)`
`│   │   ├── img1.jpg`
`│   │   └── img2.png`
`│   │   └── ...`
`│   └── output/             # imwrite로 저장된 처리된 결과`
`│       ├── img1_gray.png`
`│       ├── img1_bin.png`
`│       ├── img1_resize2x.png`
`│       ├── img1_flipH.png`
`│       ├── img1_rot30.png`
`│       ├── img1_hist.png`
`│       └── ...`
`└── report/`
`    └── Assignment_Report.docx # 단일 Word 파일`

***

### 232페이지
**이미지 필터링 과제 —**
*   **목표**
    *   이 과제의 목적은 다음과 같습니다:
    *   합성곱을 사용하여 **처음부터** 에지 탐지 및 스무딩 필터를 구현합니다.
    *   수동 결과와 **OpenCV 내장 함수**를 비교합니다.
    *   노이즈가 있는 이미지에 대한 필터의 효과를 탐색하고 윤곽선을 겹쳐서 에지를 시각화합니다.
*   **데이터셋**
    *   **제공된 세트**: Lab 02 image set.zip (모든 이미지 사용).
    *   **자신의 이미지**: 휴대폰/카메라로 **최소 두 장의 사진**을 찍습니다 (필터링되지 않은 자연스러운 이미지).
*   **2. 구현할 작업 (수동 구현)**
    *   **2.1 합성곱 함수**
        *   2D 합성곱 함수를 구현합니다:

(코드 스니펫)
`def convolution(image: np.ndarray, kernel: np.ndarray, padding: str = 'zero') -> np.ndarray:`
`    ...`

        *   **image**: 2D 그레이스케일 이미지.
        *   **kernel**: 2D 필터 (홀수 크기).
        *   **padding**: 최소 'zero'가 지원되어야 합니다. ('replicate'/'reflect'도 권장).
        *   **출력**은 **입력과 동일한 크기**여야 합니다.
        *   내부적으로 부동 소수점 산술을 사용한 다음, uint8로 다시 스케일링합니다.

***

### 233페이지
**이미지 필터링 과제 —**
*   **2.2 필터 (수동)**
    *   합성곱 함수를 다음 필터에 적용합니다:
    *   **에지 탐지**
        *   Prewitt (3×3, x 및 y 방향, 크기 맵)
        *   Sobel (3×3, x 및 y 방향, 크기 맵)
        *   라플라시안 (4-이웃 또는 8-이웃 커널)
    *   **스무딩**
        *   평균 필터 (3×3, 5×5, 7×7)
        *   가우시안 필터 (3×3, 5×5)
        *   **중간값 필터** (3×3, 5×5) — 각 픽셀을 이웃의 중간값으로 대체하여 수동으로 구현합니다.
*   **OpenCV의 내장 함수를 사용해서는 안 됩니다**, 예를 들어:
    *   `cv2.filter2D`
    *   `cv2.Sobel`
    *   `cv2.Laplacian`
    *   `cv2.blur`, `cv2.GaussianBlur`, `cv2.medianBlur`
    *   또는 `scipy.signal.convolve2d`
*   수동 필터에는 **오직 numpy 연산만** 허용됩니다.
*   **비교 섹션**에서는 결과를 확인하기 위해 OpenCV 내장 함수를 사용하는 것이 허용(및 권장)됩니다.

***

### 234페이지
**이미지 필터링 과제 —**
*   **2.3 노이즈 추가 및 제거**
    *   자신의 이미지에 노이즈를 추가하고 노이즈 제거를 위해 다른 필터를 테스트합니다.
    *   **노이즈 생성 스니펫**:

(코드 스니펫)
`# 가우시안 노이즈`
`def add_gaussian_noise(image, mean=0, sigma=20):`
`    gauss = np.random.normal(mean, sigma, image.shape).astype(np.float32)`
`    noisy = image.astype(np.float32) + gauss`
`    return np.clip(noisy, 0, 255).astype(np.uint8)`

`# 소금-후추 노이즈`
`def add_salt_pepper_noise(image, prob=0.02):`
`    noisy = image.copy()`
`    rnd = np.random.rand(*image.shape)`
`    noisy[rnd < prob/2] = 0      # 후추`
`    noisy[rnd > 1 - prob/2] = 255 # 소금`
`    return noisy`

*   **당신의 작업**:
    *   선택한 이미지에 가우시안 노이즈와 소금-후추 노이즈를 추가합니다.
    *   평균, 가우시안, 중간값 필터를 적용하여 노이즈를 줄입니다.
    *   각 노이즈 유형에 대해 어떤 필터가 가장 잘 작동하는지 시각적으로 비교합니다.

***

### 235페이지
**이미지 필터링 과제 —**
*   **2.4 에지 이진화 및 윤곽선 오버레이**
    *   에지 탐지(Sobel/Prewitt/Laplacian)를 계산한 후:
    *   임계값(예: `cv2.threshold` 또는 간단한 수동 임계값)을 사용하여 에지 맵을 이진화합니다.
    *   원본 그레이스케일 이미지 위에 **윤곽선을 오버레이**합니다:

(코드 스니펫)
`# 예시: 원본 이미지에 흰색 윤곽선 오버레이`
`edges = (edge_map > 100).astype(np.uint8) * 255`

`contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)`
`overlay = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)`
`cv2.drawContours(overlay, contours, -1, (0, 0, 255), 1) # 빨간색 윤곽선`

*   이 단계는 탐지된 에지를 원본 사진에서 직접 "볼" 수 있도록 도와주어, 필터가 무엇을 감지하고 있는지 해석하기 쉽게 만듭니다.

***

### 236페이지
**이미지 필터링 과제 —**
*   **3. OpenCV 비교**
    *   OpenCV 함수를 사용하여 동일한 필터를 반복합니다:
        *   `cv2.filter2D` (Prewitt, Laplacian, Average용 사용자 정의 커널)
        *   `cv2.Sobel`
        *   `cv2.Laplacian`
        *   `cv2.blur`
        *   `cv2.GaussianBlur`
        *   `cv2.medianBlur`
    *   자세한 내용은 공식 문서를 확인하십시오:
        *   https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html
        *   https://docs.opencv.org/4.x/d5/d0f/tutorial_py_gradients.html
    *   **동일한 커널 크기** 하에서 수동 결과와 OpenCV 결과를 비교합니다. 차이점에 대해 언급하세요.

***

### 237페이지
**이미지 필터링 과제 —**
*   **3. OpenCV 비교**
    *   OpenCV 함수를 사용하여 동일한 필터를 반복합니다:
        *   `cv2.filter2D` (Prewitt, Laplacian, Average용 사용자 정의 커널)
        *   `cv2.Sobel`
        *   `cv2.Laplacian`
        *   `cv2.blur`
        *   `cv2.GaussianBlur`
        *   `cv2.medianBlur`

(코드 스니펫)
`# 평균 필터 (OpenCV)`
`blur = cv2.blur(img, (5,5))`

`# 가우시안 필터 (OpenCV)`
`gauss = cv2.GaussianBlur(img, (5,5), 0)`

`# 중간값 필터 (OpenCV)`
`median = cv2.medianBlur(img, 5)`

`# Sobel 필터 (OpenCV)`
`gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=3)`
`gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=3)`

`# 라플라시안 필터 (OpenCV)`
`lap = cv2.Laplacian(img, cv2.CV_32F, ksize=3)`

***

### 238페이지
**이미지 필터링 과제 —**
*   **4. 보고서 (단일 pdf 파일)**
    *   보고서에는 다음이 포함되어야 합니다:
    *   **방법**: 합성곱, 노이즈 추가, 중간값 필터링을 어떻게 구현했는지.
    *   **결과**:
        *   에지 탐지 예제 (수동 vs OpenCV).
        *   다른 커널을 사용한 스무딩.
        *   노이즈 제거 비교.
        *   윤곽선 오버레이.
    *   **토론**:
        *   Prewitt, Sobel, Laplacian 간의 차이점.
        *   에지 탐지 전 스무딩의 효과.
        *   가우시안 vs 소금-후추 노이즈에 어떤 필터가 가장 잘 작동하는가? 왜?
        *   수동 vs OpenCV 함수 간에 관찰된 차이점.

***

### 239페이지
**이미지 필터링 과제 —**
*   **5. 제출**
    *   코드 (ipynb 또는 .py)
    *   보고서 (pdf)
    *   자신의 사진 2장 이상 (own_images/ 폴더)

***

### 240페이지
**제출 구조**

(파일 구조 이미지)
`Assignment_<StudentID>_<Name>.zip`
`├── src/`
`│   ├── assignment.ipynb    # 메인 Jupyter Notebook (또는 선호 시 .py 파일)`
`│   └── utils.py            # (선택 사항) 헬퍼 함수`
`├── report/`
`│   └── Assignment_Report.docx # 보고서 파일 (Word 형식, 페이지 제한 없음)`
`└── own_images/`
`    ├── photo1.jpg          # 학생이 직접 촬영한 이미지`
`    ├── photo2.jpg`
`    └── ...`

***

### 241페이지
**프로그래밍 과제 (OpenCV + MediaPipe)**
*   **1. 비디오 I/O 연습**
    *   **목표**: 비디오 스트림을 캡처, 표시, 저장하는 방법을 배웁니다.
    *   **작업**:
        *   자신의 장치(예: 노트북 웹캠, 데스크톱 웹캠 또는 스마트폰 카메라)를 사용하여 짧은 비디오 클립을 녹화합니다.
        *   녹화된 비디오를 OpenCV에서 로드하고 재생합니다.
        *   각 프레임에 간단한 이미지 연산을 적용합니다, 예:
            *   그레이스케일로 변환 (`cv2.cvtColor`) / 수평으로 뒤집기 (`cv2.flip`)
            *   적절한 코덱(예: XVID 또는 MP4V)으로 `cv2.VideoWriter`를 초기화합니다.
            *   처리된 비디오를 `.avi` 또는 `.mp4`로 저장합니다.
        *   실시간 웹캠 (`cv2.VideoCapture(0)`) 피드를 **FPS 오버레이**와 함께 표시합니다.
*   **2. GUI 상호작용 연습**
    *   **목표**: 키보드, 트랙바, 마우스를 이용한 대화형 제어를 배웁니다.
    *   **작업**:
        *   **키보드 제어**: 키(q, s 등)를 눌러 다른 동작(예: 그레이스케일 토글, 스냅샷)을 트리거합니다.
        *   **트랙바 제어**: 트랙바로 임계값을 조정하여 → **이진화 결과**를 동적으로 업데이트합니다.
        *   **마우스 상호작용**:
            *   왼쪽 클릭으로 점을 그리고, 오른쪽 클릭으로 원을 그립니다.
            *   왼쪽 마우스 버튼으로 드래그하여 **관심 영역(ROI)을 자릅니다**.

***

### 242페이지
**프로그래밍 과제 (OpenCV + MediaPipe)**
*   **3. 그리기 함수 연습**
    *   **목표**: OpenCV의 기본 그리기 및 주석 함수를 연습합니다.
    *   **작업**:
        *   빈 캔버스에 도형(사각형, 원, 선, 다각형)을 그립니다.
        *   다른 글꼴, 색상, 크기로 텍스트 주석(`cv2.putText`)을 추가합니다.
        *   간단한 **스케치패드**를 만듭니다: 마우스를 누르고 드래그하여 자유로운 선을 그립니다.
*   **4. MediaPipe 연습**
    *   **목표**: 실시간 랜드마크 탐지 및 시각화를 탐색합니다.
    *   **작업**:
        *   **인간 자세**: 웹캠을 사용하여 33개의 신체 랜드마크가 있는 스켈레톤을 표시합니다.
        *   **손 자세**: 양손을 탐지하고, 21개의 랜드마크를 그리며, 핀치 거리(엄지-검지)를 표시합니다.
        *   **얼굴 메시**: 468개의 랜드마크와 얼굴 윤곽을 실시간으로 표시합니다.
*   **제출 가이드라인**
    *   **제출물**: 코드
    *   **보고서** (pdf)

***

### 243페이지
**보고서 필수 섹션**
*   **구현 요약**
    *   구현한 내용에 대한 간략한 설명
    *   어떤 주요 OpenCV 함수가 사용되었는지
    *   고수준 워크플로우 (전체 코드 아님, 주요 로직만)
*   **실행 결과**
    *   프로그램 실행 **스크린샷** (예: 원본 vs 처리된 비디오 프레임, 주석이 달린 이미지, 랜드마크 오버레이)
    *   다른 경우를 보여주기 위해 필요한 경우 여러 예제 포함
    *   각 스크린샷 아래에 명확한 캡션
*   **토론**
    *   직면했던 어려움과 해결 방법
    *   한계 또는 가능한 개선 사항
    *   이 과제를 통해 배운 점
