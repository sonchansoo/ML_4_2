## 🔍 강의 요약
이번 강의에서는 이미지 분류(Image Classification)의 기본적인 개념과 접근법을 다룹니다. k-최근접 이웃(k-NN)과 같은 전통적인 Feature 기반 분류 방식의 원리와 계산 비용, 데이터 의존성 등의 한계점을 알아봅니다. 이러한 한계를 극복하기 위해 등장한 선형 분류기(Linear Classifier)의 개념을 학습하고, Logistic Regression을 통해 모델이 어떻게 함수를 학습하는지 살펴봅니다. 더 나아가, SIFT나 HOG와 같은 수작업 Feature(Hand-crafted Feature)의 근본적인 문제점을 지적하며, Feature 자체를 데이터로부터 학습하는 딥러닝으로의 패러다임 전환(AlexNet)을 설명합니다. 마지막으로, 선형 분류기의 기하학적 의미와 선형 분리가 불가능한 데이터에 대한 한계점을 학습합니다.

## 💡 핵심 포인트
-   **이미지 분류**: 주어진 이미지에 대해 미리 정해진 카테고리 중 하나로 분류하는 작업입니다.
-   **k-최근접 이웃 (k-NN)**: 새로운 이미지의 Feature를 모든 학습 예제 데이터의 Feature와 비교하여 가장 가까운 k개의 이웃을 찾고, 다수결 투표로 클래스를 결정하는 방식입니다.
-   **k-NN의 한계**: 학습 데이터가 많아질수록 계산 비용이 급격히 증가하며, 픽셀 거리 기반 유사도는 조명, 변형 등에 매우 취약합니다.
-   **선형 분류기 (Linear Classifier)**: 모든 데이터를 저장하는 대신, 클래스를 구분하는 결정 경계(Decision Boundary) 함수를 학습하여 분류를 수행하는 효율적인 모델입니다.
-   **수작업 Feature의 한계**: HOG, SIFT 등 사람이 직접 설계한 Feature는 조명, 가려짐, 변형 등 현실 세계의 다양한 변화에 강인하지 못하다는 근본적인 한계가 있습니다.
-   **패러다임 전환**: AlexNet의 등장으로, Feature 추출기 자체를 모델이 데이터로부터 자동으로 학습하는 'Learned Feature' 시대가 열렸습니다.
-   **선형 분류기의 한계**: 이름 그대로 선형 결정 경계만 만들 수 있어, XOR 문제와 같이 선형으로 분리할 수 없는 데이터는 제대로 분류하지 못합니다.

## 세부 내용 📚
### 1️⃣ 이미지 분류의 전통적인 접근법: Feature 기반 분류
-   **Feature 추출 및 매칭**: 전통적인 방식은 이미지에서 중요한 지점(Keypoint)을 찾고, 그 주변 영역의 특성을 숫자 벡터(Descriptor)로 변환하여 Feature를 추출합니다.
    -   새로운 이미지가 들어오면, 미리 준비된 예제(고양이, 강아지) 이미지들의 Feature들과 비교합니다.
    -   두 Feature 벡터 간의 유사성은 유클리드 거리(Euclidean distance) 등으로 측정하며, 거리가 가장 가까운 쌍을 '매칭된 Feature'로 간주합니다.
-   **k-최근접 이웃 (k-NN) 분류**:
    -   **1-NN**: 새로운 이미지는 가장 많은 Feature가 매칭된 예제 이미지의 클래스로 분류됩니다. (e.g., 고양이 예제와 3개, 강아지 예제와 1개가 매칭되면 '고양이'로 판단)
    -   **k-NN**: 가장 유사한 상위 k개의 예제를 찾은 후, 다수결 투표(Majority Voting)를 통해 최종 클래스를 결정합니다. 예를 들어 k=3일 때, 가장 비슷한 예제가 고양이, 고양이, 강아지 순이라면 '고양이'로 최종 분류합니다.

### 2️⃣ k-NN 분류기의 한계
-   **🐌 계산 비용 (Computational Cost)**: 분류 시마다 새로운 이미지를 **모든** 학습 데이터와 비교해야 하므로, 데이터가 수백만 장 이상인 대규모 분류에서는 사실상 비현실적입니다.
-   **😢 데이터 의존성 (Data Dependency)**: 각 클래스에 대해 다양한 변화(자세, 색상, 배경)를 포함하는 충분한 양의 데이터가 없으면 성능이 크게 저하됩니다.
-   **📏 픽셀 거리의 한계 (Limitation of Pixel Distance)**: Feature 추출 없이 원본 이미지의 픽셀 값 차이로 유사도를 측정하는 경우, 이미지를 단 1픽셀만 이동시켜도 완전히 다른 이미지로 인식하는 등 의미 있는 시각적 유사성을 잡아내지 못합니다.

### 3️⃣ 선형 분류기 (Linear Classifier)의 등장
-   k-NN의 비효율성을 극복하기 위해, 모든 예제를 기억하는 대신 클래스를 직접 구분하는 **함수 `f`**를 학습하는 아이디어가 등장했습니다.
-   이 함수는 Feature 공간에서 클래스를 나누는 **결정 경계(Decision Boundary)**를 학습합니다. 2차원에서는 선(line), 고차원에서는 초평면(Hyperplane)이 됩니다.
-   **Logistic Regression**은 가장 단순한 형태의 선형 분류기입니다.
    -   **입력**: 이미지에서 추출된 Feature 벡터 `x`
    -   **프로세스**:
        1.  입력 Feature에 가중치 `w`를 곱하고 편향 `b`를 더하여 선형 결합 점수 `s`를 계산합니다 (`s = w^T * x + b`).
        2.  이 점수를 Sigmoid(이진 분류) 또는 Softmax(다중 분류) 함수에 통과시켜 0과 1 사이의 확률 값으로 변환합니다.
    -   **학습**: 예측 확률과 실제 정답(Label) 간의 오차를 **Cross-Entropy Loss**로 측정하고, 이 오차를 최소화하는 방향으로 `w`와 `b`를 업데이트합니다.

### 4️⃣ 고정된 Feature 추출기의 한계와 패러다임 전환
-   **👎 수작업 Feature(Hand-crafted Feature)의 문제점**:
    -   **정보 손실**: HOG와 같은 Feature는 색상이나 질감 정보를 버리고 엣지 방향 정보만 유지하여 중요한 단서를 잃을 수 있습니다.
    -   **다양성에 대한 취약성**: 조명 변화, 배경 복잡도, 객체 가려짐(Occlusion), 변형(Deformation), 클래스 내 다양성(Intra-class variation) 등 현실 세계의 복잡한 문제에 강인하게 대처하기 어렵습니다.
    -   **끝없는 탐색**: 특정 문제를 해결하기 위해 새로운 Feature를 계속해서 수동으로 설계하는 것은 확장성과 일반성이 떨어집니다.
-   **✨ 패러다임 전환: 학습된 Feature (Learned Feature)**:
    -   이러한 한계로 인해, Feature를 사람이 설계하는 대신 **모델이 데이터로부터 직접 유용한 Feature를 학습**하게 하자는 아이디어로 전환되었습니다.
    -   **AlexNet (2012)**: Feature 추출과 분류를 하나의 네트워크 안에서 동시에(end-to-end) 학습한 최초의 딥러닝 모델입니다. 이는 기존의 모든 수작업 Feature 기반 방법론의 성능을 압도하며 컴퓨터 비전 분야에 딥러닝 시대를 열었습니다.
    -   **성공 요인**: 인터넷을 통한 방대한 이미지 데이터 접근, ImageNet과 같은 대규모 데이터셋, 그리고 딥러닝 학습을 가능하게 한 강력한 GPU의 발전이 있었습니다.

### 5️⃣ 선형 분류기의 기하학적 해석과 한계
-   **📐 기하학적 의미**: 선형 분류기는 각 클래스마다 하나의 결정 평면(Decision Plane)을 가집니다. 새로운 데이터가 이 평면의 어느 쪽에 위치하는지에 따라 클래스 점수(logit)의 부호와 크기가 결정되며, 이는 모델의 확신도를 나타냅니다.
-   **🧠 학습된 가중치의 시각화**: 이미지 픽셀을 입력으로 사용하는 선형 분류기의 가중치 `W`를 다시 이미지 형태로 시각화하면, 모델이 각 클래스를 인식하기 위해 학습한 일종의 '평균 템플릿' 또는 '프로토타입'을 볼 수 있습니다.
-   **🚫 한계**: 선형 분류기는 데이터를 하나의 선이나 평면으로만 나눌 수 있습니다. 따라서 아래와 같이 **선형으로 분리할 수 없는(not linearly separable)** 데이터 분포는 제대로 학습하지 못합니다.
    -   **XOR 패턴**: 클래스가 대각선 방향으로 분포된 경우
    -   **Ring/Annulus 패턴**: 하나의 클래스가 다른 클래스를 고리 모양으로 감싸고 있는 경우
    -   이러한 문제를 해결하기 위해서는 **비선형(Non-linear) 결정 경계**를 만들 수 있는 SVM 커널이나 신경망(Neural Networks)과 같은 더 강력한 모델이 필요합니다.

## 오늘의 연습문제 📝
-   **문제 1**: k-NN 분류기의 두 가지 주요 한계점은 무엇이며, 선형 분류기는 이러한 문제들을 어떤 아이디어를 통해 해결하고자 하는지 설명해 보세요.
-   **문제 2**: 'Hand-crafted feature'와 'Learned feature'의 개념적 차이를 설명하고, AlexNet이 컴퓨터 비전 분야에서 '패러다임 전환'을 가져온 핵심적인 이유를 서술해 보세요.
-   **문제 3**: 선형 분류기가 '선형(Linear)'이라고 불리는 이유를 결정 경계(Decision Boundary)의 관점에서 설명하고, 선형 분류기가 해결하기 어려운 데이터 분포의 예시를 두 가지 들어보세요.
