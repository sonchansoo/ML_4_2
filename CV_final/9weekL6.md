## 🔍 강의 요약

이번 강의에서는 컴퓨터 비전의 핵심 모델인 신경망(Neural Network)의 기본 원리와 최적화 방법에 대해 학습합니다. 선형 분류기(Linear Classifier)가 가진 한계를 알아보고, 이를 극복하기 위해 활성화 함수(Activation Function)를 통해 비선형성(Nonlinearity)을 도입하는 과정을 살펴봅니다. 여러 층을 쌓아 만드는 다층 신경망(Multi-layer Neural Network)의 구조와 학습 목표를 정의하고, 경사 하강법(Gradient Descent)을 시작으로 SGD, Momentum, RMSProp, Adam과 같은 실용적인 최적화 알고리즘들을 통해 모델을 효과적으로 학습시키는 방법을 배웁니다.

## 💡 핵심 포인트

-   **선형성의 한계:** 선형 분류기는 직선(또는 초평면)으로만 데이터를 나눌 수 있어 복잡한 패턴을 학습하지 못합니다.
-   **비선형성의 도입:** 신경망은 선형 변환 이후에 활성화 함수(Activation Function)를 적용하여 비선형성을 추가합니다.
-   **활성화 함수의 중요성:** 활성화 함수가 없다면 여러 층을 쌓아도 결국 하나의 선형 모델과 동일해져 깊은 네트워크의 의미가 사라집니다.
-   **학습의 목표:** 모델의 예측이 정답과 얼마나 다른지를 측정하는 손실 함수(Loss Function, 예: Cross-Entropy Loss)의 값을 최소화하는 최적의 가중치(Weight)를 찾는 것입니다.
-   **최적화 알고리즘:** 경사 하강법(Gradient Descent)은 손실 함수의 기울기를 따라 파라미터를 업데이트하여 손실을 최소화하는 방법입니다.
-   **실용적 최적화:** 전체 데이터셋을 사용하는 것은 비효율적이므로, 작은 데이터 묶음(Mini-batch)을 사용하는 확률적 경사 하강법(SGD)이 널리 쓰입니다.
-   **고급 최적화 기법:** Adam과 같은 고급 옵티마이저는 SGD의 불안정성과 느린 수렴 문제를 해결하여 더 빠르고 안정적인 학습을 가능하게 합니다.
-   **학습률 스케줄링:** 학습 과정 동안 학습률(Learning Rate)을 동적으로 조절하여 학습 안정성과 성능을 향상시킬 수 있습니다.

## 세부 내용 📚

### 1️⃣ 선형 분류기의 한계와 신경망의 필요성

-   **선형 분류기(Linear Classifier)**는 입력 데이터(Feature Vector)에 가중치(Weight)를 곱하고 편향(Bias)을 더하는 단일 선형 변환(`z = Wx + b`)을 통해 클래스 점수를 계산합니다.
-   이 방식은 데이터를 오직 직선 또는 초평면(Hyperplane)으로만 구분할 수 있어, XOR 패턴이나 고리 모양(Ring/Annulus) 분포처럼 **선형적으로 분리할 수 없는(Non-linearly Separable)** 데이터는 제대로 분류하지 못합니다.
-   이러한 한계를 극복하기 위해 **비선형(Nonlinear)** 결정 경계를 만들 수 있는 더 표현력 높은 모델이 필요하며, 이것이 바로 **신경망(Neural Network)**입니다.

### 2️⃣ 비선형성 도입: 활성화 함수와 레이어 쌓기

-   신경망은 선형 변환 결과에 **활성화 함수(Activation Function)**라는 비선형 함수를 적용하여 비선형성을 도입합니다.
    -   **대표적인 활성화 함수:** ReLU, Sigmoid, Tanh 등
-   하지만 단일 레이어에 활성화 함수만 추가하는 것은 점수의 척도만 왜곡할 뿐, 모델의 근본적인 표현력을 높이지 못합니다.
-   진정한 비선형성을 얻기 위해서는 **레이어를 여러 개 쌓아야(Stacking Layers)** 합니다. 즉, `(선형 변환 -> 활성화 함수)` 과정을 반복합니다.
    -   **2-Layer Neural Network:** `z² = W² * φ(W¹ * x + b¹) + b²`
-   **활성화 함수의 역할:** 만약 활성화 함수 없이 선형 레이어만 계속 쌓으면, 모든 계산은 결국 하나의 큰 선형 변환(`W_new * x`)으로 축약되어 깊은 네트워크의 이점을 잃게 됩니다.

### 3️⃣ 신경망의 구조와 생물학적 영감

-   **구조:**
    -   **Input Layer:** 입력 데이터(Feature)의 수만큼 노드를 가집니다.
    -   **Hidden Layer(s):** 입력과 출력 사이에 위치하며, 모델의 표현력을 결정하는 핵심 부분입니다. 노드의 수는 사용자가 정하는 하이퍼파라미터(Hyperparameter)입니다.
    -   **Output Layer:** 분류할 클래스의 수만큼 노드를 가집니다.
-   **Hidden Layer의 역할:** 히든 레이어의 뉴런(노드) 수가 많아질수록 모델의 **수용력(Capacity)**이 커져 더 복잡하고 유연한 결정 경계를 만들 수 있지만, **과적합(Overfitting)**의 위험도 함께 증가합니다.
-   **생물학적 영감:**
    -   신경망의 기본 단위인 **퍼셉트론(Perceptron)**은 1950년대에 생물학적 뉴런의 작동 방식(여러 신호를 받아 임계값을 넘으면 활성화)에서 영감을 받아 제안되었습니다.
    -   하지만 이는 단순한 비유일 뿐, 실제 뇌의 뉴런은 훨씬 복잡한 비선형 시스템으로 작동합니다.

### 4️⃣ 신경망 학습: 손실 함수와 최적화 목표

-   신경망 학습의 목표는 주어진 데이터에 대해 모델의 예측을 최대한 정확하게 만드는 최적의 가중치(W)와 편향(b)을 찾는 것입니다.
-   **학습 과정:**
    1.  **정답 표현:** 실제 정답 레이블을 **원-핫 인코딩(One-hot Encoding)** 벡터(예: 고양이 = `[1, 0, 0]`)로 표현합니다.
    2.  **오차 측정:** 모델의 예측 확률과 실제 정답(원-핫 벡터) 사이의 차이를 **손실 함수(Loss Function)**로 계산합니다. 분류 문제에서는 주로 **교차 엔트로피 손실(Cross-Entropy Loss)**이 사용됩니다.
        -   `Loss = -log(정답 클래스에 대한 예측 확률)`
        -   정답을 확신하면(확률이 1에 가까우면) 손실이 0에 가까워지고, 틀린 답을 확신하면(확률이 0에 가까우면) 손실이 무한대에 가깝게 커집니다.
    3.  **최적화 목표:** 전체 데이터셋에 대한 **평균 손실(Total Loss)을 최소화**하는 파라미터 `W*`를 찾는 것입니다.

### 5️⃣ 최적화 기법 1: 경사 하강법 (Gradient Descent)

-   **경사 하강법(Gradient Descent, GD)**은 손실 함수를 최소화하기 위한 가장 기본적인 최적화 알고리즘입니다.
-   **원리:** 손실 함수를 파라미터에 대해 미분하여 얻은 **기울기(Gradient)**의 **반대 방향**으로 파라미터를 조금씩 이동시킵니다. 이는 마치 산에서 가장 가파른 내리막길을 따라 한 걸음씩 내려가는 것과 같습니다.
-   **업데이트 규칙:** `W_new = W_old - η * ∇L(W)`
    -   `η (eta)`는 **학습률(Learning Rate)**로, 한 번에 얼마나 이동할지를 결정하는 보폭(Step size)에 해당합니다.
-   **한계:**
    -   **Full-Batch GD:** 전체 데이터셋의 기울기를 계산해야 하므로 데이터가 클 경우 매우 느리고 메모리 소모가 큽니다.
    -   **Local Minima:** 경사 하강법은 전역 최솟값(Global Minimum)이 아닌 지역 최솟값(Local Minimum)에 빠져 멈출 수 있습니다.

### 6️⃣ 최적화 기법 2: 실용적인 고급 알고리즘

-   **확률적 경사 하강법 (Stochastic Gradient Descent, SGD):**
    -   전체 데이터 대신 **미니배치(Mini-batch)**라는 작은 데이터 묶음을 무작위로 뽑아 기울기를 근사 계산합니다.
    -   훨씬 빠르고 메모리 효율적이며, 노이즈가 있는 업데이트 덕분에 오히려 지역 최솟값을 탈출하는 데 도움이 되기도 합니다. 오늘날 거의 모든 신경망 학습에 사용됩니다.
-   **Momentum:**
    -   이전 업데이트의 방향과 속도를 **관성(Inertia)**처럼 유지하여 현재 기울기와 결합합니다. 이를 통해 SGD의 지그재그 움직임을 완화하고, 지역 최솟값이나 평탄한 지역을 더 빠르게 통과하도록 돕습니다.
-   **RMSProp:**
    -   각 파라미터마다 학습률을 다르게 적용하는 **적응형 학습률(Adaptive Learning Rate)** 기법입니다.
    -   최근 기울기의 크기를 추적하여, 기울기가 컸던 파라미터는 학습률을 줄이고 작았던 파라미터는 학습률을 키워 진동을 줄이고 수렴을 안정시킵니다.
-   **Adam (Adaptive Moment Estimation):**
    -   **Momentum**과 **RMSProp**의 장점을 결합한 옵티마이저입니다. 기울기의 평균(1차 모멘트)과 분산(2차 모멘트의 제곱근)을 모두 추적하여 파라미터를 업데이트합니다.
    -   성능이 뛰어나고 안정적이어서 현재 가장 널리 사용되는 기본 옵티마이저 중 하나입니다.

### 7️⃣ 학습률과 스케줄링 전략

-   **학습률(Learning Rate, η)**은 학습의 성공을 좌우하는 매우 중요한 하이퍼파라미터입니다.
    -   **너무 크면:** 최적점을 지나치며 발산(Divergence)하거나 진동합니다.
    -   **너무 작으면:** 학습 속도가 매우 느려집니다.
-   **학습률 스케줄링(Learning Rate Scheduling):** 학습이 진행됨에 따라 학습률을 동적으로 조절하는 전략입니다.
    -   **Step Decay:** 특정 학습 지점(Epoch)마다 학습률을 일정 비율로 줄입니다.
    -   **Cosine Annealing:** 코사인 함수 곡선을 따라 학습률을 부드럽게 감소시킵니다.
    -   **Linear Warm-Up:** 학습 초반에 불안정한 파라미터를 안정시키기 위해 학습률을 작은 값에서 서서히 목표치까지 증가시킨 후, 감소 스케줄을 적용합니다.

## 오늘의 연습문제 📝

-   **문제 1:** 다층 신경망(Multi-layer Neural Network)에서 활성화 함수(Activation Function)가 필수적인 이유는 무엇인가요? 만약 활성화 함수 없이 여러 개의 선형 레이어만 쌓는다면 어떤 결과가 발생할지 설명해 보세요.
-   **문제 2:** 전체 데이터셋을 사용하는 경사 하강법(Full-Batch Gradient Descent)의 실용적인 문제점은 무엇이며, 확률적 경사 하강법(Stochastic Gradient Descent, SGD)은 이 문제를 어떻게 해결하나요? 또한, SGD가 새롭게 야기하는 문제점은 무엇이 있나요?
-   **문제 3:** Adam 옵티마이저는 Momentum과 RMSProp의 장점을 결합한 것으로 알려져 있습니다. Momentum과 RMSProp이 각각 해결하고자 하는 SGD의 문제점은 무엇인지 설명해 보세요.
