다음은 제공된 PDF 문서의 모든 페이지를 한국어로 번역한 내용입니다.

***

### **페이지 1**

**컴퓨터 비전의 기초**
**2025년 2학기**
**- 엄광현 –**
**인공지능학과**

***

### **페이지 2**

**실습 02:**
**OpenCV 고급 주제: 비디오**
**I/O, GUI 및 확장 기능**
*   비디오 I/O
*   GUI
*   확장 기능

***

### **페이지 3**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   지금까지 우리는 이미지 처리의 기본과 간단한 연산들을 다루었습니다.
*   하지만 **실제 비전 기반 애플리케이션**을 구축하려면 더 나아가야 합니다.

*   **왜 이것을 배워야 할까요?**
    *   **실제 애플리케이션 개발**
        *   단순히 정적 이미지를 처리하는 것을 넘어,
        *   비디오 스트림을 캡처하고(비디오 I/O),
        *   결과를 상호작용적으로 표시하며(GUI),
        *   다양한 확장 함수를 활용해야 합니다.
        *   이를 통해 완벽하게 작동하는 애플리케이션을 만들 수 있습니다.

    *   **산업 및 연구를 위한 필수 기술**
        *   자율 주행, 의료 영상, 감시, AR/VR과 같은 분야에서,
        *   비디오 입출력 및 GUI 처리는 필수적입니다.

    *   **프로젝트 및 데모의 기초**
        *   캡스톤 프로젝트, 연구 발표 또는 제품 데모를 위해 코드를 보여주는 것만으로는 충분하지 않습니다—
            **상호작용이 가능한 실행 프로그램**이 필요합니다.
        *   고급 OpenCV 기능이 그 기반을 제공합니다.

***

### **페이지 4**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   **실제 비전 애플리케이션 (예시)**
    *   OpenCV는 기본적인 이미지 처리만을 위한 것이 아닙니다—**실용적이고 작동하는 애플리케이션**을 구축할 수 있게 해줍니다.

**(이미지: OpenCV를 이용한 고속도로에서의 이동 객체 탐지 예시)**

***

### **페이지 5**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   **실제 비전 애플리케이션 (예시)**
    *   OpenCV는 기본적인 이미지 처리만을 위한 것이 아닙니다—**실용적이고 작동하는 애플리케이션**을 구축할 수 있게 해줍니다.

**(이미지: 캠퍼스 내 보행자들을 탐지하는 예시)**

***

### **페이지 6**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   **실제 비전 애플리케이션 (예시)**
    *   OpenCV는 기본적인 이미지 처리만을 위한 것이 아닙니다—**실용적이고 작동하는 애플리케이션**을 구축할 수 있게 해줍니다.

**(이미지: 도로 위 차량들을 보여주는 예시)**

***

### **페이지 7**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   **실제 비전 애플리케이션 (예시)**
    *   OpenCV는 기본적인 이미지 처리만을 위한 것이 아닙니다—**실용적이고 작동하는 애플리케이션**을 구축할 수 있게 해줍니다.

**(이미지: 흑백으로 촬영된, 위에서 내려다본 사람들의 모습)**

***

### **페이지 8**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   **실제 비전 애플리케이션 (예시)**
    *   OpenCV는 기본적인 이미지 처리만을 위한 것이 아닙니다—**실용적이고 작동하는 애플리케이션**을 구축할 수 있게 해줍니다.

**(이미지: 사람의 얼굴과 눈을 탐지하고, 손목시계를 인식하는 예시)**

***

### **페이지 9**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   **실제 비전 애플리케이션 (예시)**
    *   OpenCV는 기본적인 이미지 처리만을 위한 것이 아닙니다—**실용적이고 작동하는 애플리케이션**을 구축할 수 있게 해줍니다.

**(이미지: 두 사람의 얼굴과 눈을 각각 탐지하는 예시)**

***

### **페이지 10**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   **실제 비전 애플리케이션 (예시)**
    *   OpenCV는 기본적인 이미지 처리만을 위한 것이 아닙니다—**실용적이고 작동하는 애플리케이션**을 구축할 수 있게 해줍니다.

**(이미지: 얼굴 랜드마크와 손 제스처를 인식하여 텍스트를 출력하는 예시)**

***

### **페이지 11**

**OpenCV의 고급 기능에 대해 알아봅시다.**

*   **실제 비전 애플리케이션 (예시)**
    *   OpenCV는 기본적인 이미지 처리만을 위한 것이 아닙니다—**실용적이고 작동하는 애플리케이션**을 구축할 수 있게 해줍니다.

**(이미지: 손에 든 공을 추적하는 예시)**

***

### **페이지 12**

**비디오란 무엇인가?**

*   **인간의 관점**
    *   우리는 세상을 정적인 이미지로 인식하지 않습니다.
    *   우리가 보는 것은 **시간에 따른 이미지의 연속적인 변화**입니다.

*   **컴퓨터에서의 디지털 비디오**
    *   비디오는 본질적으로 시간 순서대로 쌓인 **이미지(프레임)의 시퀀스**입니다.
    *   각 프레임은 정지된 사진이지만, 빠르게 연속적으로 보여주면 움직임의 환상을 만들어냅니다.

*   **프레임 간격 (프레임 레이트)**
    *   프레임 사이의 시간 간격은 보통 매우 짧습니다 (밀리초 단위로 측정).
    *   **더 높은 프레임 레이트 (더 짧은 간격)**: 더 부드러운 움직임 (예: 60 fps).
    *   **더 낮은 프레임 레이트 (더 긴 간격)**: 더 뚝뚝 끊기는 움직임 (예: 10 fps는  jerky하게 보임).

**(이미지: 높은 프레임 레이트와 낮은 프레임 레이트의 움직임 차이를 보여주는 비교)**

***

### **페이지 13**

**비디오란 무엇인가?**

*   **인간의 관점**
    *   우리는 세상을 정적인 이미지로 인식하지 않습니다.
    *   우리가 보는 것은 **시간에 따른 이미지의 연속적인 변화**입니다.

*   **컴퓨터에서의 디지털 비디오**
    *   비디오는 본질적으로 시간 순서대로 쌓인 **이미지(프레임)의 시퀀스**입니다.
    *   각 프레임은 정지된 사진이지만, 빠르게 연속적으로 보여주면 움직임의 환상을 만들어냅니다.

*   **프레임 간격 (프레임 레이트)**
    *   프레임 사이의 시간 간격은 보통 매우 짧습니다 (밀리초 단위로 측정).
    *   **더 높은 프레임 레이트 (더 짧은 간격)**: 더 부드러운 움직임 (예: 60 fps).
    *   **더 낮은 프레임 레이트 (더 긴 간격)**: 더 뚝뚝 끊기는 움직임 (예: 10 fps는 jerky하게 보임).

**(이미지: 연속된 이미지 파일들이 모여 비디오를 구성하는 것을 보여주는 예시)**

***

### **페이지 14**

**비디오란 무엇인가?**

*   **인간의 관점**
    *   우리는 세상을 정적인 이미지로 인식하지 않습니다.
    *   우리가 보는 것은 **시간에 따른 이미지의 연속적인 변화**입니다.

*   **컴퓨터에서의 디지털 비디오**
    *   비디오는 본질적으로 시간 순서대로 쌓인 **이미지(프레임)의 시퀀스**입니다.
    *   각 프레임은 정지된 사진이지만, 빠르게 연속적으로 보여주면 움직임의 환상을 만들어냅니다.

*   **프레임 간격 (프레임 레이트)**
    *   프레임 사이의 시간 간격은 보통 매우 짧습니다 (밀리초 단위로 측정).
    *   **더 높은 프레임 레이트 (더 짧은 간격)**: 더 부드러운 움직임 (예: 60 fps).
    *   **더 낮은 프레임 레이트 (더 긴 간격)**: 더 뚝뚝 끊기는 움직임 (예: 10 fps는 jerky하게 보임).

**(이미지: 1초 동안 10, 15, 30, 45, 60 fps의 차이를 시각적으로 보여주는 예시)**

***

### **페이지 15**

**비디오 I/O: 비디오 읽기**

*   **비디오를 읽는다는 것은 무엇을 의미할까요?**
    *   비디오 = 이미지(프레임)의 시퀀스.
    *   프레임과 함께 메타데이터(예: 프레임 레이트, 해상도, 코덱)가 저장됩니다.
    *   비디오 읽기 = 처음부터 프레임을 순차적으로 하나씩 로드하는 것.

*   **프레임 레이트의 중요성**
    *   프레임이 얼마나 빨리 재생될지를 결정합니다.
    *   예시: 30 fps = 초당 30개의 이미지가 표시됨.

**(코드 예시: OpenCV를 사용하여 비디오 파일을 읽고 화면에 표시하는 Python 코드)**

***

### **페이지 16**

**cv2.waitKey() 소개**

*   **waitKey란 무엇인가?**
    *   OpenCV GUI 창에서 지정된 시간 동안 **키보드 이벤트**를 기다리는 OpenCV 함수입니다.
    *   눌린 키의 **ASCII 코드**를 반환합니다.

*   **기본 역할**
    *   OpenCV 창이 응답 상태를 유지하도록 합니다 (이 함수가 없으면 imshow가 새로고침되지 않음).
    *   주어진 시간 동안 키 입력을 기다립니다.

*   **시간 인수에 따른 동작**
    *   `cv2.waitKey(0)` → 키가 눌릴 때까지 무한정 기다립니다.
    *   `cv2.waitKey(25)` → 25ms 동안 기다린 후 자동으로 계속 진행합니다.
    *   키가 눌리지 않으면 **-1**을 반환합니다.

*   **왜 중요한가?**
    *   `imshow`로 표시된 이미지와 비디오가 계속 **업데이트(새로고침)**되도록 보장합니다.
    *   반환된 키 코드는 **프로그램 동작을 제어**하는 데 사용될 수 있습니다 (예: ESC로 종료, s로 프레임 저장 등).

*   **"waitKey는 OpenCV 창을 상호작용적으로 만듭니다. 이것이 없으면 아무것도 움직이지 않으며, 이것이 있으면 키보드 입력을 사용하여 프로그램 흐름을 제어할 수 있습니다."**

***

### **페이지 17**

**예시: cv2.waitKey()를 이용한 키보드 제어**

*   `cv2.waitKey(0)` → 키가 눌릴 때까지 무한정 기다립니다.
*   `ord('q')` → 'q'의 ASCII 코드. 숫자 27과 같은 것을 사용하는 것보다 기억하기 쉽습니다.

*   **이 프로그램은 눌린 키에 따라 다르게 반응합니다:**
    *   **q**: 프로그램 종료
    *   **s**: 이미지 저장
    *   **g**: 그레이스케일로 전환
    *   **c**: 컬러로 다시 전환
    *   **ESC**: 종료

*   `cv2.waitKey()` + 조건문은 추가적인 GUI 라이브러리 없이도 프로그램을 간단한 대화형 UI로 만들 수 있습니다.

**(코드 예시: 키보드 입력에 따라 이미지 저장, 그레이스케일 변환 등의 작업을 수행하는 Python 코드)**

***

### **페이지 18**

**OpenCV로 비디오 작성하기 (cv2.VideoWriter)**

*   `VideoWriter`는 `VideoCapture`의 반대와 같습니다. 프레임을 읽는 대신, 프레임을 계속 추가하여 새로운 비디오 파일을 만듭니다.
    *   각 프레임에 대해 `.write(frame)`을 사용합니다.
    *   `cv2.VideoWriter_fourcc` → 코덱을 지정합니다:
        *   'XVID' → .avi (Windows/Linux에서 일반적으로 작동)
        *   'mp4v' → .mp4 (크로스플랫폼용으로 자주 사용됨)

**(코드 예시: 비디오를 읽어 프레임을 뒤집은 후 새로운 비디오 파일로 저장하는 Python 코드)**

***

### **페이지 19**

**라이브 비디오 입력 소개**

*   **이제 라이브 비디오 입력으로 넘어가 봅시다.**
    *   지금까지는 비디오 파일을 읽었습니다. 하지만 많은 실제 애플리케이션에서는 **실시간 비디오 스트림**을 처리해야 합니다.

*   **왜 이것이 중요한가?**
    *   실시간 입력을 통해 **대화형 애플리케이션**(예: 제스처 인식, 실시간 객체 탐지)을 구축할 수 있습니다.
    *   미리 녹화된 데이터에 국한되지 않고, **지금 바로** 일어나고 있는 일을 직접 캡처하고 분석할 수 있습니다.
    *   이는 감시 카메라, 자율 주행, AR/VR과 같은 시스템의 기반이 됩니다.

*   **비디오는 어디서 오는가?**
    *   노트북의 **내장 카메라**를 사용할 수 있습니다.
    *   또는 **외부 웹캠**을 연결할 수 있습니다 — OpenCV는 두 소스 모두에서 캡처할 수 있습니다.
    *   OpenCV에서는 두 가지 모두 동일한 인터페이스로 접근합니다:
        *   `cv2.VideoCapture(0)` # 기본 (내장) 카메라
        *   `cv2.VideoCapture(1)` # 첫 번째 외부 웹캠
    *   라이브 비디오 입력을 통해 우리 프로그램이 실제 세계와 상호작용할 수 있습니다. OpenCV를 사용하면 파일 입력과 라이브 카메라 입력 간을 쉽게 전환할 수 있습니다.

***

### **페이지 20**

**라이브 비디오 입력 소개**

*   **비디오는 어디서 오는가?**
    *   노트북의 **내장 카메라**를 사용할 수 있습니다.
    *   또는 **외부 웹캠**을 연결할 수 있습니다 — OpenCV는 두 소스 모두에서 캡처할 수 있습니다.
    *   OpenCV에서는 두 가지 모두 동일한 인터페이스로 접근합니다:
        *   `cv2.VideoCapture(0)` # 기본 (내장) 카메라
        *   `cv2.VideoCapture(1)` # 첫 번째 외부 웹캠
    *   라이브 비디오 입력을 통해 우리 프로그램이 실제 세계와 상호작용할 수 있습니다. OpenCV를 사용하면 파일 입력과 라이브 카메라 입력 간을 쉽게 전환할 수 있습니다.

**(이미지: 노트북 내장 카메라와 외부 USB 웹캠)**

***

### **페이지 21**

**라이브 비디오 입력 소개**

*   **비디오는 어디서 오는가?**
    *   노트북의 **내장 카메라**를 사용할 수 있습니다.
    *   또는 **외부 웹캠**을 연결할 수 있습니다 — OpenCV는 두 소스 모두에서 캡처할 수 있습니다.
    *   OpenCV에서는 두 가지 모두 동일한 인터페이스로 접근합니다:
        *   `cv2.VideoCapture(0)` # 기본 (내장) 카메라
        *   `cv2.VideoCapture(1)` # 첫 번째 외부 웹캠
    *   라이브 비디오 입력을 통해 우리 프로그램이 실제 세계와 상호작용할 수 있습니다. OpenCV를 사용하면 파일 입력과 라이브 카메라 입력 간을 쉽게 전환할 수 있습니다.

**(이미지: 두 개의 다른 카메라(Cam 0, Cam 1)에서 동시에 비디오를 캡처하는 예시)**

***

### **페이지 22**

**예시: 비디오를 위한 키보드 UI**

*   **p** → 일시정지/재생 토글 (일시정지는 새 프레임 읽기를 멈추지만 창은 열려 있음)
*   **s** → 현재 프레임의 스냅샷 저장
*   **g** → 그레이스케일 모드
*   **c** → 컬러 모드로 복귀
*   **q** 또는 **ESC** → 프로그램 종료

*   단지 `cv2.waitKey()`와 몇 개의 `if` 문만으로 **키보드 기반 UI**를 구축할 수 있습니다: 일시정지/재생, 저장, 모드 전환, 종료.
*   이는 OpenCV가 외부 GUI 프레임워크 없이도 이미 경량 애플리케이션 인터페이스처럼 작동할 수 있음을 보여줍니다.

**(코드 예시: 비디오 재생 중 키보드 입력에 따라 일시정지, 저장, 모드 변경 등을 제어하는 Python 코드)**

***

### **페이지 23**

**GUI란 무엇인가?**

*   **GUI = 그래픽 사용자 인터페이스 (Graphical User Interface)**
    *   사용자가 창, 버튼, 슬라이더, 마우스 클릭을 사용하여 **프로그램과 시각적으로 상호작용**하는 방법입니다.
    *   명령만 입력하는 CLI(명령줄 인터페이스)의 반대 개념입니다.

*   **OpenCV에서**
    *   GUI 기능은 우리의 비전 프로그램을 **상호작용적**으로 만들 수 있게 해줍니다.
    *   단순히 이미지나 비디오를 보여주는 대신, 다음을 할 수 있습니다:
        *   창 생성
        *   **트랙바(슬라이더)**로 매개변수 제어
        *   **마우스 클릭**이나 그리기 이벤트에 응답
        *   여러 창 관리

*   OpenCV의 GUI 함수를 사용하면 간단한 비전 코드를 실시간으로 매개변수를 조정하고, 그리고, 테스트할 수 있는 대화형 애플리케이션으로 바꿀 수 있습니다.

***

### **페이지 24**

**GUI란 무엇인가?**

*   창 생성,
*   **트랙바(슬라이더)**로 매개변수 제어,
*   **마우스 클릭**이나 그리기 이벤트에 응답,
*   여러 창 관리.

**(이미지: OpenCV GUI를 사용하여 마우스로 십자선을 그리고 트랙바로 이미지 크기를 조절하는 예시)**

***

### **페이지 25**

**GUI란 무엇인가?**

*   창 생성,
*   **트랙바(슬라이더)**로 매개변수 제어,
*   **마우스 클릭**이나 그리기 이벤트에 응답,
*   여러 창 관리.

**(이미지: 여러 개의 트랙바를 사용하여 이미지 파라미터를 실시간으로 조절하는 예시)**

***

### **페이지 26**

**트랙바 (슬라이더)**

*   사용자가 드래그하여 값을 선택할 수 있게 하는 슬라이더 위젯입니다.
    *   `cv2.createTrackbar(name, window_name, value, max_value, callback)`으로 생성됩니다.
    *   `cv2.imshow`로 생성된 창에 부착합니다.
    *   슬라이더가 움직이면 프로그램의 변수를 업데이트합니다.

*   **왜 유용한가?**
    *   코드를 변경하지 않고도 매개변수를 대화형으로 조정할 수 있습니다.
    *   **사용 사례 예시:**
        *   이진 이미지의 임계값
        *   캐니 엣지 검출의 최소/최대값
        *   밝기/대비 조정

***

### **페이지 27**

**트랙바 (슬라이더)**

*   **왜 유용한가?**
    *   코드를 변경하지 않고도 매개변수를 대화형으로 조정할 수 있습니다.
    *   **사용 사례 예시:**
        *   이진 이미지의 임계값
        *   캐니 엣지 검출의 최소/최대값
        *   밝기/대비 조정

**(이미지: 트랙바를 사용하여 이진화 임계값을 조절하여 꽃 이미지의 윤곽을 다르게 표현하는 예시)**

***

### **페이지 28**

**OpenCV의 트랙바 (슬라이더)**

*   **코드 예시: 대화형 임계값 처리**

**(코드 예시: 트랙바를 움직여 실시간으로 이미지의 이진화 임계값을 변경하는 Python 코드)**

*   `createTrackbar`는 **Result** 창에 **Threshold**라는 이름의 슬라이더를 부착합니다.
*   슬라이더를 움직이면 `on_change(value)`가 자동으로 호출됩니다.
*   콜백 함수 내부에서 슬라이더 값으로 이진 임계값을 적용합니다.
*   슬라이더를 드래그하여 **실시간으로 이미지가 업데이트되는 것**을 볼 수 있습니다.

***

### **페이지 29**

**콜백이란 무엇인가?**

*   **정의**
    *   **콜백 함수**는 사용자가 직접 호출하는 것이 아니라, 특정 이벤트가 발생했을 때 **시스템이나 라이브러리에 의해 자동으로 호출**되는 함수입니다.

*   **핵심 아이디어**
    *   사용자는 함수를 "등록"합니다.
    *   나중에 이벤트(슬라이더 움직임, 버튼 누름, 마우스 클릭)가 발생하면, 시스템이 사용자를 대신하여 함수를 호출합니다.

*   **비유**
    *   **알람 시계**를 설정하는 것을 생각해보세요:
        *   사용자가 직접 알람을 울리지 않습니다.
        *   정해진 시간에 알람이 사용자를 깨우기 위해 "콜백"합니다.
    *   마찬가지로, OpenCV는 트랙바가 변경될 때 사용자의 함수를 호출합니다.

*   **OpenCV에서**
    *   예시: `cv2.createTrackbar(name, window, value, max, on_change)`
    *   여기서 `on_change`가 **콜백**입니다.
    *   슬라이더가 움직일 때마다 OpenCV는 자동으로 `on_change(new_value)`를 호출합니다.

*   콜백은 이벤트가 발생하기를 **기다렸다가** 사용자가 아닌 **시스템에 의해** 실행되는 함수입니다.

***

### **페이지 30**

**예시: functools.partial을 사용하여 콜백에 추가 인자 전달하기**

*   `partial`을 사용하면 OpenCV가 보통 슬라이더 값만 제공함에도 불구하고 콜백 함수에 **추가 인자**를 전달할 수 있습니다.

**(코드 예시: `functools.partial`을 사용하여 이미지 데이터를 콜백 함수에 추가 인자로 전달하는 Python 코드)**

***

### **페이지 31**

**비디오와 트랙바**

*   **캐니 엣지 검출**
    *   슬라이더를 움직일 때:
    *   캐니 임계값이 실시간으로 변경됩니다.
    *   엣지가 즉시 더 두꺼워지거나 얇아지는 것을 볼 수 있습니다.

*   **`getTrackbarPos()`로 폴링하기**
    *   **이미지**의 경우, 트랙바는 보통 콜백을 사용합니다.
    *   **비디오**의 경우, 루프 안에서 `getTrackbarPos()`로 값을 폴링하는 것이 더 쉽습니다.
    *   이렇게 하면 비디오가 계속 새로고침되고 각 프레임에 트랙바 값이 적용됩니다.

**(코드 예시: 비디오에 캐니 엣지 검출을 적용하면서 트랙바로 임계값을 실시간 조절하는 Python 코드)**

***

### **페이지 32**

**트랙바: 콜백 vs 폴링**

*   **정지 이미지의 경우 (콜백이 잘 작동함)**
    *   슬라이더를 움직이면 OpenCV가 콜백 함수를 호출합니다.
    *   콜백 내부에서 동일한 이미지를 다시 처리하고 즉시 결과를 업데이트할 수 있습니다.
    *   이는 이미지가 정적이기 때문에 작동합니다 — 변경되는 것은 슬라이더 값뿐입니다.

*   **비디오의 경우 (콜백은 적합하지 않음)**
    *   콜백은 슬라이더 값만 제공하고 현재 비디오 프레임은 제공하지 않습니다.
    *   비디오 프레임은 계속 변하기 때문에 프레임과 슬라이더 값이 함께 필요합니다.
    *   콜백만으로는 슬라이더가 움직일 때만 트리거되기 때문에 이를 처리할 수 없습니다.

*   **해결책: `getTrackbarPos()`로 폴링하기**
    *   비디오 처리에서는 `getTrackbarPos()`를 **루프 안에** 넣습니다.
    *   **각 프레임마다:**
        *   현재 슬라이더 값을 읽습니다.
        *   해당 값으로 현재 비디오 프레임을 처리합니다.
        *   업데이트된 결과를 보여줍니다.
    *   이 방법을 통해 비디오와 트랙바가 실시간으로 동기화됩니다.

*   **이미지의 경우 콜백이 좋지만, 비디오의 경우 라이브 프레임과 슬라이더 값을 결합하기 위해 루프 내에서 폴링(`getTrackbarPos`)을 사용해야 합니다.**

***

### **페이지 33**

**트랙바 매뉴얼**

*   **트랙바 생성 구문**
    *   `cv2.createTrackbar(trackbar_name, window_name, initial_value, max_value, callback)`
        *   `trackbar_name`: 슬라이더 제목 (문자열)
        *   `window_name`: 기존 `imshow` 창과 일치해야 함
        *   `initial_value`: 시작 위치
        *   `max_value`: 최대 슬라이더 값
        *   `callback`: 값이 변경될 때 호출할 함수 (`lambda` 또는 `on_change` 가능)

*   **값 가져오기**
    *   콜백을 사용하면 슬라이더 값이 자동으로 전달됩니다.
    *   콜백 없이 또는 비디오에서는 다음을 사용합니다:
        *   `pos = cv2.getTrackbarPos("Threshold", "Result")`

*   **트랙바 값은 항상 정수**
    *   슬라이더 값은 정수만 가능합니다.
    *   부동소수점(예: 0.0–1.0)을 표현하려면 **수동으로 스케일링**해야 합니다:
        *   `alpha = cv2.getTrackbarPos("Alpha", "Result") / 100`

*   **창당 여러 트랙바**
    *   하나의 창에 여러 개의 트랙바를 부착할 수 있습니다.
    *   일반적인 예: 캐니 엣지 검출을 위한 `minVal`과 `maxVal`.

*   **콜백 유연성**
    *   폴링만 원할 경우 콜백은 더미 함수(`pass`)가 될 수 있습니다.
    *   또는 이미지를 직접 재처리할 수 있습니다 (정적 이미지의 경우).

***

### **페이지 34**

**요약**

*   트랙바는 OpenCV 프로그램을 **상호작용적**으로 만듭니다.
*   항상 창에 연결됩니다 (`window_name`을 통해).
*   값은 정수입니다 (부동소수점이 필요하면 스케일링).
*   **이미지**의 경우: 콜백이 좋습니다.
*   **비디오**의 경우: 루프 내에서 폴링(`getTrackbarPos`)을 사용합니다.

*   **사용 사례**: 임계값 처리, 엣지 검출, 밝기/대비, 파라미터 튜닝 등.
*   **제한 사항**: 정수 전용 값, 하나의 창에만 연결, 간단한 UI만 가능 (탭이나 복잡한 위젯 없음).

***

### **페이지 35**

**마우스 GUI란 무엇인가?**

*   **정의**
    *   OpenCV의 마우스 GUI는 사용자가 **마우스**를 사용하여 이미지나 비디오 창과 상호작용하게 하는 것을 의미합니다.
    *   프로그램은 창 내부의 **클릭, 드래그, 또는 움직임**과 같은 이벤트에 반응할 수 있습니다.

*   **왜 중요한가**
    *   지금까지는 상호작용을 위해 키보드(`waitKey`)나 슬라이더(`trackbars`)를 사용했습니다.
    *   하지만 많은 비전 애플리케이션에서는 사용자가 이미지 위를 직접 **가리키거나 그려야** 합니다.
        *   예: 관심 영역(ROI) 선택, 객체 라벨링, 도형 그리기.

*   **OpenCV에서**
    *   `cv2.setMouseCallback(window_name, callback_function)`으로 구현됩니다.
    *   콜백 함수는 다음에 대한 정보를 받습니다:
        *   **이벤트 유형** (왼쪽 클릭, 오른쪽 클릭, 더블 클릭, 마우스 이동, 드래그 등)
        *   이벤트가 발생했을 때의 커서 **좌표 (x, y)**
        *   **플래그** (예: 이동 중 눌린 키).

*   마우스 GUI는 단순히 이미지를 보는 것을 넘어, OpenCV 창에서 직접 클릭, 선택, 그리기를 통해 이미지와 상호작용할 수 있게 해줍니다.

***

### **페이지 36**

**마우스 GUI란 무엇인가?**

*   **정의**
    *   OpenCV의 마우스 GUI는 사용자가 **마우스**를 사용하여 이미지나 비디오 창과 상호작용하게 하는 것을 의미합니다.
    *   프로그램은 창 내부의 **클릭, 드래그, 또는 움직임**과 같은 이벤트에 반응할 수 있습니다.

**(이미지: OpenCV GUI를 사용하여 마우스로 십자선을 그리고 트랙바로 이미지 크기를 조절하는 예시)**

***

### **페이지 37**

**OpenCV의 마우스 이벤트**

*   **일반적인 마우스 이벤트**
    *   `cv2.EVENT_LBUTTONDOWN` → 왼쪽 마우스 버튼 누름
    *   `cv2.EVENT_LBUTTONUP` → 왼쪽 마우스 버튼 뗌
    *   `cv2.EVENT_RBUTTONDOWN` → 오른쪽 마우스 버튼 누름
    *   `cv2.EVENT_MOUSEMOVE` → 마우스 이동
    *   `cv2.EVENT_LBUTTONDBLCLK` → 왼쪽 더블 클릭

*   `cv2.setMouseCallback("Image", mouse_callback)`
    *   `mouse_callback` 함수를 "Image" 창에 바인딩합니다.
    *   해당 창에서 마우스 이벤트가 발생할 때마다 OpenCV는 이 함수를 호출합니다.

*   **event, x, y, flags, param**
    *   `event`: 마우스 이벤트 유형
    *   `x, y`: 이벤트 발생 시 마우스 좌표
    *   `flags`: 이동 중 수정자 키(Ctrl, Shift 등) 또는 마우스 버튼 상태
    *   `param`: 선택적 사용자 데이터 (추가 정보 전달 가능)

*   **이 예시에서:**
    *   **왼쪽 클릭** → 빨간 점을 그리고 좌표를 출력합니다.
    *   **오른쪽 클릭** → 좌표만 출력합니다.
    *   **마우스 이동** → (무시되지만, 커서 위치를 표시하는 데 사용될 수 있음).

**(코드 예시 및 출력: 마우스 클릭 위치에 따라 다른 동작을 수행하는 Python 코드와 그 결과)**

***

### **페이지 38**

**OpenCV에서의 마우스 드래깅**

*   **무엇인가?**
    *   마우스 드래깅은 마우스 버튼을 **클릭하고 누른 상태**에서 버튼을 떼기 전에 **마우스를 움직이는 것**을 의미합니다.
    *   OpenCV에서는 다음과 같은 이벤트 시퀀스로 표현됩니다:
        *   `EVENT_LBUTTONDOWN` → 왼쪽 버튼이 눌렸을 때.
        *   `EVENT_MOUSEMOVE` (버튼이 눌린 동안) → 드래깅 움직임.
        *   `EVENT_LBUTTONUP` → 버튼이 떼어졌을 때.

*   **왜 유용한가?**
    *   드래깅은 사용자가 이미지에서 **관심 영역(ROI)을 선택**하게 하는 자연스러운 방법입니다.
    *   주로 다음과 같은 용도로 사용됩니다:
        *   이미지에서 영역 자르기.
        *   객체 주위에 경계 상자 그리기.
        *   대화형 라벨링 및 주석 달기.

**(이미지: 영화 '쥬라기 공원'의 한 장면에서 공룡 머리 주위에 드래그하여 ROI를 선택하고 해당 부분을 잘라내는 예시)**

***

### **페이지 39**

**OpenCV에서의 마우스 드래깅**

*   **무엇인가?**
    *   마우스 드래깅은 마우스 버튼을 **클릭하고 누른 상태**에서 버튼을 떼기 전에 **마우스를 움직이는 것**을 의미합니다.
    *   OpenCV에서는 다음과 같은 이벤트 시퀀스로 표현됩니다:
        *   `EVENT_LBUTTONDOWN` → 왼쪽 버튼이 눌렸을 때.
        *   `EVENT_MOUSEMOVE` (버튼이 눌린 동안) → 드래깅 움직임.
        *   `EVENT_LBUTTONUP` → 버튼이 떼어졌을 때.

*   **왜 유용한가?**
    *   드래깅은 사용자가 이미지에서 **관심 영역(ROI)을 선택**하게 하는 자연스러운 방법입니다.
    *   주로 다음과 같은 용도로 사용됩니다:
        *   이미지에서 영역 자르기.
        *   객체 주위에 경계 상자 그리기.
        *   대화형 라벨링 및 주석 달기.

*   **OpenCV는 어떻게 처리하는가**
    *   `cv2.setMouseCallback(window, callback)`을 사용하면 마우스 이벤트가 발생할 때마다 OpenCV가 함수를 호출합니다.
    *   콜백 내부에서 이벤트 유형과 마우스 좌표(x, y)를 확인할 수 있습니다.
    *   **ROI 선택을 위해:**
        *   `LBUTTONDOWN`에서 시작점을 기록합니다.
        *   `MOUSEMOVE`에서 사각형을 동적으로 업데이트합니다.
        *   `LBUTTONUP`이 발생하면 사각형을 확정합니다.

*   **예시 흐름:**
    *   사용자가 (x0, y0)에서 왼쪽 마우스 버튼을 누릅니다.
    *   드래그하는 동안 (x0, y0)과 현재 (x, y) 사이에 사각형이 그려집니다.
    *   버튼을 떼면 ROI가 확정되고 잘라내거나 저장할 수 있습니다.

***

### **페이지 40**

**OpenCV에서의 마우스 드래깅**

*   **Img**: 원본 이미지 (파일에서 로드됨).
*   **Vis**: 원본 이미지의 **작업용 복사본**.
*   **drag**: 사용자가 현재 드래그 중인지 여부를 나타내는 **불리언 플래그**.
*   **x0, y0**: 사용자가 왼쪽 마우스 버튼을 처음 눌렀을 때의 **시작 좌표**.
*   **last_roi**: 가장 최근에 선택된 **ROI** (잘라낸 이미지).

**(코드 예시: 마우스 드래그로 ROI를 선택하고, 's' 키로 저장하며, 'r' 키로 리셋하는 Python 코드의 일부)**

***

### **페이지 41**

**OpenCV에서의 마우스 드래깅**

*   OpenCV에서의 마우스 드래그는 단순히 **마우스 다운 → 이동 → 마우스 업** 이벤트의 조합입니다. 우리는 이 패턴을 사용하여 사용자가 이미지에서 직접 영역을 그리고 선택할 수 있도록 합니다.

**(코드 예시: 마우스 이벤트(LBUTTONDOWN, MOUSEMOVE, LBUTTONUP)를 처리하여 드래그하는 동안 사각형을 그리고, 드래그가 끝나면 ROI를 추출하는 콜백 함수의 Python 코드)**

***

### **페이지 42**

**OpenCV의 마우스 휠**

*   OpenCV 마우스 콜백에는 휠에 대한 이벤트 상수가 있습니다:
    *   `cv2.EVENT_MOUSEWHEEL` → 수직 스크롤
    *   `cv2.EVENT_MOUSEHWHEEL` → 수평 스크롤

*   이벤트가 트리거될 때, `flags` 매개변수는 휠 방향과 단계를 인코딩합니다.
    *   **양수 값** → 앞으로 스크롤 (휠 업)
    *   **음수 값** → 뒤로 스크롤 (휠 다운)

*   마우스 휠 이벤트는 모든 시스템에서 사용 가능하지 않을 수 있습니다.
    *   OpenCV가 **Qt 지원**으로 빌드되었을 때 잘 작동합니다 (예: Qt 백엔드를 사용하는 Windows/Linux).
    *   일부 기본 빌드(예: `pip install opencv-python`)에서는 휠 이벤트가 트리거되지 않을 수 있습니다.

*   휠 이벤트가 작동하지 않는 경우, **키보드 단축키** (+, -)를 사용하여 줌/스크롤을 시뮬레이션할 수 있습니다.

**(코드 예시 및 출력: 마우스 휠 스크롤 방향에 따라 다른 메시지를 출력하는 Python 코드와 그 결과)**

***

### **페이지 43**

**오른쪽 클릭 + 드래그 줌 (Ctrl 키 사용)**

*   OpenCV의 마우스 이벤트에는 **수정자 키 플래그**가 포함됩니다.
    *   `cv2.EVENT_FLAG_CTRLKEY` → Ctrl 키가 눌림
    *   `cv2.EVENT_FLAG_SHIFTKEY` → Shift 키가 눌림
    *   `cv2.EVENT_FLAG_ALTKEY` → Alt 키가 눌림

*   이 플래그들을 확인하여 사용자가 **Ctrl + 오른쪽 버튼**을 누르고 있을 때만 줌이 발생하도록 할 수 있습니다.

*   마우스 이벤트를 수정자 키(Ctrl, Shift, Alt)와 결합하여 정밀하고 사용자 친화적인 상호작용을 설계할 수 있습니다.

*   **조합된 상호작용에 유용함:**
    *   **Ctrl + 드래그** → 줌
    *   **Shift + 클릭** → 다중 선택
    *   **Alt + 클릭** → 사용자 정의 함수

**(코드 예시: Ctrl 키를 누른 상태에서 오른쪽 마우스 버튼을 드래그하여 이미지를 줌인/줌아웃하는 Python 코드)**

***

### **페이지 44**

**마우스 버튼 플래그**

*   이 플래그들은 이벤트가 발생하는 동안 마우스 버튼의 **현재 상태**를 나타냅니다.
    *   `cv2.EVENT_FLAG_LBUTTON` → 왼쪽 버튼이 눌린 상태
    *   `cv2.EVENT_FLAG_RBUTTON` → 오른쪽 버튼이 눌린 상태
    *   `cv2.EVENT_FLAG_MBUTTON` → 중간 버튼이 눌린 상태

*   **예시:**
    *   왼쪽 버튼을 누른 상태로 마우스를 움직이면, 이벤트는 `EVENT_MOUSEMOVE`가 되고 플래그는 `EVENT_FLAG_LBUTTON`이 됩니다 → 드래그 감지에 완벽합니다.

***

### **페이지 45**

**비디오 프레임에서의 마우스 이벤트**

*   마우스 이벤트는 비디오 창에서도 여전히 작동합니다.
    *   콜백은 "Video" 창에 연결됩니다.
    *   비디오가 재생되는 동안 이벤트(LBUTTONDOWN, MOUSEMOVE, LBUTTONUP)가 트리거됩니다.

*   **ROI 선택을 위한 드래깅:**
    *   **왼쪽 클릭** → 시작점 기록
    *   **버튼을 누른 채로 이동** → 미리보기 사각형 그리기
    *   **버튼 떼기** → 현재 프레임에 사각형 확정

*   비디오는 계속 새로고침되기 때문에, 드래그하는 동안 비디오를 영구적으로 변경하지 않고 사각형을 표시하기 위해 **현재 프레임의 복사본**(`temp = ctc[“frame”].copy()`)을 사용합니다.

**(코드 예시: 재생 중인 비디오 위에서 마우스 드래그로 ROI를 선택하는 Python 코드)**

***

### **페이지 46**

**마우스 GUI 요약**

*   **마우스 GUI란?**
    *   마우스를 사용하여 OpenCV 창과 상호작용하는 것.
    *   클릭, 드래그, 이동, 스크롤 이벤트를 모두 캡처할 수 있습니다.

*   **핵심 함수**
    *   `cv2.setMouseCallback(window, callback, param)` → 함수를 창에 바인딩합니다.
    *   콜백은 `(event, x, y, flags, param)`을 받습니다.

*   **중요 이벤트**
    *   `EVENT_LBUTTONDOWN / UP` → **왼쪽 클릭**
    *   `EVENT_RBUTTONDOWN / UP` → **오른쪽 클릭**
    *   `EVENT_MOUSEMOVE` → **커서 이동**
    *   `EVENT_LBUTTONDBLCLK` → **더블 클릭**
    *   `EVENT_MOUSEWHEEL / HWHEEL` → **스크롤 (수직/수평)**

*   **플래그**
    *   `EVENT_FLAG_LBUTTON`, `EVENT_FLAG_RBUTTON`, `EVENT_FLAG_MBUTTON`
    *   `EVENT_FLAG_CTRLKEY`, `EVENT_FLAG_SHIFTKEY`, `EVENT_FLAG_ALTKEY`
    *   드래깅이나 조합된 동작(Ctrl+드래그, Shift+클릭 등)을 감지하는 데 사용됩니다.

*   **일반적인 사용 사례**
    *   클릭 시 마우스 좌표 표시
    *   도형 그리기 (점, 선, 사각형)
    *   드래그로 ROI 선택
    *   드래그 + 수정자 키로 줌/팬
    *   스크롤 기반 상호작용 (지원되는 경우)

***

### **페이지 47**

**OpenCV의 창 관리**

*   지금까지 `imshow`는 그냥 창을 열기만 했습니다. 하지만 OpenCV는 실제로는 창을 크기 조절하고, 이동하고, 전체 화면으로 만드는 등 GUI 앱처럼 완벽한 제어를 제공합니다.

*   **왜 창을 관리해야 하는가?**
    *   기본적으로 `cv2.imshow()`는 간단한 자동 크기 조절 창을 만듭니다.
    *   하지만 실제 애플리케이션에서는 다음이 필요할 수 있습니다:
        *   고정 크기 창
        *   화면의 특정 위치로 창 이동
        *   여러 출력을 나란히 처리

**(이미지: 원본 이미지와 잘라낸 이미지를 두 개의 별도 창으로 보여주는 예시)**

***

### **페이지 48**

**OpenCV의 창 관리**

*   지금까지 `imshow`는 그냥 창을 열기만 했습니다. 하지만 OpenCV는 실제로는 창을 크기 조절하고, 이동하고, 전체 화면으로 만드는 등 GUI 앱처럼 완벽한 제어를 제공합니다.

*   **왜 창을 관리해야 하는가?**
    *   기본적으로 `cv2.imshow()`는 간단한 자동 크기 조절 창을 만듭니다.
    *   하지만 실제 애플리케이션에서는 다음이 필요할 수 있습니다:
        *   고정 크기 창
        *   화면의 특정 위치로 창 이동
        *   여러 출력을 나란히 처리

**(이미지: 여러 개의 작은 창을 화면에 배열하여 표시하는 예시)**

***

### **페이지 49**

**OpenCV의 창 관리**

*   지금까지 `imshow`는 그냥 창을 열기만 했습니다. 하지만 OpenCV는 실제로는 창을 크기 조절하고, 이동하고, 전체 화면으로 만드는 등 GUI 앱처럼 완벽한 제어를 제공합니다.

*   **왜 창을 관리해야 하는가?**
    *   기본적으로 `cv2.imshow()`는 간단한 자동 크기 조절 창을 만듭니다.
    *   하지만 실제 애플리케이션에서는 다음이 필요할 수 있습니다:
        *   고정 크기 창
        *   화면의 특정 위치로 창 이동
        *   여러 출력을 나란히 처리

*   **창 유형 생성/설정**
    *   `cv2.namedWindow("MyWindow", cv2.WINDOW_NORMAL)` # 크기 조절 가능
    *   `cv2.namedWindow("MyWindow", cv2.WINDOW_AUTOSIZE)` # 기본값

*   **창 크기 조절**
    *   `cv2.resizeWindow("MyWindow", 600, 400)`

*   **화면에서 창 이동**
    *   `cv2.moveWindow("MyWindow", 100, 200)` # (x, y) 위치

*   **전체 화면 또는 반 화면**
    *   `cv2.setWindowProperty("MyWindow", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)`
    *   `cv2.setWindowProperty("MyWindow", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_NORMAL)`

*   **일반적인 사용 사례**
    *   원본 vs 처리된 이미지를 나란히 표시 → `moveWindow()`
    *   큰 이미지를 화면에 맞게 크기 조절 → `resizeWindow()`
    *   전체 화면 모드로 데모 시연 (프레젠테이션 또는 키오스크 모드)

***

### **페이지 50**

**OpenCV의 오버레이 및 주석**

*   **오버레이/주석이란 무엇인가?**
    *   **오버레이**: 이미지나 비디오 프레임 위에 추가 그래픽을 그리는 것.
    *   **주석**: 이미지에 있는 것을 강조하거나 설명하기 위해 시각적 정보(상자, 마커, 텍스트)를 추가하는 것.

*   **왜 필요한가?**
    *   순수한 이미지만으로는 결과를 전달하기에 충분하지 않은 경우가 많습니다.
    *   **오버레이는 다음을 돕습니다:**
        *   탐지된 객체 강조 (경계 상자, 원)
        *   관심 영역(ROI) 표시
        *   측정값 표시 (길이, 각도, 좌표)
        *   라벨 또는 지침 추가 (텍스트 오버레이)
    *   컴퓨터 비전 데모에서 오버레이는 결과를 **명확하고 해석 가능하게** 만듭니다.

**(이미지: OpenCV 로고와 함께 사각형, 원, 선, 폴리라인, 텍스트 등 다양한 그리기 기능을 보여주는 그림)**

***

### **페이지 51**

**OpenCV의 일반적인 오버레이 함수**

*   **도형 그리기**
    *   `cv2.line()` – 선 그리기
    *   `cv2.rectangle()` – 사각형 그리기
    *   `cv2.circle()` – 원 그리기
    *   `cv2.polylines()` – 다각형 그리기

*   **텍스트 추가**
    *   `cv2.putText()` – 라벨, 제목, 주석 추가

*   **블렌딩**
    *   `cv2.addWeighted(img1, alpha, img2, beta, gamma)` – 두 이미지 결합 (예: 원본 이미지에 히트맵 오버레이)

*   오버레이와 주석은 컴퓨터 비전 결과를 **가시적이고 이해할 수 있는 것**으로 바꿉니다. 이것들이 없으면 결과는 숫자로만 남지만, 이것들이 있으면 명확한 스토리를 얻을 수 있습니다.

***

### **페이지 52**

**그리기 함수**

*   `line(img, pt1, pt2, color, thickness)` → 직선을 그립니다.
*   `rectangle(img, pt1, pt2, color, thickness)` → 윤곽선이 있는 사각형 (-1 = 채워짐).
*   `circle(img, center, radius, color, thickness)` → 원 (-1 = 채워짐).
*   `putText(img, text, org, font, scale, color, thickness)` → 이미지 위에 텍스트를 오버레이합니다.
*   **색상은 BGR 순서(파랑, 초록, 빨강)이며, RGB가 아닙니다.**

**(코드 예시 및 결과 이미지: 검은 배경에 선, 사각형, 원, 텍스트를 그리는 Python 코드와 그 결과물인 "OpenCV Drawing Demo" 이미지)**

***

### **페이지 53**

**그리기 함수**

*   `line(img, pt1, pt2, color, thickness)` → 직선을 그립니다.
*   `rectangle(img, pt1, pt2, color, thickness)` → 윤곽선이 있는 사각형 (-1 = 채워짐).
*   `circle(img, center, radius, color, thickness)` → 원 (-1 = 채워짐).
*   `putText(img, text, org, font, scale, color, thickness)` → 이미지 위에 텍스트를 오버레이합니다.
*   **색상은 BGR 순서(파랑, 초록, 빨강)이며, RGB가 아닙니다.**

**(이미지: 손가락 제스처를 인식하고 그 위에 선과 텍스트를 그리는 디지털 드로잉 예시)**

***

### **페이지 54**

**투명 오버레이가 있는 비디오 (블렌딩)**

*   `overlay = frame.copy()` → 임시 레이어를 만듭니다.
*   오버레이 위에 채워진 도형(예: 녹색 사각형)을 그립니다.
*   `cv2.addWeighted(overlay, alpha, frame, 1-alpha, 0)` → 오버레이를 원본 프레임과 블렌딩합니다.
    *   `alpha = 0.3` → 오버레이 투명도 (30%)
    *   `1-alpha = 0.7` → 원본 프레임 가중치
*   **결과**: 비디오 피드 위에 반투명 녹색 상자가 나타납니다.

**(코드 예시 및 결과 이미지: 비디오 위에 반투명 사각형과 텍스트를 오버레이하는 Python 코드와 그 결과물)**

***

### **페이지 55**

**고급 OpenCV 함수 (프로젝트용)**

1.  **기하학적 변환**
    *   `cv2.resize()` – 이미지 크기 조절 (업/다운 스케일링)
    *   `cv2.warpAffine()`, `cv2.warpPerspective()` – 회전, 이동, 원근 변환
    *   **유용한 분야**: 이미지 정렬, AR, 포스터 합성

2.  **이미지 블렌딩 및 마스킹**
    *   `cv2.addWeighted()` – 투명도를 가진 이미지 블렌딩
    *   `cv2.bitwise_and/or/xor/not()` – 마스킹 연산
    *   `cv2.seamlessClone()` – 자연스러운 이미지 복제
    *   **유용한 분야**: 사진 편집, 배경 교체

3.  **형태학적 연산**
    *   `cv2.erode()`, `cv2.dilate()` – 노이즈 제거, 모양 수정
    *   `cv2.morphologyEx()` – 열림, 닫힘, 그래디언트, 탑햇
    *   **유용한 분야**: 이진 이미지 정리, 텍스트 전처리

4.  **윤곽선 및 모양 분석**
    *   `cv2.findContours()`, `cv2.drawContours()` – 객체 경계
    *   `cv2.boundingRect()`, `cv2.approxPolyDP()` – 사각형, 다각형
    *   `cv2.contourArea()`, `cv2.arcLength()` – 모양 측정
    *   **유용한 분야**: 객체 탐지, 모양 인식

***

### **페이지 56**

**고급 OpenCV 함수 (프로젝트용)**

5.  **특징 탐지**
    *   `cv2.goodFeaturesToTrack()` – 코너 탐지 (Shi–Tomasi)
    *   `cv2.HoughLines()`, `cv2.HoughCircles()` – 선 및 원 탐지
    *   (소개만) SIFT, ORB 특징 매칭
    *   **유용한 분야**: AR 마커, 패턴 탐지

6.  **움직임 및 객체 추적**
    *   `cv2.calcOpticalFlowPyrLK()` – 옵티컬 플로우 추적
    *   `cv2.meanShift()`, `cv2.CamShift()` – 객체 추적
    *   `cv2.createBackgroundSubtractorMOG2()` – 배경 제거
    *   **유용한 분야**: 비디오 추적, 움직임 분석

7.  **색 공간 및 히스토그램**
    *   `cv2.cvtColor()` – HSV, LAB 등
    *   `cv2.inRange()` – 색상 마스킹
    *   `cv2.calcHist()`, `cv2.equalizeHist()` – 히스토그램 분석
    *   **유용한 분야**: 색상 기반 탐지, 조명 보정

8.  **템플릿 매칭**
    *   `cv2.matchTemplate()`, `cv2.minMaxLoc()` – 작은 패턴/템플릿 찾기
    *   **유용한 분야**: 아이콘 탐지, 로고 매칭

***

### **페이지 57**

**OpenCV를 넘어서: 유용한 라이브러리 및 도구**

*   **1. MediaPipe (by Google)**
    *   **얼굴 탐지, 손 추적, 자세 추정**을 위한 사전 훈련된 파이프라인
    *   사용하기 쉽고, 웹캠에서 실시간 성능 제공
    *   **유용한 분야**: 제스처 제어, 피트니스 앱, AR 효과

*   **2. dlib**
    *   얼굴 탐지 및 얼굴 랜드마크 (68점 검출기)
    *   얼굴 인식 (인코딩 및 매칭)
    *   **유용한 분야**: 얼굴 필터, 신원 확인

*   **3. scikit-image**
    *   고급 이미지 처리 (노이즈 제거, 분할, 특징 추출)
    *   더 많은 알고리즘으로 OpenCV를 보완
    *   **유용한 분야**: 과학적 이미지 분석, 전처리

*   **4. OpenPose (CMU)**
    *   실시간 다중 인원 2D/3D 자세 추정
    *   MediaPipe보다 더 발전했지만 더 무거움
    *   **유용한 분야**: 모션 캡처, 스포츠 분석, HCI

*   **5. PyTorch / TensorFlow + OpenCV**
    *   분류/분할을 위한 딥러닝 모델 사용
    *   OpenCV(I/O, 전처리, 시각화)를 DL 프레임워크와 결합
    *   **유용한 분야**: 객체 탐지, 이미지 인식, 의료 AI 데모

***

### **페이지 58**

**OpenCV를 넘어서: 유용한 라이브러리 및 도구**

*   **6. EasyOCR / Tesseract**
    *   이미지에서 광학 문자 인식(OCR)
    *   **유용한 분야**: 사진 속 텍스트 읽기, 문서 스캔

*   **7. 기타 재미있고 유용한 도구**
    *   **pytesseract** – OpenCV 전처리를 이용한 간단한 OCR
    *   **OpenCV의 DNN 모듈** – 사전 훈련된 모델(YOLO, MobileNet 등) 직접 실행
    *   **ImageAI** – 객체 탐지/분류를 위한 간단한 인터페이스

*   **프로젝트 영감**
    *   **손 제스처 제어 게임** (MediaPipe hands + OpenCV 오버레이)
    *   **가상 메이크업 앱** (dlib 얼굴 랜드마크 + OpenCV 필터)
    *   **OCR 기반 번역기** (OpenCV 전처리 + Tesseract OCR + Google 번역 API)
    *   **자세 추정 피트니스 코치** (MediaPipe pose + OpenCV 비디오 분석)
    *   **AR 착용 앱** (OpenCV 원근 변환 + 마스크 블렌딩)

***

### **페이지 59**

**OpenCV × MediaPipe**

*   **MediaPipe란 무엇인가?**
    *   Google이 만든 **실시간 컴퓨터 비전**을 위한 경량 프레임워크.
    *   **자세, 손, 얼굴 랜드마크**를 위한 즉시 사용 가능한 모델을 제공합니다.
    *   CPU에서도 빠르게 실행되어 노트북 및 모바일 장치에 적합합니다.

**(이미지: MediaPipe를 활용한 운전자 졸음 감지, 비디오 제스처 제어, AI 피트니스 트레이너 등 다양한 애플리케이션 예시)**

***

### **페이지 60**

**OpenCV를 넘어서: 유용한 라이브러리 및 도구**

*   **MediaPipe란 무엇인가?**
    *   Google이 만든 **실시간 컴퓨터 비전**을 위한 경량 프레임워크.
    *   **자세, 손, 얼굴 랜드마크**를 위한 즉시 사용 가능한 모델을 제공합니다.
    *   CPU에서도 빠르게 실행되어 노트북 및 모바일 장치에 적합합니다.

**(이미지: MediaPipe를 사용하여 손의 랜드마크를 실시간으로 추적하는 예시)**

***

### **페이지 61**

**OpenCV를 넘어서: 유용한 라이브러리 및 도구**

*   **MediaPipe란 무엇인가?**
    *   Google이 만든 **실시간 컴퓨터 비전**을 위한 경량 프레임워크.
    *   **자세, 손, 얼굴 랜드마크**를 위한 즉시 사용 가능한 모델을 제공합니다.
    *   CPU에서도 빠르게 실행되어 노트북 및 모바일 장치에 적합합니다.

**(이미지: MediaPipe를 사용하여 얼굴 메쉬와 양손의 랜드마크를 동시에 추적하는 예시)**

***

### **페이지 62**

**OpenCV를 넘어서: 유용한 라이브러리 및 도구**

*   **MediaPipe란 무엇인가?**
    *   Google이 만든 **실시간 컴퓨터 비전**을 위한 경량 프레임워크.
    *   **자세, 손, 얼굴 랜드마크**를 위한 즉시 사용 가능한 모델을 제공합니다.
    *   CPU에서도 빠르게 실행되어 노트북 및 모바일 장치에 적합합니다.

**(이미지: YOLOv7과 MediaPipe의 인간 자세 추정 성능을 비교하는 예시)**

***

### **페이지 63**

**OpenCV를 넘어서: 유용한 라이브러리 및 도구**

*   **주요 기능**
    *   **신체 자세 (33개 랜드마크)** – 피트니스 코치, 동작 인식
    *   **손 자세 (21개 랜드마크)** – 제스처 제어, 가상 드로잉
    *   **얼굴 메쉬 (468개 랜드마크)** – AR 필터, 얼굴 추적, 머리 자세

**(이미지: MediaPipe의 신체 자세 랜드마크 33개의 위치와 번호를 보여주는 다이어그램)**

***

### **페이지 64**

**OpenCV를 넘어서: 유용한 라이브러리 및 도구**

*   **주요 기능**
    *   **신체 자세 (33개 랜드마크)** – 피트니스 코치, 동작 인식
    *   **손 자세 (21개 랜드마크)** – 제스처 제어, 가상 드로잉
    *   **얼굴 메쉬 (468개 랜드마크)** – AR 필터, 얼굴 추적, 머리 자세

**(이미지: MediaPipe의 손 자세 랜드마크 21개의 위치와 명칭을 보여주는 다이어그램)**

***

### **페이지 65**

**OpenCV를 넘어서: 유용한 라이브러리 및 도구**

*   **주요 기능**
    *   **신체 자세 (33개 랜드마크)** – 피트니스 코치, 동작 인식
    *   **손 자세 (21개 랜드마크)** – 제스처 제어, 가상 드로잉
    *   **얼굴 메쉬 (468개 랜드마크)** – AR 필터, 얼굴 추적, 머리 자세

**(이미지: MediaPipe의 얼굴 메쉬 468개 랜드마크의 상세한 위치와 이를 활용한 AR 필터 적용 예시)**

***

### **페이지 66**

**MediaPipe Hands 초기화: 두 가지 스타일**

*   **1) `with` 사용 (권장, 자동 정리)**
    *   **장점:**
        *   자동 리소스 관리 (`close()` 호출 필요 없음)
        *   짧은 스크립트에 더 깔끔하고 안전함

**(코드 예시: `with` 문을 사용하여 MediaPipe Hands 객체를 초기화하고 사용하는 Python 코드)**

***

### **페이지 67**

**MediaPipe Hands 초기화: 두 가지 스타일**

*   **2) 수동 생성 및 닫기**
    *   **장점:**
        *   객체의 수명 주기에 대한 더 많은 제어
        *   클래스가 여러 함수/루프에 걸쳐 사용될 때 유용함

**(코드 예시: MediaPipe Hands 객체를 수동으로 생성하고 사용 후 `close()`를 호출하는 Python 코드)**

***

### **페이지 68**

**MediaPipe 인체 자세**

*   두 코드는 동일한 작업을 수행합니다: 웹캠 캡처 → BGR→RGB 변환 → MediaPipe Pose 실행 → 33개의 신체 랜드마크 + 스켈레톤 그리기 → `imshow()`로 표시.

**(코드 예시: MediaPipe Pose를 사용하여 웹캠 영상에서 실시간으로 인체 자세를 추정하고 시각화하는 Python 코드)**

***

### **페이지 69**

**컴퓨터 비전의 기초**
**2025년 2학기**
**- 엄광현 –**
**인공지능학과**

***

### **페이지 70**

**강의 4:**
**특징 탐지 및 매칭**
*   특징이란 무엇인가?
*   특징 탐지
*   특징 기술
*   특징 매칭
*   응용
    *   파노라마 스티칭, 객체 인식 (검색)

***

### **페이지 71**

**고양이 vs. 개**

*   **"어떻게 구별할 수 있나요?"**
    *   아마도 귀, 눈, 털 패턴, 심지어 얼굴 모양을 볼 것입니다.

**(이미지: 고양이와 개의 얼굴 사진)**

***

### **페이지 72**

**"숫자만으로 고양이를 알아볼 수 있나요?"**

*   **숫자로서의 픽셀**
    *   **"하지만 이 고양이 이미지를 가져와서 픽셀 값만 보여준다면 — 큰 배열 속의 숫자들만 — 여전히 고양이인지 알 수 있을까요?"**
*   **그렇지 않습니다. 개별 픽셀 값은 우리에게 많은 것을 알려주지 않습니다.**

**(이미지: 컴퓨터가 이미지를 픽셀 값의 숫자 배열로 보는 것을 보여주는 그림)**

***

### **페이지 73**

**"숫자만으로 고양이를 알아볼 수 있나요?"**

*   **"그렇다면 실제로 인식을 가능하게 하는 것은 무엇일까요?"**
    *   원시 픽셀 값이 아닙니다.
    *   **픽셀들이 함께 변하는 패턴**입니다: 모양을 정의하는 엣지, 털을 암시하는 텍스처, 그리고 객체를 배경과 분리하는 대비.

**(이미지: 고양이 사진에서 눈과 털 부분의 픽셀 패턴을 확대하여 보여주는 그림)**

***

### **페이지 74**

**엣지 탐지**

*   지금까지 우리는 **엣지 탐지**를 수행하는 방법을 배웠습니다.
*   그리고 이것을 이미지에 적용하면, 경계와 윤곽을 강조하는 **엣지 맵**과 같은 결과를 얻습니다.

**(이미지: 원본 고양이 사진과 캐니 엣지 탐지를 적용한 결과인 엣지 맵을 나란히 보여주는 그림)**

***

### **페이지 75**

**엣지 맵: 그래서 뭐?**

*   엣지 맵은 방금 본 예시처럼 간단한 숫자 격자로 표현될 수 있습니다.
*   하지만 이 배열만 보고 이미지가 **고양이**인지 **개**인지 정말 알 수 있을까요?
*   **귀**, **눈**, 또는 몸의 **윤곽**과 같은 특정 부분을 식별할 수 있을까요?

**(이미지: 엣지 맵을 0, 1, 2, 3으로 구성된 숫자 배열로 표현한 그림. 각 숫자는 엣지의 강도를 나타냄)**

*   0 = 엣지 없음
*   1 = 약한 엣지
*   2 = 중간 엣지
*   3 = 강한 엣지

***

### **페이지 76**

**엣지 맵: 그래서 뭐?**

*   엣지 맵은 방금 본 예시처럼 간단한 숫자 격자로 표현될 수 있습니다.
*   하지만 이 배열만 보고 이미지가 **고양이**인지 **개**인지 정말 알 수 있을까요?
*   **귀**, **눈**, 또는 몸의 **윤곽**과 같은 특정 부분을 식별할 수 있을까요?
*   단순한 엣지를 넘어 모양, 부분, 의미 있는 패턴을 포착하는 **특징**을 추출해야 합니다.

**(이미지: 캐니 엣지 탐지 결과와 이를 나타내는 숫자 배열을 함께 보여주는 그림)**

*   0 = 엣지 없음
*   1 = 약한 엣지
*   2 = 중간 엣지
*   3 = 강한 엣지

***

### **페이지 77**

**엣지에서 특징으로**

*   지금까지 우리는 엣지 맵이 단지 경계만을 강조한다는 것을 보았습니다.
*   하지만 인식을 위해서는 더 많은 것이 필요합니다: **특징**을 포착해야 합니다.
*   **특징**이란 무엇을 의미할까요?
    *   고양이 귀의 **뾰족한 모양**을 생각해보세요.
    *   또는 눈의 **둥근 패턴**을 생각해보세요.
    *   또는 고양이와 개를 구별하는 **털의 질감**을 생각해보세요.
*   **특징**은 이미지에서 객체를 식별하는 데 도움이 되는 **독특한 패턴 또는 특성**입니다.
*   다시 말해: **엣지는 변화가 일어나는 곳을 보여주지만, 특징은 그 변화가 무엇을 의미하는지 알려줍니다.**

**(이미지: 고양이의 귀, 눈, 털을 각각 보여주는 사진)**

***

### **페이지 78**

**엣지에서 특징으로**

*   지금까지 우리는 엣지 맵이 단지 경계만을 강조한다는 것을 보았습니다.
*   하지만 인식을 위해서는 더 많은 것이 필요합니다: **특징**을 포착해야 합니다.
*   **특징**이란 무엇을 의미할까요?
    *   고양이 귀의 **뾰족한 모양**을 생각해보세요.
    *   또는 눈의 **둥근 패턴**을 생각해보세요.
    *   또는 고양이와 개를 구별하는 **털의 질감**을 생각해보세요.
*   **특징**은 이미지에서 객체를 식별하는 데 도움이 되는 **독특한 패턴 또는 특성**입니다.
*   다시 말해: **엣지는 변화가 일어나는 곳을 보여주지만, 특징은 그 변화가 무엇을 의미하는지 알려줍니다.**

**(이미지: 고양이 귀와 눈의 원본 사진과 각각의 엣지 맵을 나란히 보여주는 그림)**

***

### **페이지 79**

**엣지에서 특징으로**

*   고양이의 **귀**를 보세요. **엣지 맵**은 그 **윤곽**을 강조할 수 있습니다.
*   하지만 엣지 맵만으로 귀가 **뾰족한 모양**을 가졌다고 말할 수 있을까요?
    *   답은 **아니오**입니다.
*   모양이 뾰족하다는 것을 이해하려면 엣지를 넘어서야 합니다.
*   바로 여기서 **특징**이 등장합니다 — 특징은 **모양, 패턴, 질감**과 같은 더 높은 수준의 속성을 설명합니다.

**(이미지: 고양이의 귀, 눈, 수염, 코의 원본 사진과 각각의 엣지 맵을 나란히 보여주는 그림. 각 엣지 맵은 뾰족한 모양, 둥근 패턴, 선 구조, 삼각형/타원 모양을 나타냄)**

***

### **페이지 80**

**엣지로부터 어떻게 모양을 보는가?**

*   **뾰족한 귀**를 인식하려면 단순히 엣지 픽셀을 보는 것만으로는 충분하지 않습니다.
*   우리는 경로를 따라 **엣지를 따라가야** 합니다.
*   **우리가 추적하면서:**
    *   엣지는 **위쪽으로** 갑니다,
    *   그런 다음 코너에서 **날카롭게 구부러집니다**,
    *   그런 다음 **아래쪽으로** 움직입니다,
    *   그리고 결국 얼굴 경계로 다시 연결됩니다.
*   이러한 **회전과 방향**을 결합하여 우리는 모양을 **뾰족하다**고 이해합니다.

**(이미지: 고양이 귀 원본 사진과 그 엣지 맵을 나란히 보여주는 그림)**

***

### **페이지 81**

**엣지로부터 어떻게 모양을 보는가?**

*   **뾰족한 귀**를 인식하려면 단순히 엣지 픽셀을 보는 것만으로는 충분하지 않습니다.
*   우리는 경로를 따라 **엣지를 따라가야** 합니다.
*   **우리가 추적하면서:**
    *   엣지는 **위쪽으로** 갑니다,
    *   그런 다음 코너에서 **날카롭게 구부러집니다**,
    *   그런 다음 **아래쪽으로** 움직입니다,
    *   그리고 결국 얼굴 경계로 다시 연결됩니다.
*   이러한 **회전과 방향**을 결합하여 우리는 모양을 **뾰족하다**고 이해합니다.

**(이미지: 고양이 귀 엣지 맵 위로 엣지의 방향을 나타내는 화살표가 그려진 그림)**

***

### **페이지 82**

**엣지로부터 어떻게 모양을 보는가?**

*   **뾰족한 귀**를 인식하려면 단순히 엣지 픽셀을 보는 것만으로는 충분하지 않습니다.
*   우리는 경로를 따라 **엣지를 따라가야** 합니다.
*   **우리가 추적하면서:**
    *   엣지는 **위쪽으로** 갑니다,
    *   그런 다음 코너에서 **날카롭게 구부러집니다**,
    *   그런 다음 **아래쪽으로** 움직입니다,
    *   그리고 결국 얼굴 경계로 다시 연결됩니다.
*   이러한 **회전과 방향**을 결합하여 우리는 모양을 **뾰족하다**고 이해합니다.

**(이미지: 고양이 귀 엣지 맵 위로 엣지의 경로와 코너 지점을 강조하는 그림)**

***

### **페이지 83**

**엣지로부터 어떻게 모양을 보는가?**

*   **뾰족한 귀**를 인식하려면 단순히 엣지 픽셀을 보는 것만으로는 충분하지 않습니다.
*   우리는 경로를 따라 **엣지를 따라가야** 합니다.
*   **우리가 추적하면서:**
    *   엣지는 **위쪽으로** 갑니다,
    *   그런 다음 코너에서 **날카롭게 구부러집니다**,
    *   그런 다음 **아래쪽으로** 움직입니다,
    *   그리고 결국 얼굴 경계로 다시 연결됩니다.
*   이러한 **회전과 방향**을 결합하여 우리는 모양을 **뾰족하다**고 이해합니다.

**(이미지: 고양이 귀 엣지 맵 위로 엣지의 경로와 여러 코너 지점을 강조하는 그림)**

***

### **페이지 84**

**핵심 포인트: 코너**

*   엣지를 따라가다 보면 종종 **구부러지는 지점**을 마주칩니다.
    *   이 지점들은 엣지의 방향이 **급격하게 변하는** 곳을 표시합니다.
    *   이러한 지점을 **코너**라고 합니다.

*   **코너가 중요한 이유**
    *   코너는 단순한 직선 엣지가 제공할 수 없는 정보를 제공합니다.
    *   **예를 들어:**
        *   고양이 귀의 **끝**은 코너입니다.
        *   눈의 **모서리**도 코너입니다.
    *   코너를 탐지함으로써 우리는 단순한 경계가 아닌 **독특한 모양**을 포착하기 시작합니다.

**(이미지: 고양이 귀와 눈의 코너 지점, 그리고 엣지 맵에서의 코너 지점을 강조하는 그림)**

***

### **페이지 85**

**그렇다면, 코너를 어떻게 탐지할까?**

*   직관적으로, **코너**는 엣지의 **방향이 급격하게 변하는** 곳입니다.
*   하지만 컴퓨터가 이것을 어떻게 자동으로 탐지할 수 있을까요?
*   한 점 주변의 **"강도 변화"**를 설명할 수학적인 방법이 필요합니다.

**(이미지: 체커보드 패턴과 3D 블록 객체에서 코너 지점을 탐지한 예시)**

***

### **페이지 86**

**그렇다면, 코너를 어떻게 탐지할까?**

*   **아이디어:**
    *   작은 창을 다른 방향으로 이동시키면…
    *   **평평한** 영역에서는 → 거의 변화가 없습니다.
    *   **엣지**를 따라서는 → 한 방향으로는 작은 변화가 있지만 다른 방향으로는 변화가 없습니다.
    *   **코너**에서는 → **양쪽 방향 모두에서 큰 변화**가 있습니다.
*   이 원리는 **코너 탐지 알고리즘**(예: 해리스 코너 탐지기)의 기초를 형성합니다.

**(이미지: 평평한 영역, 엣지, 코너에서 작은 창을 이동시킬 때의 픽셀 강도 변화를 시각적으로 비교하는 그림)**

*   **"평평한" 영역:** 모든 방향으로 변화 없음
*   **"엣지" 영역:** 엣지 방향을 따라서는 변화 없음
*   **"코너" 영역:** 모든 방향으로 상당한 변화

***

### **페이지 87**

**그렇다면, 코너를 어떻게 탐지할까?**

*   **아이디어:**
    *   작은 창을 다른 방향으로 이동시키면…
    *   **평평한** 영역에서는 → 거의 변화가 없습니다.
    *   **엣지**를 따라서는 → 한 방향으로는 작은 변화가 있지만 다른 방향으로는 변화가 없습니다.
    *   **코너**에서는 → **양쪽 방향 모두에서 큰 변화**가 있습니다.

**(이미지: 평평한 영역, 엣지, 코너에서 작은 빨간 사각형 창을 이동시킬 때의 픽셀 변화를 시각적으로 보여주는 그림)**

***

### **페이지 88**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

**(수식 및 이미지: SSD(Sum of Squared Differences)를 계산하는 공식과 창(window)을 보여주는 그림)**

***

### **페이지 89**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

**(수식 및 이미지: SSD 계산 과정을 시각적으로 보여주는 그림. 이동 전후의 창을 빼고 제곱하여 합산함)**

***

### **페이지 90**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **평평한 영역**

**(수식 및 이미지: 평평한 영역에서 창을 이동시켰을 때 SSD 값이 어떻게 되는지 묻는 그림. 결과는 0에 가까울 것임)**

***

### **페이지 91**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **평평한 영역**

**(수식 및 이미지: 평평한 영역에서 창을 이동시켰을 때 SSD 값이 0이 되는 것을 보여주는 그림)**

***

### **페이지 92**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **평평한 영역**

**(수식 및 이미지: 평평한 영역에서 여러 방향으로 창을 이동시켜도 SSD 값이 0이 되는 것을 보여주는 그림)**

***

### **페이지 93**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **평평한 영역**

**(수식 및 이미지: 평평한 영역에서 여러 방향으로 창을 이동시켜도 SSD 값이 0이 되는 것을 보여주는 그림)**

***

### **페이지 94**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **평평한 영역**
    *   E(u, v) ≈ 0 (모든 방향에서) → 평평한 영역

**(수식 및 이미지: 평평한 영역에서 SSD 값이 모든 방향에서 0에 가깝다는 결론을 보여주는 그림)**

***

### **페이지 95**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **엣지**

**(수식 및 이미지: 엣지 영역에 위치한 창을 보여주는 그림)**

***

### **페이지 96**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **엣지**

**(수식 및 이미지: 엣지 영역에서 엣지와 평행한 방향으로 창을 이동시켰을 때 SSD 값이 어떻게 되는지 묻는 그림)**

***

### **페이지 97**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **엣지**

**(수식 및 이미지: 엣지 영역에서 엣지와 평행한 방향으로 창을 이동시켰을 때 SSD 값이 0이 되는 것을 보여주는 그림)**

***

### **페이지 98**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **엣지**

**(수식 및 이미지: 엣지 영역에서 엣지와 수직인 방향으로 창을 이동시켰을 때 SSD 값이 어떻게 되는지 묻는 그림)**

***

### **페이지 99**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **엣지**

**(수식 및 이미지: 엣지 영역에서 엣지와 수직인 방향으로 창을 이동시켰을 때 SSD 값이 50(큰 값)이 되는 것을 보여주는 그림)**

***

### **페이지 100**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **엣지**
    *   E(u, v) ≈ 0인 방향이 존재하면 → 엣지

**(수식 및 이미지: 엣지 영역에서는 SSD 값이 0에 가까운 방향이 존재한다는 결론을 보여주는 그림)**

***

### **페이지 101**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **코너**

**(수식 및 이미지: 코너 영역에 위치한 창을 보여주는 그림)**

***

### **페이지 102**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **코너**

**(수식 및 이미지: 코너 영역에서 창을 한 방향으로 이동시켰을 때 SSD 값이 어떻게 되는지 묻는 그림)**

***

### **페이지 103**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **코너**

**(수식 및 이미지: 코너 영역에서 창을 한 방향으로 이동시켰을 때 SSD 값이 50(큰 값)이 되는 것을 보여주는 그림)**

***

### **페이지 104**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **코너**

**(수식 및 이미지: 코너 영역에서 창을 다른 방향으로 이동시켰을 때 SSD 값이 어떻게 되는지 묻는 그림)**

***

### **페이지 105**

**컴퓨터가 변화를 측정하는 방법**

*   **픽셀 값을 비교**함으로써.
    *   작은 창을 가져와 → x 또는 y 방향으로 약간 이동시킵니다.
    *   각 픽셀에 대해: 이전과 이후의 **차이**를 계산합니다.
    *   차이를 제곱합니다 (음수를 피하기 위해). 모두 더합니다.
    *   **이동된 창의 제곱 차이 합 (SSD)**

*   **코너**
    *   E(u, v) ≈ 0인 방향이 없으면 → 코너

**(수식 및 이미지: 코너 영역에서는 SSD 값이 0에 가까운 방향이 없다는 결론을 보여주는 그림)**

***

### **페이지 106**

**모라벡 코너 탐지기**

*   모라벡 코너 탐지기는 이미지 창을 **네 가지 주요 방향**으로 작게 이동시켰을 때의 강도 변화 E(u, v)를 계산합니다.
*   각 픽셀에 대해, 방향성 강도 변화 함수 E(u, v)의 **최소값**이 **코너 응답**으로 정의됩니다.
*   이 응답이 미리 정의된 임계값 T를 초과하면, 해당 픽셀은 **코너 포인트**로 분류됩니다.

*   **코너**
    *   R > T → 코너
    *   else → 코너 아님

**(이미지 및 수식: 모라벡 코너 탐지기의 계산 과정을 시각적으로 설명하는 그림. 코너 지점에서는 모든 방향의 변화량이 커서 최소값도 크게 나옴)**

***

### **페이지 107**

**모라벡 코너 탐지기**

*   모라벡 코너 탐지기는 이미지 창을 **네 가지 주요 방향**으로 작게 이동시켰을 때의 강도 변화 E(u, v)를 계산합니다.
*   각 픽셀에 대해, 방향성 강도 변화 함수 E(u, v)의 **최소값**이 **코너 응답**으로 정의됩니다.
*   이 응답이 미리 정의된 임계값 T를 초과하면, 해당 픽셀은 **코너 포인트**로 분류됩니다.

*   **엣지**
    *   R > T → 코너
    *   else → 코너 아님

**(이미지 및 수식: 모라벡 코너 탐지기의 계산 과정을 시각적으로 설명하는 그림. 엣지 지점에서는 엣지 방향으로의 변화량이 0이므로 최소값도 0이 됨)**

***

### **페이지 108**

**코너 탐지기 (1990년대)**

*   모두 동일한 아이디어에 기반합니다: 코너는 강도 변화가 **모든 방향에서 큰** 곳입니다.
    *   **모라벡 (1977)**: 가장 초기 버전으로, 몇 가지 방향의 변화만 비교했습니다.
    *   **해리스 (1988)**: **모든 방향**을 더 부드럽게 처리하여 개선했습니다.
    *   **시-토마시 (1994)**: 해리스에 작은 수학적 수정을 가하여 실제 적용에서 더 잘 작동했습니다.
    *   → 이름과 수학은 약간 다르지만, **핵심 개념은 동일합니다.**

**(이미지: 원본 독수리 사진과 해리스, 시-토마시 코너 탐지기를 적용한 결과를 비교하는 그림)**

***

### **페이지 109**

**테일러 급수 근사**

*   테일러 급수 근사는 함수 f(x)가 점 x₀ 근처에서 어떻게 동작하는지 추정하여, 근처 점 x₁에서의 값을 예측합니다.
*   **1차 근사**에서는 함수가 x₀에서의 **기울기**(1차 도함수)를 기반으로 선형적으로 변한다고 가정합니다.
*   수학적으로는 다음과 같이 표현됩니다:

**(수식 및 그래프: 1차 테일러 급수 근사를 설명하는 공식과 그래프)**

***

### **페이지 110**

**고차 테일러 급수 확장**

*   테일러 급수는 1차 도함수를 넘어 **2차, 3차, 그리고 더 높은 차수의 항**을 포함하도록 확장될 수 있습니다 — 이론적으로는 무한대까지.
*   각 추가 항은 x₀ 근처에서의 근사 **정확도**를 높입니다.
*   고차 도함수가 포함될수록, 근사는 함수의 국소적 동작을 더 정밀하게 포착하게 됩니다.

**(수식: 고차 항을 포함하는 테일러 급수 확장 공식)**

***

### **페이지 111**

**2D에서의 1차 테일러 근사**

*   이 아이디어를 2D 함수 f(x, y)로 확장하여, 점이 (x, y)에서 (u, v)만큼 이동할 때, 1차 테일러 근사는 다음과 같이 표현됩니다:
*   여기서 fₓ와 fᵧ는 각각 x와 y에 대한 f의 **편도함수**를 나타내며, 각 방향으로의 함수 변화율을 나타냅니다.

**(수식 및 그래프: 2D 함수에 대한 1차 테일러 근사를 설명하는 공식과 그래프)**

***

### **페이지 112**

**테일러 근사에서 코너 이차 형식으로**

*   창 w에 대한 모라벡 코너 오차에서 시작합니다.
*   이미지의 **1차 테일러 확장**을 적용합니다.
*   이를 대입하면 (u, v)에 대한 **이차 형식**이 나옵니다.

**(수식: 모라벡 코너 오차 공식에 1차 테일러 근사를 대입하여 이차 형식으로 변환하는 과정)**

***

### **페이지 113**

**테일러 근사에서 코너 이차 형식으로**

*   창 w에 대한 모라벡 코너 오차에서 시작합니다.
*   이미지의 **1차 테일러 확장**을 적용합니다.
*   이를 대입하면 (u, v)에 대한 **이차 형식**이 나옵니다.

**(수식: 이전 페이지의 수식에서 상쇄되는 항들을 표시)**

***

### **페이지 114**

**테일러 근사에서 코너 이차 형식으로**

*   창 w에 대한 모라벡 코너 오차에서 시작합니다.
*   이미지의 **1차 테일러 확장**을 적용합니다.
*   이를 대입하면 (u, v)에 대한 **이차 형식**이 나옵니다.

**(수식: 상쇄된 항들을 제거하여 간소화된 공식)**

***

### **페이지 115**

**테일러 근사에서 코너 이차 형식으로**

*   창 w에 대한 모라벡 코너 오차에서 시작합니다.
*   이미지의 **1차 테일러 확장**을 적용합니다.
*   이를 대입하면 (u, v)에 대한 **이차 형식**이 나옵니다.

**(수식: 제곱 항을 전개하여 더 상세한 공식으로 변환)**

***

### **페이지 116**

**테일러 근사에서 코너 이차 형식으로**

*   창 w에 대한 모라벡 코너 오차에서 시작합니다.
*   이미지의 **1차 테일러 확장**을 적용합니다.
*   이를 대입하면 (u, v)에 대한 **이차 형식**이 나옵니다.

**(수식: 합산 기호를 각 항으로 분리하여 정리)**

***

### **페이지 117**

**테일러 근사에서 코너 이차 형식으로**

*   창 w에 대한 모라벡 코너 오차에서 시작합니다.
*   이미지의 **1차 테일러 확장**을 적용합니다.
*   이를 대입하면 (u, v)에 대한 **이차 형식**이 나옵니다.

**(수식: 공식을 행렬과 벡터의 곱 형태로 표현. 이 행렬이 이차 모멘트 행렬 M임)**

***

### **페이지 118**

**테일러 근사에서 코너 이차 형식으로**

*   여기서 M은 창에 대한 **이차 모멘트 (구조) 행렬**입니다.
*   E(u, v)의 그래프는 **타원 포물면 (2차 곡면)**입니다.
*   그 **모양과 방향**은 M에 의해 (그 고유값/고유벡터를 통해) 결정됩니다.
*   **모든 방향에서 큰 곡률**(두 고유값이 모두 큼)은 **코너**를 나타냅니다.

**(수식: 이전 페이지의 공식을 최종적으로 벡터와 행렬 M의 곱으로 표현)**

***

### **페이지 119**

**이차 모멘트 행렬의 고유 분해**

*   이차 모멘트 행렬 M은 그 **고유벡터**와 **고유값**으로 분해될 수 있습니다.
    *   **대칭**: Iₓ(x, y)Iᵧ(x, y) = Iᵧ(x, y)Iₓ(x, y)
    *   **양의 준정부호**: 고유값은 **음수가 아닙니다**.
*   **여기서**
    *   **R**: 회전 행렬 (열이 고유벡터), 직교 행렬.
    *   **λ₁, λ₂**: 각 주축을 따른 곡률 강도를 나타내는 고유값.

**(수식: 이차 모멘트 행렬 M의 정의와 이를 고유값과 고유벡터로 분해하는 공식)**

***

### **페이지 120**

**이차 모멘트 행렬의 고유 분해**

*   이것을 방정식에 대입하고, 좌표를 **회전된 프레임** (u', v') = [u v]R로 표현하면,
*   단순화된 형태를 얻을 수 있습니다.

**(수식: E(u, v) 공식에 M의 고유 분해를 대입하는 과정)**

***

### **페이지 121**

**이차 모멘트 행렬의 고유 분해**

*   이것을 방정식에 대입하고, 좌표를 **회전된 프레임** (u', v') = [u v]R로 표현하면,
*   단순화된 형태를 얻을 수 있습니다.

**(수식: 이전 페이지의 수식을 계속해서 단순화하는 과정)**

***

### **페이지 122**

**이차 모멘트 행렬의 고유 분해**

*   이것을 방정식에 대입하고, 좌표를 **회전된 프레임** (u', v') = [u v]R로 표현하면,
*   단순화된 형태를 얻을 수 있습니다.

**(수식: 최종적으로 E(u', v')를 고유값과 회전된 좌표로 표현한 공식)**

***

### **페이지 123**

**이차 모멘트 행렬의 고유 분해**

*   이것은 각 축을 따른 곡률이 고유값에 의해 결정되는 **포물면 (그릇 모양의 표면)**을 나타냅니다:
    *   **λ₁, λ₂ 모두 큼** → **코너** (모든 방향에서 강한 변화)
    *   **하나는 크고, 하나는 작음** → **엣지**
    *   **둘 다 작음** → **평평한 영역**

**(이미지 및 수식: 포물면의 방정식과 그릇을 특정 높이에서 잘랐을 때 원이 되는 것을 보여주는 그림)**

***

### **페이지 124**

**이차 모멘트 행렬의 고유 분해**

*   이것은 각 축을 따른 곡률이 고유값에 의해 결정되는 **포물면 (그릇 모양의 표면)**을 나타냅니다:
    *   **λ₁, λ₂ 모두 큼** → **코너** (모든 방향에서 강한 변화)
    *   **하나는 크고, 하나는 작음** → **엣지**
    *   **둘 다 작음** → **평평한 영역**

**(이미지: 평평한 영역, 엣지, 코너에 해당하는 세 가지 다른 모양의 포물면 그래프)**

***

### **페이지 125**

**이차 모멘트 행렬의 고유 분해**

*   이것은 각 축을 따른 곡률이 고유값에 의해 결정되는 **포물면 (그릇 모양의 표면)**을 나타냅니다:
    *   **λ₁, λ₂ 모두 큼** → **코너** (모든 방향에서 강한 변화)
    *   **하나는 크고, 하나는 작음** → **엣지**
    *   **둘 다 작음** → **평평한 영역**

**(이미지: 두 고유값 λ₁, λ₂의 크기에 따라 평평한 영역, 엣지, 코너가 어떻게 구분되는지를 보여주는 2D 공간 다이어그램)**

***

### **페이지 126**

**이차 모멘트 행렬의 고유 분해**

*   이것은 각 축을 따른 곡률이 고유값에 의해 결정되는 **포물면 (그릇 모양의 표면)**을 나타냅니다:
    *   **λ₁, λ₂ 모두 큼** → **코너** (모든 방향에서 강한 변화)
    *   **하나는 크고, 하나는 작음** → **엣지**
    *   **둘 다 작음** → **평평한 영역**

**(이미지: 이전 페이지의 다이어그램을 더 상세하게 설명하는 그림)**

***

### **페이지 127**

**시-토마시 코너 탐지기**

*   한 점이 얼마나 "코너 같은지"를 정량화하기 위해, 시-토마시는 **코너 응답**을 다음과 같이 정의합니다:
    *   R = min(λ₁, λ₂)
*   **코너**는 임계값을 적용하여 탐지됩니다:
    *   R > T (여기서 T는 미리 정의된 임계값)

*   **엣지**를 탐지하려면, **하나의 고유값만** 임계값을 초과하는 점을 선택할 수 있습니다.
*   λ₁ < T, λ₂ < T → 평평한 영역

**(이미지: 두 고유값의 크기에 따라 엣지, 코너, 균일한 영역을 구분하는 그래프)**

***

### **페이지 128**

**코너 강도 정량화 – 해리스 응답 함수**

*   코너가 얼마나 강한지를 측정하기 위해, 우리는 두 고유값 λ₁, λ₂의 **크기**를 모두 반영하는 단일 스칼라 값을 원합니다.
*   M이 2×2 행렬이므로, 우리는 다음을 압니다:
    *   det(M) = λ₁λ₂
    *   trace(M) = λ₁ + λ₂
*   해리스는 **코너 응답 함수 R**을 제안했습니다:
    *   R = det(M) - k(trace(M))²
    *   (여기서 k는 상수, 보통 0.04–0.06)

*   **평평한 영역** (λ₁ ≈ λ₂ ≈ 0)
    *   R ≈ 0

**(이미지: 두 고유값의 크기에 따라 해리스 응답 함수 R의 값이 어떻게 변하는지를 보여주는 영역별 다이어그램)**

***

### **페이지 129**

**코너 강도 정량화 – 해리스 응답 함수**

*   코너가 얼마나 강한지를 측정하기 위해, 우리는 두 고유값 λ₁, λ₂의 **크기**를 모두 반영하는 단일 스칼라 값을 원합니다.
*   M이 2×2 행렬이므로, 우리는 다음을 압니다:
    *   det(M) = λ₁λ₂
    *   trace(M) = λ₁ + λ₂
*   해리스는 **코너 응답 함수 R**을 제안했습니다:
    *   R = det(M) - k(trace(M))²
    *   (여기서 k는 상수, 보통 0.04–0.06)

*   **평평한 영역** (λ₁ ≈ λ₂ ≈ 0)
    *   R ≈ 0
*   **코너의 경우** (λ₁ ≈ λ₂ ≫ 0)
    *   R > 0

**(이미지: 이전 페이지의 다이어그램에 코너 경우의 R 값 계산을 추가)**

***

### **페이지 130**

**코너 강도 정량화 – 해리스 응답 함수**

*   코너가 얼마나 강한지를 측정하기 위해, 우리는 두 고유값 λ₁, λ₂의 **크기**를 모두 반영하는 단일 스칼라 값을 원합니다.
*   M이 2×2 행렬이므로, 우리는 다음을 압니다:
    *   det(M) = λ₁λ₂
    *   trace(M) = λ₁ + λ₂
*   해리스는 **코너 응답 함수 R**을 제안했습니다:
    *   R = det(M) - k(trace(M))²
    *   (여기서 k는 상수, 보통 0.04–0.06)

*   **평평한 영역** (λ₁ ≈ λ₂ ≈ 0)
    *   R ≈ 0
*   **코너의 경우** (λ₁ ≈ λ₂ ≫ 0)
    *   R > 0
*   **엣지의 경우** (λ₁ ≫ λ₂ ≈ 0)
    *   R < 0

**(이미지: 이전 페이지의 다이어그램에 엣지 경우의 R 값 계산을 추가)**

***

### **페이지 131**

**R 임계값은 어떻게 설정되는가?**

*   우리는 일반적으로 최대 R 값에 대해 **정규화**하거나 **상대적인 임계값**을 적용합니다. 예:
    *   R > T, 여기서 T = α ∙ R_max
    *   (α는 0.01–0.1 사이의 상수)

**(이미지 및 수식: 해리스 응답 함수와 임계값 설정 방법을 보여주는 그림)**

***

### **페이지 132**

**해리스 코너 탐지기의 장점**

*   해리스 코너 탐지기의 주요 장점은 **명시적인 고유값 분해를 요구하지 않는다**는 것입니다.
    *   고유값을 명시적으로 계산할 필요가 없어 계산이 더 빠릅니다.
    *   단순한 행렬 연산(행렬식 및 트레이스)만 사용합니다.
    *   모든 픽셀에 대해 연속적인 코너 강도 측정값을 제공합니다.

**(수식: 해리스 코너 탐지기에 사용되는 행렬식, 트레이스, 응답 함수 R의 공식)**

***

### **페이지 133**

**코너 탐지기 (1990년대)**

**(이미지: 다양한 이미지(사무실, 체스판, 3D 블록, 체스 말)에 코너 탐지기를 적용한 여러 결과 예시)**

***

### **페이지 134**

**코너 탐지기 - 응용**

*   **특징 매칭**
    *   두 개의 다른 이미지에서 동일한 코너를 찾습니다.
    *   스테레오 비전, 파노라마 스티칭의 기초가 됩니다.

**(이미지: 노트르담 대성당의 두 다른 각도 사진에서 코너 특징점을 매칭한 결과)**

***

### **페이지 135**

**코너 탐지기 - 응용**

*   **특징 매칭**
    *   두 개의 다른 이미지에서 동일한 코너를 찾습니다.
    *   스테레오 비전, 파노라마 스티칭의 기초가 됩니다.

**(이미지: 건물의 왼쪽 이미지와 오른쪽 이미지에서 코너 특징점을 매칭한 결과)**

***

### **페이지 136**

**코너 탐지기 - 응용**

*   **특징 매칭**
    *   두 개의 다른 이미지에서 동일한 코너를 찾습니다.
    *   스테레오 비전, 파노라마 스티칭의 기초가 됩니다.

**(이미지: 자유의 여신상의 두 다른 사진에서 코너 특징점을 매칭한 결과)**

***

### **페이지 137**

**코너 탐지기 - 응용**

*   **특징 매칭**
    *   두 개의 다른 이미지에서 동일한 코너를 찾습니다.
    *   스테레오 비전, 파노라마 스티칭의 기초가 됩니다.

**(이미지: 한 건물의 두 다른 사진에서 코너 특징점을 매칭한 결과)**

***

### **페이지 138**

**코너 탐지기 – 한계**

*   **스케일 민감성**
    *   이미지를 확대하거나 축소하면 동일한 코너가 사라지거나 이동할 수 있습니다.
    *   해리스나 시-토마시 같은 탐지기는 **고정된 창 크기**를 가정하므로 → 코너는 스케일에 의존적입니다. -> 이 탐지기들은 **스케일 불변이 아닙니다**.

**(이미지: 이미지를 확대했을 때 코너가 엣지로 분류되는 문제를 시각적으로 설명하는 그림. "스케일 불변이 아님!"이라는 문구가 강조됨)**

***

### **페이지 139**

**코너 탐지기 – 한계**

*   **스케일 민감성**
    *   이미지를 확대하거나 축소하면 동일한 코너가 사라지거나 이동할 수 있습니다.
    *   해리스나 시-토마시 같은 탐지기는 **고정된 창 크기**를 가정하므로 → 코너는 스케일에 의존적입니다. -> 이 탐지기들은 **스케일 불변이 아닙니다**.

*   **노이즈 민감성**
    *   작은 창은 노이즈에 반응하여 잘못된 코너를 생성할 수 있습니다.
    *   큰 창은 미세한 세부 사항을 놓칠 수 있습니다.

**(이미지: 노이즈가 없는 이미지와 노이즈가 추가된 이미지에서 코너 탐지 결과가 어떻게 달라지는지 비교하는 그림)**

***

### **페이지 140**

**코너 탐지기 – 한계**

*   **스케일 민감성**
    *   이미지를 확대하거나 축소하면 동일한 코너가 사라지거나 이동할 수 있습니다.
    *   해리스나 시-토마시 같은 탐지기는 **고정된 창 크기**를 가정하므로 → 코너는 스케일에 의존적입니다. -> 이 탐지기들은 **스케일 불변이 아닙니다**.

*   **노이즈 민감성**
    *   작은 창은 노이즈에 반응하여 잘못된 코너를 생성할 수 있습니다.
    *   큰 창은 미세한 세부 사항을 놓칠 수 있습니다.

*   이는 스케일 및 회전 불변 특징 탐지기 – SIFT, SURF, ORB 등 (2004-2011)의 개발로 이어졌습니다.

**(이미지: 노이즈가 없는 이미지와 노이즈가 추가된 이미지에서 코너 탐지 결과가 어떻게 달라지는지 비교하는 그림)**

***

### **페이지 141**

**특징 기술**

**(이미지: 데이터 과학을 상징하는 그래픽)**

***

### **페이지 142**

**코너 탐지기… 그 다음은?**

*   지금까지 **코너 탐지**를 통해 이미지에서 *관심 지점*을 찾는 방법을 배웠습니다.
*   하지만 그 다음은? 이 단계에서 코너는 단지 **좌표**의 집합에 불과합니다.
*   이 좌표들만으로는 주변의 지역적 이미지 내용에 대해 충분히 알 수 없습니다.

**(이미지: 엣지 맵에서 코너로 표시된 지점을 보여주는 그림. 코너가 단지 숫자 배열의 특정 위치임을 나타냄)**

***

### **페이지 143**

**코너 탐지기… 그 다음은?**

*   지금까지 **코너 탐지**를 통해 이미지에서 *관심 지점*을 찾는 방법을 배웠습니다.
*   하지만 그 다음은? 이 단계에서 코너는 단지 **좌표**의 집합에 불과합니다.
*   이 좌표들만으로는 주변의 지역적 이미지 내용에 대해 충분히 알 수 없습니다.

**(이미지: 고양이 귀와 눈의 코너, 그리고 엣지 맵에서의 코너들을 보여주며, 이 좌표들만으로는 무엇인지 알 수 없다는 물음표를 표시)**

***

### **페이지 144**

**코너 탐지기… 그 다음은?**

*   지금까지 **코너 탐지**를 통해 이미지에서 *관심 지점*을 찾는 방법을 배웠습니다.
*   하지만 그 다음은? 이 단계에서 코너는 단지 **좌표**의 집합에 불과합니다.
*   이 좌표들만으로는 주변의 지역적 이미지 내용에 대해 충분히 알 수 없습니다.

**(이미지: 엣지 맵의 코너들이 어떤 모양(예: 뾰족한 모양)을 나타내는지 질문)**

***

### **페이지 145**

**코너 탐지기… 그 다음은?**

*   하지만 그 지점 주변을 보고, 예를 들어 양쪽으로 아래로 뻗어 있는 엣지를 본다면, **귀 끝**과 같은 **뾰족한 모양**을 추론할 수 있습니다.
*   따라서 우리는 코너의 **주변**을 **기술(describe)**해야 합니다.
*   이것이 **특징 기술**의 역할입니다: 코너 주변의 **지역적 패턴**을 인코딩하여 어떤 종류의 구조를 나타내는지 알 수 있게 합니다.

**(이미지: 엣지 맵의 코너 주변 엣지 패턴을 통해 '귀 끝'이라는 의미를 추론하는 과정을 보여줌)**

***

### **페이지 146**

**특징 기술이란 무엇인가**

*   **특징 기술**은 관심 지점(예: 코너) 주변의 지역적 이웃을 **수치적 표현**으로 변환하는 과정입니다.
*   단순히 좌표를 저장하는 대신, 그 지점 주변의 **패턴, 방향, 강도 분포**를 벡터나 시그니처로 표현합니다.
*   -> 이를 통해 서로 다른 이미지 간에 특징을 **비교**할 수 있습니다.

**(이미지: 두 개의 다른 각도에서 찍은 그래피티 사진에서 동일한 특징 영역을 사각형으로 표시)**

***

### **페이지 147**

**특징 기술이란 무엇인가**

*   **특징 기술**은 관심 지점(예: 코너) 주변의 지역적 이웃을 **수치적 표현**으로 변환하는 과정입니다.
*   단순히 좌표를 저장하는 대신, 그 지점 주변의 **패턴, 방향, 강도 분포**를 벡터나 시그니처로 표현합니다.
*   -> 이를 통해 서로 다른 이미지 간에 특징을 **비교**할 수 있습니다.

**(이미지: 이전 페이지의 이미지에서 각 특징 영역이 어떻게 기술자(descriptor) 벡터로 변환되는지를 보여줌)**

***

### **페이지 148**

**특징 기술이란 무엇인가**

*   **특징 기술**은 관심 지점(예: 코너) 주변의 지역적 이웃을 **수치적 표현**으로 변환하는 과정입니다.
*   단순히 좌표를 저장하는 대신, 그 지점 주변의 **패턴, 방향, 강도 분포**를 벡터나 시그니처로 표현합니다.
*   -> 이를 통해 서로 다른 이미지 간에 특징을 **비교**할 수 있습니다.

**(이미지: 두 이미지의 기술자 벡터가 유사하여 "매칭됨!!"으로 표시됨)**

***

### **페이지 149**

**특징 기술이란 무엇인가**

*   **특징 기술**은 관심 지점(예: 코너) 주변의 지역적 이웃을 **수치적 표현**으로 변환하는 과정입니다.
*   단순히 좌표를 저장하는 대신, 그 지점 주변의 **패턴, 방향, 강도 분포**를 벡터나 시그니처로 표현합니다.
*   -> 이를 통해 서로 다른 이미지 간에 특징을 **비교**할 수 있습니다.

**(이미지: 축구공 객체와 실제 장면 속 축구공 간의 특징점들이 매칭되는 것을 보여주는 그림)**

***

### **페이지 150**

**특징 기술이란 무엇인가**

*   **특징 기술**은 관심 지점(예: 코너) 주변의 지역적 이웃을 **수치적 표현**으로 변환하는 과정입니다.
*   단순히 좌표를 저장하는 대신, 그 지점 주변의 **패턴, 방향, 강도 분포**를 벡터나 시그니처로 표현합니다.
*   -> 이를 통해 서로 다른 이미지 간에 특징을 **비교**할 수 있습니다.

**(이미지: 두 개의 다른 시점에서 찍은 건물 사진 간에 특징점들이 매칭되는 것을 보여주는 그림)**

***

### **페이지 151**

**특징 기술이란 무엇인가**

*   우리는 이미 **엣지**와 **코너**에 대해 배웠기 때문에, 특징 기술을 위해 **그래디언트 기반 접근법**으로 시작하는 것이 자연스럽습니다.
*   코너 주변에 그래디언트가 어떻게 분포되어 있는지 포착하고, 그 정보를 기술자로 요약할 수 있습니다.
    *   이것이 **SIFT**와 같은 방법의 핵심 아이디어입니다.

**(이미지: 엣지 맵의 코너 주변 그래디언트 패턴이 기술자 벡터로 변환되는 과정을 보여줌)**

***

### **페이지 152**

**그래디언트 기반 특징 기술**

*   먼저, **그래디언트 기반 특징 기술**의 대표적인 개념으로서, 지금부터의 설명은 **SIFT**를 기반으로 할 것입니다.

**(이미지: SIFT 알고리즘에 대한 David G. Lowe의 논문 "Object recognition from local scale-invariant features"의 스크린샷)**

***

### **페이지 153**

**그래디언트 기반 특징 기술**

*   먼저, 각 키포인트 주변의 **지역 패치**에서 **그래디언트**를 계산합니다.
    *   이것은 우리가 엣지 탐지에서 사용했던 **소벨 연산자**와 같은 연산입니다.
    *   이를 통해 각 픽셀의 **그래디언트 크기**와 **그래디언트 방향**을 모두 얻습니다.

*   Iₓ, Iᵧ: x 및 y 방향의 이미지 미분.
*   m(x, y): 픽셀 (x, y)에서 엣지가 얼마나 강한지를 알려주는 **그래디언트 크기**.
*   θ(x, y): 픽셀 (x, y)에서 엣지의 방향을 알려주는 **그래디언트 방향**.

**(수식 및 이미지: 그래디언트 크기와 방향을 계산하는 공식과, 이미지에서 그래디언트 X, Y를 계산하여 그래디언트 크기와 방향 맵을 생성하는 과정을 보여주는 다이어그램)**

***

### **페이지 154**

**HOG (방향성 그래디언트 히스토그램)**

*   자, 이제 우리는 현재 키포인트(보통 코너) 주변에 그래디언트가 어떻게 분포되어 있는지 **기술**하고 싶다고 말했습니다.
    *   그렇다면 이 분포를 어떻게 표현할 수 있을까요?
*   음, 어떤 값들이 어떻게 분포되어 있는지 이해하고 싶을 때, 우리는 보통 무엇을 사용하나요? -> **히스토그램**.
*   따라서 **그래디언트 히스토그램(HOG)**을 만드는 것이 합리적입니다 — 지역 패치 내에서 그래디언트가 얼마나 강하고 어떤 방향으로 분포되어 있는지를 요약하는 방법입니다.

**(이미지: 그래디언트 맵(벡터)이 생성되는 과정을 보여주는 다이어그램)**

***

### **페이지 155**

**HOG (방향성 그래디언트 히스토그램)**

*   자, 이제 우리는 현재 키포인트(보통 코너) 주변에 그래디언트가 어떻게 분포되어 있는지 **기술**하고 싶다고 말했습니다.
    *   그렇다면 이 분포를 어떻게 표현할 수 있을까요?
*   음, 어떤 값들이 어떻게 분포되어 있는지 이해하고 싶을 때, 우리는 보통 무엇을 사용하나요? -> **히스토그램**.
*   따라서 **그래디언트 히스토그램(HOG)**을 만드는 것이 합리적입니다 — 지역 패치 내에서 그래디언트가 얼마나 강하고 어떤 방향으로 분포되어 있는지를 요약하는 방법입니다.

**(이미지: 입력 이미지의 키포인트 주변 8x8 패치에서 그래디언트 맵을 계산하고, 이를 히스토그램으로 변환하는 과정을 보여주는 그림)**

***

### **페이지 156**

**HOG (방향성 그래디언트 히스토그램)**

*   자, 이제 우리는 현재 키포인트(보통 코너) 주변에 그래디언트가 어떻게 분포되어 있는지 **기술**하고 싶다고 말했습니다.
    *   그렇다면 이 분포를 어떻게 표현할 수 있을까요?
*   음, 어떤 값들이 어떻게 분포되어 있는지 이해하고 싶을 때, 우리는 보통 무엇을 사용하나요? -> **히스토그램**.
*   따라서 **그래디언트 히스토그램(HOG)**을 만드는 것이 합리적입니다 — 지역 패치 내에서 그래디언트가 얼마나 강하고 어떤 방향으로 분포되어 있는지를 요약하는 방법입니다.

**(이미지: 그래디언트 맵에서 히스토그램으로 변환하는 과정에 물음표를 표시하여 어떻게 변환하는지 질문)**

***

### **페이지 157**

**HOG (방향성 그래디언트 히스토그램)**

*   하지만 여기서 중요한 부분이 있습니다:
    *   스칼라 값과 달리, **그래디언트는 벡터**입니다 — **크기**(엣지가 얼마나 강한지)와 **방향**(엣지가 어느 쪽을 가리키는지)을 모두 가집니다.
    *   따라서 그래디언트를 "누적"할 때, 단순히 개수를 세는 것이 아니라 그들의 **방향**도 고려해야 합니다.

**(이미지: 이전 페이지의 그림을 반복하여 보여줌)**

***

### **페이지 158**

**HOG (방향성 그래디언트 히스토그램)**

*   하지만 여기서 중요한 부분이 있습니다:
    *   스칼라 값과 달리, **그래디언트는 벡터**입니다 — **크기**(엣지가 얼마나 강한지)와 **방향**(엣지가 어느 쪽을 가리키는지)을 모두 가집니다.
    *   따라서 그래디언트를 "누적"할 때, 단순히 개수를 세는 것이 아니라 그들의 **방향**도 고려해야 합니다.
*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.

**(이미지: 그래디언트 맵을 45° 단위로 나누어 8개의 방향 빈으로 만드는 과정을 보여주는 그림)**

***

### **페이지 159**

**HOG (방향성 그래디언트 히스토그램)**

*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.
*   그런 다음 각 픽셀에 대해 그래디언트 방향을 보고 해당 방향 빈에 **크기**를 더합니다.

**(이미지: 그래디언트 맵의 각 벡터를 8개의 방향 빈 중 하나에 할당하는 과정을 시각적으로 보여줌)**

***

### **페이지 160**

**HOG (방향성 그래디언트 히스토그램)**

*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.
*   그런 다음 각 픽셀에 대해 그래디언트 방향을 보고 해당 방향 빈에 **크기**를 더합니다.

**(이미지: 특정 픽셀의 그래디언트 벡터가 방향 1번 빈에 해당하고, 그 크기(magnitude)가 해당 빈에 더해지는 과정을 보여줌)**

***

### **페이지 161**

**HOG (방향성 그래디언트 히스토그램)**

*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.
*   그런 다음 각 픽셀에 대해 그래디언트 방향을 보고 해당 방향 빈에 **크기**를 더합니다.

**(이미지: 그래디언트 크기가 1번 빈의 막대그래프로 누적되는 것을 보여줌)**

***

### **페이지 162**

**HOG (방향성 그래디언트 히스토그램)**

*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.
*   그런 다음 각 픽셀에 대해 그래디언트 방향을 보고 해당 방향 빈에 **크기**를 더합니다.

**(이미지: 다른 픽셀의 그래디언트 벡터가 7번 빈에 해당하고, 그 크기가 해당 빈에 더해지는 과정을 보여줌)**

***

### **페이지 163**

**HOG (방향성 그래디언트 히스토그램)**

*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.
*   그런 다음 각 픽셀에 대해 그래디언트 방향을 보고 해당 방향 빈에 **크기**를 더합니다.

**(이미지: 그래디언트 크기가 7번 빈의 막대그래프로 누적되는 것을 보여줌)**

***

### **페이지 164**

**HOG (방향성 그래디언트 히스토그램)**

*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.
*   그런 다음 각 픽셀에 대해 그래디언트 방향을 보고 해당 방향 빈에 **크기**를 더합니다.

**(이미지: 또 다른 픽셀의 그래디언트 벡터가 1번 빈에 해당하고, 그 크기가 기존 값에 추가로 누적되는 과정을 보여줌)**

***

### **페이지 165**

**HOG (방향성 그래디언트 히스토그램)**

*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.
*   그런 다음 각 픽셀에 대해 그래디언트 방향을 보고 해당 방향 빈에 **크기**를 더합니다.

**(이미지: 1번 빈의 막대그래프가 이전 값에 새로운 값이 더해져 더 높아지는 것을 보여줌)**

***

### **페이지 166**

**HOG (방향성 그래디언트 히스토그램)**

*   그래디언트 방향은 0°에서 360°까지의 범위를 가지므로, 이 범위를 각각 45°를 커버하는 **8개의 빈(bin)**으로 나눕니다.
*   그런 다음 각 픽셀에 대해 그래디언트 방향을 보고 해당 방향 빈에 **크기**를 더합니다.
*   패치의 모든 픽셀에 대해 이 작업을 수행하면 다음과 같은 정보를 알려주는 히스토그램을 얻게 됩니다:
    *   이 영역에서 어떤 방향의 그래디언트가 가장 우세하고, 얼마나 강한지.

**(이미지: 8x8 패치의 모든 그래디언트 정보를 취합하여 최종적인 8방향 HOG(방향성 그래디언트 히스토그램)를 생성하는 과정을 보여줌)**

***

### **페이지 167**

**HOG (방향성 그래디언트 히스토그램)**

*   패치의 모든 픽셀에 대해 이 작업을 수행하면 다음과 같은 정보를 알려주는 히스토그램을 얻게 됩니다:
    *   이 영역에서 어떤 방향의 그래디언트가 가장 우세하고, 얼마나 강한지.
*   이 히스토그램은 이제 지역 그래디언트 구조의 **간결한 요약(기술자)**이 됩니다 — 키포인트 주변의 **질감과 모양 패턴**을 포착하는 시그니처입니다.

**(이미지: 생성된 HOG를 시각화하여 하나의 기술자(descriptor)로 표현하는 과정을 보여줌)**

***

### **페이지 168**

**HOG (방향성 그래디언트 히스토그램)**

*   지금까지 우리는 지역 패치 내에서 **방향성 그래디언트 히스토그램**을 만들었습니다.
    *   이 히스토그램은 이미 어떤 방향의 그래디언트가 우세한지를 알려줍니다 —
        따라서 키포인트 주변의 전체적인 질감이나 엣지 패턴에 대한 좋은 요약입니다.
*   **하지만 문제가 있습니다:**
    *   우리는 전체 패치(예: 키포인트 주변의 16×16 창)에 대해 **단 하나의 히스토그램**만 사용하고 있습니다.
    *   이는 전체 영역의 모든 그래디언트를 함께 섞는 것을 의미합니다.

**(이미지: 전체 이미지에서 추출된 16x16 패치 전체에 대해 단 하나의 히스토그램만 생성되는 문제점을 지적)**

***

### **페이지 169**

**HOG (방향성 그래디언트 히스토그램)**

*   그러면 우리는 더 이상 알 수 없게 됩니다:
    *   패치의 어느 부분에 수평 엣지가 있는지?
    *   패치의 어느 부분에 수직 엣지가 있는지?
*   이것은 얼굴을 다음과 같이 설명하는 것과 같습니다:
    *   수직선과 수평선이 모두 있다.
    *   하지만 그 선들이 **어디에** 있는지는 모른다.

**(이미지: 이전 페이지의 그림을 반복하여 보여줌)**

***

### **페이지 170**

**왜 하위 영역으로 나누는가**

*   이 문제를 해결하기 위해, 우리는 패치를 더 작은 **하위 영역**으로 나눕니다.
    *   예를 들어, 16×16 패치를 **4×4 하위 영역**으로 나눕니다.
    *   그리고 **각각**에 대해 별도의 그래디언트 히스토그램을 만듭니다.
*   이제 각 하위 영역은 지역적 방향 패턴을 포착합니다 —
    *   왼쪽 위, 오른쪽 위, 왼쪽 아래, 오른쪽 아래 등.
*   이 모든 히스토그램을 결합하면,
    *   어떤 그래디언트 방향이 존재하는지뿐만 아니라, 패치 내에서 **어디에** 위치하는지도 알 수 있습니다.

**(이미지: 16x16 패치를 4x4 하위 영역으로 나누고, 각 하위 영역에 대해 HOG를 계산하는 과정을 보여주는 그림)**

***

### **페이지 171**

**완전한 128차원 SIFT 기술자 구축**

*   이러한 공간적 구성은 기술자에 훨씬 더 많은 구별 능력을 부여합니다.
    *   그래디언트 구성은 비슷하지만 공간적 배열이 다른 패턴을 구별할 수 있게 해줍니다.
*   4×4 = **16개**의 하위 영역이 있고, 각각 **8개**의 히스토그램 값을 가지므로, 결국 16 × 8 = **128차원 벡터**가 됩니다.
*   이 128-D 벡터가 **SIFT 기술자**입니다 — 그래디언트가 키포인트 주변에서 **방향적으로** 그리고 **공간적으로** 어떻게 분포되어 있는지를 설명하는 간결한 수치적 "시그니처"입니다.

**(이미지: 16개의 하위 영역과 각 영역의 8개 방향 빈을 곱하여 128차원의 SIFT 기술자가 생성되는 과정을 보여줌)**

***

### **페이지 172**

**완전한 128차원 SIFT 기술자 구축**

*   이러한 공간적 구성은 기술자에 훨씬 더 많은 구별 능력을 부여합니다.
    *   그래디언트 구성은 비슷하지만 공간적 배열이 다른 패턴을 구별할 수 있게 해줍니다.
*   4×4 = **16개**의 하위 영역이 있고, 각각 **8개**의 히스토그램 값을 가지므로, 결국 16 × 8 = **128차원 벡터**가 됩니다.
*   이 128-D 벡터가 **SIFT 기술자**입니다 — 그래디언트가 키포인트 주변에서 **방향적으로** 그리고 **공간적으로** 어떻게 분포되어 있는지를 설명하는 간결한 수치적 "시그니처"입니다.

**(이미지: 16개의 8-빈 히스토그램을 순서대로 연결하여 128차원 SIFT 기술자 벡터를 만들고, 이를 정규화하는 과정을 보여주는 그림)**

***

### **페이지 173**

**기술자를 회전 불변으로 만들기**

*   지금까지 우리는 지역 패치 내의 그래디언트 분포를 기반으로 128차원 기술자를 만드는 방법을 설명했습니다.
*   하지만 이미지가 **회전**되면 어떻게 될까요?
*   장면 속의 동일한 물리적 코너를 상상해 봅시다.
    *   두 개의 이미지에 나타납니다: 하나는 정상, 다른 하나는 30도 회전됨.

**(이미지: 장난감 트럭의 원본 이미지와 회전된 이미지에서 동일한 SIFT 특징들이 어떻게 매칭되는지를 보여주는 그림)**

***

### **페이지 174**

**기술자를 회전 불변으로 만들기**

*   그러면 그래디언트에는 무슨 일이 일어날까요?
    *   패치 내의 모든 그래디언트 방향도 30도 회전할 것입니다.
*   이는 우리가 만든 모든 방향 히스토그램이 이제 한두 개의 빈만큼 이동한다는 것을 의미합니다.
    *   따라서 전체 기술자가 변경됩니다.
*   **동일한 코너**임에도 불구하고,
    *   단지 이미지가 회전했다는 이유만으로 **다른 기술자**를 생성할 것입니다.

**(이미지: 동일한 코너라도 이미지가 회전하면 SIFT 기술자가 완전히 달라지는 문제점을 보여주는 그림)**

***

### **페이지 175**

**기술자를 회전 불변으로 만들기**

*   이는 우리가 만든 모든 방향 히스토그램이 이제 한두 개의 빈만큼 이동한다는 것을 의미합니다.
    *   따라서 전체 기술자가 변경됩니다.
*   **동일한 코너**임에도 불구하고,
    *   단지 이미지가 회전했다는 이유만으로 **다른 기술자**를 생성할 것입니다.
*   우리는 **동일한 실제 세계의 점**이 이미지가 어떻게 **회전**되든 상관없이 **동일한 기술자**를 갖기를 원합니다.

**(이미지: 회전된 SIFT 기술자를 다시 원래 방향으로 정렬하여 원본 SIFT 기술자와 동일하게 만드는 개념을 보여주는 그림)**

***

### **페이지 176**

**주 방향을 이용한 회전 불변성**

*   키포인트의 지역 패치 내의 모든 그래디언트 방향을 보고 가장 빈번하거나 가장 강한 방향을 찾습니다.
    *   이것이 패치의 **주(지배적) 방향**이 됩니다.
*   그런 다음, 이 방향이 0°를 가리키도록 전체 기술자를 회전시킵니다.
*   다시 말해, 우리는 기술자의 좌표 프레임을 키포인트의 주 방향에 맞춥니다.

**(이미지: 45°의 주 방향을 가진 패치를 회전시켜 주 방향이 0°가 되도록 정규화하는 과정을 보여주는 그림)**

***

### **페이지 177**

**주 방향을 이용한 회전 불변성**

*   키포인트의 지역 패치 내의 모든 그래디언트 방향을 보고 가장 빈번하거나 가장 강한 방향을 찾습니다.
    *   이것이 패치의 **주(지배적) 방향**이 됩니다.
*   그런 다음, 이 방향이 0°를 가리키도록 전체 기술자를 회전시킵니다.
*   다시 말해, 우리는 기술자의 좌표 프레임을 키포인트의 주 방향에 맞춥니다.

**(이미지: 90°의 주 방향을 가진 패치도 회전시켜 0°로 정규화하는 과정을 보여줌)**

***

### **페이지 178**

**주 방향을 이용한 회전 불변성**

*   키포인트의 지역 패치 내의 모든 그래디언트 방향을 보고 가장 빈번하거나 가장 강한 방향을 찾습니다.
    *   이것이 패치의 **주(지배적) 방향**이 됩니다.
*   그런 다음, 이 방향이 0°를 가리키도록 전체 기술자를 회전시킵니다.
*   다시 말해, 우리는 기술자의 좌표 프레임을 키포인트의 주 방향에 맞춥니다.

**(이미지: 두 개의 회전된 이미지에서 각각 아핀 영역을 추출하고, 주 방향을 기준으로 정규화하여 회전 모호성을 제거한 후 기술자를 비교하는 전체 과정을 보여주는 그림)**

***

### **페이지 179**

**주 방향을 이용한 회전 불변성**

*   키포인트의 지역 패치 내의 모든 그래디언트 방향을 보고 가장 빈번하거나 가장 강한 방향을 찾습니다.
    *   이것이 패치의 **주(지배적) 방향**이 됩니다.
*   그런 다음, 이 방향이 0°를 가리키도록 전체 기술자를 회전시킵니다.
*   다시 말해, 우리는 기술자의 좌표 프레임을 키포인트의 주 방향에 맞춥니다.

**(이미지: 키포인트의 주 방향(θ)을 기준으로 좌표와 그래디언트를 모두 회전시키는 개념을 시각적으로 보여주는 그림)**

***

### **페이지 180**

**최종 특징 집합**

*   SIFT에서, 각 탐지된 키포인트는 **하나의 128차원 기술자**를 생성합니다.
    *   따라서 이미지당 기술자의 수는 하나로 고정된 것이 아니라 **탐지된 키포인트의 수와 동일**합니다.
    *   이미지 = 여러 키포인트 → 여러 128D 기술자.
*   따라서 최종 특징 집합은 **N × 128 행렬**입니다.
    *   여기서 N은 키포인트의 수입니다.

**(이미지: 이미지에서 여러 키포인트를 추출하고, 각 키포인트에서 128D 기술자를 생성하여 N x 128D의 가변 길이 특징 집합을 만드는 과정을 보여줌)**

***

### **페이지 181**

**SIFT 기술자 요약**

*   **핵심 아이디어:**
    *   각 키포인트 주변에서 **HOG (방향성 그래디언트 히스토그램)**를 계산합니다.

*   **공간 구조:**
    *   지역 패치를 **4×4 하위 영역**으로 나누고, 각 하위 영역에 대해 **방향 히스토그램 (8개 빈)**을 만듭니다.

*   **특징 차원:**
    *   모든 히스토그램을 연결하여 **128차원 기술자** (4×4×8)를 만듭니다.
    *   따라서 최종 이미지 표현은 (# 키포인트 × 128) 차원을 가집니다 — 키포인트당 하나의 128-D 벡터.

*   **회전 불변:**
    *   패치는 키포인트의 주 방향에 맞춰 정렬됩니다.

***

### **페이지 182**

**SIFT 이후**

*   SIFT 이후, 여러 다른 지역 특징 기술자 (및 탐지기)가 제안되었습니다.
    *   **SURF, BRIEF, ORB, FAST.**
*   이후의 기술자들은 정확도보다는 속도에 더 중점을 두었습니다. 많은 경우 유사한 성능을 보이지만, SIFT는 여전히 안정성과 견고성으로 인해 널리 사용됩니다.

**(표: SIFT, SURF, BRIEF, ORB, FAST 기술자의 연도, 유형, 주요 목표, 매칭 정확도/견고성, 속도를 비교하는 표)**

***

### **페이지 183**

**특징 매칭**

**(이미지: 데이터 과학을 상징하는 그래픽)**

***

### **페이지 184**

**특징 매칭이란 무엇인가?**

*   **특징 매칭**은 두 개 이상의 이미지 사이에서 **대응하는 점**을 찾는 과정입니다.
    *   장면에서 동일한 물리적 위치나 구조를 나타내는 점들.
*   두 이미지가 다른 시점, 스케일, 또는 조명 조건에서 동일한 객체나 장면을 보여줄 때, 특징 매칭은 비슷하게 보이는 점들을 식별하여 그들을 **연결**할 수 있게 해줍니다.

**(이미지: 노트르담 대성당의 두 다른 사진에서 대응하는 특징점들을 선으로 연결하여 보여주는 그림)**

***

### **페이지 185**

**작동 원리**

*   두 이미지, **이미지 A**와 **이미지 B**에 대해:
    *   먼저, 각 이미지에서 **키포인트**(예: 코너 또는 블롭)를 탐지합니다.
    *   각 키포인트에 대해, 그 지역적 외형을 나타내는 **기술자 벡터**(예: SIFT의 128D)를 계산합니다.
    *   두 이미지 간의 모든 기술자를 비교합니다.
    *   (거리 측정법에 기반하여) **가장 유사한** 한 쌍의 기술자가 **매치**로 간주됩니다.

**(이미지: 두 개의 다른 각도에서 찍은 마네키네코 고양이 인형 사진에서 A2와 B2가 같은 점인지 묻고, 각 점의 기술자를 계산하고 유사도를 측정하여 매칭 여부를 결정하는 과정을 보여주는 그림)**

***

### **페이지 186**

**왜 특징 매칭을 하는가?**

*   특징 매칭은 많은 컴퓨터 비전 작업에서 기본적인 단계입니다:
    *   특징 매칭은 지역 기술자를 비교하여 이미지들을 함께 연결합니다 —
        기하학적 정렬, 객체 인식, 다중 뷰 이해를 가능하게 합니다.

**이미지 검색**

**(이미지: 여러 객체가 있는 장면에서 특정 객체(기차, 개구리)를 특징 매칭을 통해 찾아내는 이미지 검색 예시)**

***

### **페이지 187**

**왜 특징 매칭을 하는가?**

*   특징 매칭은 많은 컴퓨터 비전 작업에서 기본적인 단계입니다:
    *   특징 매칭은 지역 기술자를 비교하여 이미지들을 함께 연결합니다 —
        기하학적 정렬, 객체 인식, 다중 뷰 이해를 가능하게 합니다.

**이미지 스티칭 (파노라마)**

**(이미지: 여러 장의 겹치는 산 사진에서 특징점을 매칭하여 하나의 넓은 파노라마 이미지로 합치는 과정을 보여주는 그림)**

***

### **페이지 188**

**왜 특징 매칭을 하는가?**

*   특징 매칭은 많은 컴퓨터 비전 작업에서 기본적인 단계입니다:
    *   특징 매칭은 지역 기술자를 비교하여 이미지들을 함께 연결합니다 —
        기하학적 정렬, 객체 인식, 다중 뷰 이해를 가능하게 합니다.

**이미지 스티칭 (파노라마)**

**(이미지: 요세미티 국립공원의 여러 사진을 특징 매칭을 통해 파노라마로 만드는 예시)**

***

### **페이지 189**

**왜 특징 매칭을 하는가?**

*   특징 매칭은 많은 컴퓨터 비전 작업에서 기본적인 단계입니다:
    *   특징 매칭은 지역 기술자를 비교하여 이미지들을 함께 연결합니다 —
        기하학적 정렬, 객체 인식, 다중 뷰 이해를 가능하게 합니다.

**이미지 스티칭 (파노라마)**

**(이미지: 두 개의 겹치는 이미지에서 각각의 특징점(u1, u2, ..., u128)과 (v1, v2, ..., v128)을 추출하는 과정을 보여줌)**

***

### **페이지 190**

**왜 특징 매칭을 하는가?**

*   특징 매칭은 많은 컴퓨터 비전 작업에서 기본적인 단계입니다:
    *   특징 매칭은 지역 기술자를 비교하여 이미지들을 함께 연결합니다 —
        기하학적 정렬, 객체 인식, 다중 뷰 이해를 가능하게 합니다.

| 작업 | 설명 |
| :--- | :--- |
| **이미지 스티칭** | 겹치는 사진들을 정렬하고 병합하여 하나의 파노라마로 만듦 |
| **객체 인식** | 다른 이미지에서 동일한 객체를 식별함 |
| **3D 재구성 (SfM)** | 여러 뷰에서 키포인트를 매칭하여 3D 구조를 복원함 |
| **추적 및 위치 파악** | 비디오 프레임에서 시간의 흐름에 따라 동일한 특징을 추적함 |

***

### **페이지 191**

**쌍별 비교**

*   **쌍별 비교**
    *   이미지 A의 모든 키포인트에 대해, 그 기술자를 **이미지 B의 모든 기술자**와 비교합니다.
    *   두 기술자 간의 유사성(또는 차이)은 **거리 측정법**으로 측정되며, 일반적으로 다음을 사용합니다:
        *   **유클리드 거리** (SIFT, SURF용)
        *   **해밍 거리** (ORB, BRIEF와 같은 이진 기술자용)
    *   이는 많은 수의 쌍별 거리를 생성합니다.

**(이미지: 이미지 A의 한 키포인트(A2)를 이미지 B의 모든 키포인트(B1~B5)와 비교하는 과정을 보여주는 그림)**

***

### **페이지 192**

**쌍별 비교**

*   **쌍별 비교**
    *   이미지 A의 모든 키포인트에 대해, 그 기술자를 **이미지 B의 모든 기술자**와 비교합니다.
    *   두 기술자 간의 유사성(또는 차이)은 **거리 측정법**으로 측정되며, 일반적으로 다음을 사용합니다:
        *   **유클리드 거리** (SIFT, SURF용)
        *   **해밍 거리** (ORB, BRIEF와 같은 이진 기술자용)
    *   이는 많은 수의 쌍별 거리를 생성합니다.

**(이미지: A2와 B의 모든 키포인트 간의 거리를 계산하고, 그 중 가장 가까운 B2를 찾는 과정을 보여주는 그림)**

***

### **페이지 193**

**유클리드 거리**

*   **유클리드 거리**는 두 벡터가 공간에서 얼마나 떨어져 있는지를 측정합니다 — 본질적으로 두 점 사이의 **직선 거리**입니다.
    *   특징 매칭에서는 두 기술자가 얼마나 유사하거나 다른지를 정량화합니다.

*   **수학적 정의**
    *   두 기술자 벡터에 대해:
        *   a = [a₁, a₂, ..., aₙ]
        *   b = [b₁, b₂, ..., bₙ]
    *   유클리드 거리는 다음과 같이 정의됩니다:
        *   d(a, b) = √( (a₁-b₁)² + (a₂-b₂)² + ... + (aₙ-bₙ)² )
    *   각 구성 요소의 차이를 제곱하고, 합산한 다음, 제곱근을 취합니다.

*   유클리드 거리는 두 특징 벡터 사이의 직선 거리를 측정합니다. 이미지 매칭에서는 **유사성 척도**로 사용되며 — 거리가 작을수록 기술자 간의 매칭이 더 강함을 나타냅니다.

**(수식 및 이미지: 유클리드 거리의 기하학적 의미와 계산 공식을 보여주는 그림)**

***

### **페이지 194**

**최근접 이웃 매칭**

*   **정의**
    *   이미지 A의 각 기술자에 대해, 거리 측정법(예: 유클리드 거리)을 기반으로 이미지 B에서 **가장 가까운(가장 유사한)** 기술자를 찾습니다.
*   **수학적으로:**
    *   이것은 이미지 B의 모든 기술자 중에서 aᵢ의 **최근접 이웃**입니다.

*   **최근접 이웃 거리 비율 (NNDR) – Lowe의 비율 테스트**
    *   SIFT 논문(Lowe, 2004)에서 이 아이디어는 개선되었습니다:
        가장 가까운 하나만 취하는 대신, **가장 가까운 두 이웃을 비교**합니다.
    *   r = d₁ / d₂
        *   d₁: **가장 가까운 이웃**까지의 거리
        *   d₂: **두 번째로 가까운 이웃**까지의 거리
    *   r < 0.75인 경우에만 매치를 유지합니다.
    *   이 버전은 종종 **"비율 테스트"** 또는 **"최근접 이웃 거리 비율(NNDR) 테스트"**라고 불립니다.

***

### **페이지 195**

**특징 매칭 결과**

**(이미지: 노트르담 대성당 사진에서 특징 매칭 결과를 보여주는 그림. 올바른 매치(녹색 선)와 잘못된 매치(빨간 선)가 함께 표시됨)**

***

### **페이지 196**

**특징 매칭 결과**

**(이미지: 상자 이미지와 여러 물체가 있는 장면 이미지 간의 특징 매칭 결과. 장면 속 상자 위치에 매칭이 집중됨)**

***

### **페이지 197**

**특징 매칭 결과**

**(이미지: 두 개의 다른 각도에서 찍은 잡지 표지들 사진 간의 특징 매칭 결과)**

***

### **페이지 198**

**특징 매칭 결과**

**(이미지: TV 쇼 '빅뱅 이론'의 한 장면. 티셔츠의 로고와 원본 로고 이미지 간의 특징 매칭 결과)**

***

### **페이지 199**

**특징 매칭 결과**

**(이미지: 두 개의 다른 정지 신호 표지판 사진에서 'TO' 부분의 특징이 서로 유사하게 매칭되는 것을 보여주는 그림)**

***

### **페이지 200**

**객체 인식 (검색)**

*   특징 매칭은 두 이미지 사이의 어떤 **작은 영역**이 비슷하게 보이는지 알려줍니다.
*   만약 한 이미지의 많은 키포인트가 다른 이미지에 **대응하는 매치**를 가지고 있다면,
*   이는 그 두 영역이 **동일한 객체나 패턴**을 포함할 가능성이 높다는 것을 의미합니다.

**(이미지: 여러 객체가 있는 장면에서 특정 객체들을 특징 매칭을 통해 찾아내는 예시)**

***

### **페이지 201**

**특징 매칭을 이용한 이미지 스티칭**

*   여러 겹치는 이미지(예: 파노라마 사진)를 하나의 **끊김 없는 넓은 이미지**로 결합합니다.
*   이미지를 스티칭하려면, **어떤 부분이 겹치는지** 찾아야 하며 — 이는 이미지 간의 **특징점을 매칭**하여 수행됩니다.
*   이 매칭된 점들의 패턴을 사용하여 한 이미지가 다른 이미지에 대해 어떻게 **이동하거나 회전**해야 하는지 추정합니다.

**(이미지: 요세미티 국립공원의 여러 사진을 특징 매칭을 통해 파노라마로 만드는 예시)**

***

### **페이지 202**

**특징 매칭을 이용한 이미지 스티칭**

*   여러 겹치는 이미지(예: 파노라마 사진)를 하나의 **끊김 없는 넓은 이미지**로 결합합니다.
*   이미지를 스티칭하려면, **어떤 부분이 겹치는지** 찾아야 하며 — 이는 이미지 간의 **특징점을 매칭**하여 수행됩니다.
*   이 매칭된 점들의 패턴을 사용하여 한 이미지가 다른 이미지에 대해 어떻게 **이동하거나 회전**해야 하는지 추정합니다.

**(이미지: 여러 장의 겹치는 산 사진에서 특징점을 매칭하여 하나의 넓은 파노라마 이미지로 합치는 과정을 보여주는 그림)**

***

### **페이지 203**

**컴퓨터 비전의 기초**
**2025년 2학기**
**- 엄광현 –**
**인공지능학과**

***

### **페이지 204**

**강의 5:**
**이미지 분류**
*   이미지 분류란 무엇인가?
*   전통적인 접근법: 특징 기반 분류
*   최근접 이웃 분류
*   선형 분류기

***

### **페이지 205**

**고양이 vs. 개**

*   **"어떻게 구별할 수 있나요?"**
    *   아마도 귀, 눈, 털 패턴, 심지어 얼굴 모양을 볼 것입니다.

**(이미지: 고양이와 개의 얼굴 사진)**

***

### **페이지 206**

**"숫자만으로 고양이를 알아볼 수 있나요?"**

*   **숫자로서의 픽셀**
    *   **"하지만 이 고양이 이미지를 가져와서 픽셀 값만 보여준다면 — 큰 배열 속의 숫자들만 — 여전히 고양이인지 알 수 있을까요?"**
*   **그렇지 않습니다. 개별 픽셀 값은 우리에게 많은 것을 알려주지 않습니다.**

**(이미지: 컴퓨터가 이미지를 픽셀 값의 숫자 배열로 보는 것을 보여주는 그림)**

***

### **페이지 207**

**픽셀이 아닌 패턴에 대하여**

*   **"그렇다면 실제로 인식을 가능하게 하는 것은 무엇일까요?"**
    *   원시 픽셀 값이 아닙니다.
    *   **픽셀들이 함께 변하는 패턴**입니다: 모양을 정의하는 엣지, 털을 암시하는 텍스처, 그리고 객체를 배경과 분리하는 대비.

**(이미지: 고양이 사진에서 눈과 털 부분의 픽셀 패턴을 확대하여 보여주는 그림)**

***

### **페이지 208**

**엣지 탐지에서 특징 매칭까지**

*   지금까지 우리는 이미지에서 엣지를 탐지하고, (코너와 같은) 특징을 탐지하며, 그 특징을 기술하고, 이미지 간에 특징 매칭을 수행하는 방법을 배웠습니다.

**(이미지: 원본 고양이 사진 → 캐니 엣지 맵 → 코너 탐지 및 각 코너에 대한 기술자 벡터 생성 과정을 보여주는 그림)**

***

### **페이지 209**

**엣지 탐지에서 특징 매칭까지**

*   지금까지 우리는 이미지에서 엣지를 탐지하고, (코너와 같은) 특징을 탐지하며, 그 특징을 기술하고, 이미지 간에 특징 매칭을 수행하는 방법을 배웠습니다.

**(이미지: 두 이미지 간의 특징 매칭 과정을 보여주는 그림)**

***

### **페이지 210**

**엣지 탐지에서 특징 매칭까지**

*   지금까지 우리는 이미지에서 엣지를 탐지하고, (코너와 같은) 특징을 탐지하며, 그 특징을 기술하고, 이미지 간에 특징 매칭을 수행하는 방법을 배웠습니다.

**(이미지: 티셔츠 로고와 원본 로고 간의 특징 매칭 결과)**

***

### **페이지 211**

**특징에서 이미지 분류로**

*   그렇다면, 이미지에서 특징을 탐지하고 그 기술자를 추출했을 때,
*   우리는 실제로 어떻게 **이미지 분류**를 수행할까요?
*   컴퓨터는 이미지가 **고양이**인지 **개**인지 어떻게 알 수 있을까요?

**(이미지: 고양이 이미지에서 추출된 여러 기술자 벡터들이 '고양이' 또는 '개'로 분류되는 과정을 물음표로 표현한 그림)**

***

### **페이지 212**

**특징에서 이미지 분류로**

*   그렇다면, 이미지에서 특징을 탐지하고 그 기술자를 추출했을 때,
*   우리는 실제로 어떻게 **이미지 분류**를 수행할까요?
*   컴퓨터는 이미지가 **고양이**인지 **개**인지 어떻게 알 수 있을까요?

**(이미지: 픽셀 값 배열 → 엣지 맵 → 기술자 벡터 → '고양이' 또는 '개' 분류 과정을 보여주는 그림)**

***

### **페이지 213**

**예시로부터 학습하기**

*   만약 아무도 당신에게 고양이를 보여주며 "이것은 고양이야"라고 말해주지 않았다면, 당신은 고양이를 인식할 수 없을 것입니다.
    *   당신은 적어도 하나의 **레이블이 붙은 예시** — "고양이"라고 설명된 이미지 — 를 본 적이 있어야 합니다.
    *   개에게도 마찬가지입니다.
*   예시와 레이블 없이는 인간도 컴퓨터도 무엇을 보는지 식별할 수 없습니다.

**(이미지: 레이블이 없을 때(물음표)와 "이것은 고양이야!"라는 레이블이 주어졌을 때(느낌표)의 차이를 보여주는 그림)**

***

### **페이지 214**

**예시로부터 학습하기**

*   만약 아무도 당신에게 고양이를 보여주며 "이것은 고양이야"라고 말해주지 않았다면, 당신은 고양이를 인식할 수 없을 것입니다.
    *   당신은 적어도 하나의 **레이블이 붙은 예시** — "고양이"라고 설명된 이미지 — 를 본 적이 있어야 합니다.
    *   개에게도 마찬가지입니다.
*   예시와 레이블 없이는 인간도 컴퓨터도 무엇을 보는지 식별할 수 없습니다.

**(이미지: 레이블이 없을 때(물음표)와 "이것은 강아지야!"라는 레이블이 주어졌을 때(느낌표)의 차이를 보여주는 그림)**

***

### **페이지 215**

**특징과 예시에서 분류까지**

*   이제 우리는 탐지된 특징점과 그 기술자를 가진 이미지를 가지고 있습니다.
*   또한 예시도 가지고 있습니다:
    *   "이것은 고양이야"라고 레이블이 붙은 **고양이 이미지**,
    *   "이것은 강아지야"라고 레이블이 붙은 **강아지 이미지**.
*   그렇다면, 이 특징들과 레이블이 붙은 예시들을 사용하여 새로운 이미지를 **고양이** 또는 **개**로 어떻게 분류할 수 있을까요?

**(이미지: 새로운 이미지의 특징들과, 레이블이 있는 고양이/개 예시 이미지를 사용하여 분류하는 과정을 보여주는 그림)**

***

### **페이지 216**

**예시 이미지에서 특징 추출하기**

*   먼저 주어진 예시 이미지들로부터 특징(과 그 기술자)을 추출해야 합니다.
*   예를 들어, **고양이**와 **개** 이미지 모두에 대해 키포인트를 탐지하고 기술자를 계산합니다.
*   이러한 특징 기술자들은 각 클래스의 중요한 패턴을 나타낼 것입니다.

**(이미지: 레이블이 있는 고양이와 개 이미지 각각에서 특징 기술자를 추출하는 과정을 보여주는 그림)**

***

### **페이지 217**

**이미지 간 특징 매칭하기**

*   다음으로, 새로운 이미지와 예시 이미지의 특징 기술자(예: 128-D SIFT 벡터)를 비교할 수 있습니다.
    *   **최근접 이웃 매칭** 접근법을 사용하여, 특징점들을 점 수준에서 매칭할 수 있습니다.
    *   모든 특징 쌍 사이의 **유클리드 거리**를 계산하고, 가장 가까운 쌍을 **매칭된 특징**으로 간주합니다.
*   이 과정은 두 이미지 사이에 얼마나 많은 유사한 점이 존재하는지를 찾습니다.

**(이미지: 새로운 이미지의 기술자와 예시 이미지(고양이)의 기술자들을 비교하여 매칭하는 과정을 보여줌)**

***

### **페이지 218**

**이미지 간 특징 매칭하기**

*   다음으로, 새로운 이미지와 예시 이미지의 특징 기술자(예: 128-D SIFT 벡터)를 비교할 수 있습니다.
    *   **최근접 이웃 매칭** 접근법을 사용하여, 특징점들을 점 수준에서 매칭할 수 있습니다.
    *   모든 특징 쌍 사이의 **유클리드 거리**를 계산하고, 가장 가까운 쌍을 **매칭된 특징**으로 간주합니다.
*   이 과정은 두 이미지 사이에 얼마나 많은 유사한 점이 존재하는지를 찾습니다.

**(이미지: 매칭 결과를 보여주는 그림. 일부는 매칭되고 일부는 매칭되지 않음)**

***

### **페이지 219**

**이미지 간 특징 매칭하기**

*   다음으로, 새로운 이미지와 예시 이미지의 특징 기술자(예: 128-D SIFT 벡터)를 비교할 수 있습니다.
    *   **최근접 이웃 매칭** 접근법을 사용하여, 특징점들을 점 수준에서 매칭할 수 있습니다.
    *   모든 특징 쌍 사이의 **유클리드 거리**를 계산하고, 가장 가까운 쌍을 **매칭된 특징**으로 간주합니다.
*   이 과정은 두 이미지 사이에 얼마나 많은 유사한 점이 존재하는지를 찾습니다.

**(이미지: 새로운 고양이 이미지와 예시 고양이 이미지 간에는 많은 특징이 매칭되지만, 예시 개 이미지와는 매칭되지 않는 것을 보여줌)**

***

### **페이지 220**

**매칭 결과에 기반한 클래스 결정**

*   우리는 새로운 이미지와 **더 많은 특징이 매칭된** 예시 이미지를 선택할 수 있습니다.
*   **더 많은 수의 매치**를 가진 예시가 **더 유사하다**고 간주됩니다.
*   따라서, 매칭 결과를 바탕으로 새로운 이미지가 **고양이**인지 **개**인지 결정할 수 있습니다.
*   **다음을 사용할 수 있습니다:**
    *   좋은 매치의 수
    *   평균 기술자 거리
    *   최소 기술자 거리

**(이미지: 고양이 예시와 더 많은 매치가 발생했으므로, "이것은 고양이다!"라고 결론 내리는 과정을 보여줌)**

***

### **페이지 221**

**한계 — 클래스당 단 하나의 예시**

*   만약 각 클래스에 대해 **단 하나의 예시 이미지**만 있다면 어떨까요?
*   고양이에는 **많은 변형**이 있을 수 있습니다 — 다른 자세, 색상, 배경.
*   새로운 고양이 이미지는 그 단 하나의 예시 이미지와 잘 매칭되지 않을 수 있습니다.

**(이미지: 단 하나의 고양이 예시만 보고 다양한 종류의 새로운 고양이들을 인식해야 하는 어려움을 보여주는 그림)**

***

### **페이지 222**

**한계 — 클래스당 단 하나의 예시**

*   만약 각 클래스에 대해 **단 하나의 예시 이미지**만 있다면 어떨까요?
*   고양이에는 **많은 변형**이 있을 수 있습니다 — 다른 자세, 색상, 배경.
*   새로운 고양이 이미지는 그 단 하나의 예시 이미지와 잘 매칭되지 않을 수 있습니다.

**(이미지: 단 하나의 고양이 예시만 보고 수십 종의 다양한 고양이 사진을 보고 "이게.. 뭐지??"라고 혼란스러워하는 모습을 보여주는 그림)**

***

### **페이지 223**

**여러 개의 레이블된 예시의 필요성**

*   따라서, 우리는 클래스당 단 하나가 아닌 **여러 개의 예시**가 필요할 수 있습니다.
*   각 예시 이미지는 레이블(예: 고양이, 개)과 함께 제공되어야 합니다.
*   이는 모델이 **각 클래스 내의 다양성**을 포착하는 데 도움이 됩니다.

**(이미지: 단 하나의 예시 대신, "이것들은 모두 고양이야!"라고 레이블된 여러 고양이 예시를 보여주는 그림)**

***

### **페이지 224**

**여러 예시와의 매칭**

*   클래스당 여러 예시가 있더라도, 우리는 여전히 동일한 방식으로 **특징 매칭**을 수행합니다.
*   하지만, 이제 우리는 새로운 이미지를 **모든 예시 이미지**와 개별적으로 비교해야 합니다.
*   매칭은 새로운 이미지와 모든 예시 사이에서 이루어져야 하므로, **계산 비용**이 크게 증가합니다.

**(이미지: 하나의 새로운 이미지를 여러 개의 예시 이미지(고양이, 개) 각각과 모두 매칭해야 하는 과정을 보여주는 그림)**

***

### **페이지 225**

**최근접 이웃 분류**

*   우리는 모든 샘플 중에서 **가장 유사한 예시**를 찾아 새로운 이미지를 분류할 수 있습니다.
*   이 **단일 최근접 예시**의 클래스를 예측으로 사용할 수 있습니다.
*   또는, **상위 k개의 가장 유사한 예시**(예: k = 2, 3, ...)를 찾아 그들 사이의 **다수결 투표**로 최종 클래스를 결정할 수 있습니다.

**(이미지: 여러 예시 중 가장 잘 매칭되는(Best Match!) 이미지를 찾아 "이것은 고양이다!"라고 예측하는 과정을 보여줌)**

***

### **페이지 226**

**최근접 이웃 분류**

*   우리는 모든 샘플 중에서 **가장 유사한 예시**를 찾아 새로운 이미지를 분류할 수 있습니다.
*   이 **단일 최근접 예시**의 클래스를 예측으로 사용할 수 있습니다.
*   또는, **상위 k개의 가장 유사한 예시**(예: k = 2, 3, ...)를 찾아 그들 사이의 **다수결 투표**로 최종 클래스를 결정할 수 있습니다.

**(이미지: 가장 가까운 3개의 예시(1순위, 2순위, 3순위)를 찾고, 다수결(고양이 2표, 개 1표)에 따라 "음.. 이건 고양이일 가능성이 더 높군! (투표됨)"이라고 결론 내리는 과정을 보여줌)**

***

### **페이지 227**

**k-최근접 이웃 (k-NN)**

*   **k-최근접 이웃** 방법이 어떻게 작동하는지 시각화해 봅시다.
*   우리는 **목표 지점**(분류할 이미지)을 가지고 있습니다.
*   특징 공간에서 **가장 가까운 세 개의 점** — 가장 유사한 예시들 — 을 찾습니다.
*   최종 클래스는 이 가장 가까운 이웃들 사이의 **투표**로 결정됩니다.

**(이미지: k-NN 분류 과정을 3단계(초기 데이터 → 거리 계산 → 이웃 찾기 및 투표)로 보여주는 다이어그램)**

***

### **페이지 228**

**k-최근접 이웃 (k-NN)**

*   **k-최근접 이웃** 방법이 어떻게 작동하는지 시각화해 봅시다.
*   우리는 **목표 지점**(분류할 이미지)을 가지고 있습니다.
*   특징 공간에서 **가장 가까운 세 개의 점** — 가장 유사한 예시들 — 을 찾습니다.
*   최종 클래스는 이 가장 가까운 이웃들 사이의 **투표**로 결정됩니다.

**(이미지: k=3일 때와 k=5일 때의 최근접 이웃을 보여주는 다이어그램. k=3일 때는 빨간 삼각형으로, k=5일 때는 파란 사각형으로 분류될 수 있음을 보여줌)**

***

### **페이지 229**

**k-최근접 이웃 (k-NN)**

*   **k-최근접 이웃** 방법이 어떻게 작동하는지 시각화해 봅시다.
*   우리는 **목표 지점**(분류할 이미지)을 가지고 있습니다.
*   특징 공간에서 **가장 가까운 세 개의 점** — 가장 유사한 예시들 — 을 찾습니다.
*   최종 클래스는 이 가장 가까운 이웃들 사이의 **투표**로 결정됩니다.

**(이미지: 이전 다이어그램에 실제 고양이와 개 이미지를 추가하여 개념을 설명)**

***

### **페이지 230**

**k-최근접 이웃 (k-NN)**

*   **k-최근접 이웃** 방법이 어떻게 작동하는지 시각화해 봅시다.
*   우리는 **목표 지점**(분류할 이미지)을 가지고 있습니다.
*   특징 공간에서 **가장 가까운 세 개의 점** — 가장 유사한 예시들 — 을 찾습니다.
*   최종 클래스는 이 가장 가까운 이웃들 사이의 **투표**로 결정됩니다.

**(이미지: 동물의 '복슬복슬함'과 '털의 밝기'를 두 축으로 하는 특징 공간에서 k-NN이 어떻게 작동하는지 보여주는 예시)**

***

### **페이지 231**

**k-최근접 이웃 (k-NN)**

*   **k-최근접 이웃** 방법이 어떻게 작동하는지 시각화해 봅시다.
*   우리는 **목표 지점**(분류할 이미지)을 가지고 있습니다.
*   특징 공간에서 **가장 가까운 세 개의 점** — 가장 유사한 예시들 — 을 찾습니다.
*   최종 클래스는 이 가장 가까운 이웃들 사이의 **투표**로 결정됩니다.

**(이미지: k 값이 1, 3, 5, 7로 변함에 따라 분류 결과가 어떻게 달라지는지를 보여주는 4개의 다이어그램)**

***

### **페이지 232**

**k-최근접 이웃 (k-NN)**

*   **k-최근접 이웃** 방법이 어떻게 작동하는지 시각화해 봅시다.
*   우리는 **목표 지점**(분류할 이미지)을 가지고 있습니다.
*   특징 공간에서 **가장 가까운 세 개의 점** — 가장 유사한 예시들 — 을 찾습니다.
*   최종 클래스는 이 가장 가까운 이웃들 사이의 **투표**로 결정됩니다.

**(이미지: 1-최근접 이웃의 결정 경계(Decision Boundaries)를 시각화한 그림)**

***

### **페이지 233**

**k-최근접 이웃 (k-NN)**

*   **k-최근접 이웃** 방법이 어떻게 작동하는지 시각화해 봅시다.
*   우리는 **목표 지점**(분류할 이미지)을 가지고 있습니다.
*   특징 공간에서 **가장 가까운 세 개의 점** — 가장 유사한 예시들 — 을 찾습니다.
*   최종 클래스는 이 가장 가까운 이웃들 사이의 **투표**로 결정됩니다.

**(이미지: k=1, k=3, k=5일 때의 결정 경계가 어떻게 달라지는지를 비교하는 그림)**

***

### **페이지 234**

**k-최근접 이웃(k-NN)의 하이퍼파라미터**

*   **하이퍼파라미터란 무엇인가?**
    *   하이퍼파라미터는 데이터로부터 학습되는 것이 아니라 **알고리즘 자체에 대한 설정이나 선택**입니다.
    *   알고리즘이 **무엇을** 배우는지보다 **어떻게** 작동하는지를 제어합니다.
    *   **k (이웃의 수):**
        *   레이블 투표에 몇 개의 가장 가까운 샘플을 사용해야 하는가?
    *   **거리 측정법:**
        *   샘플 간의 "근접성"을 어떻게 측정할 것인가? (예: 유클리드, 맨해튼, 코사인 거리 등)

**(이미지: 하이퍼파라미터 선택의 중요성을 설명하는 텍스트. "최고의 k 값은 무엇인가?", "최고의 거리는 무엇인가?" 등의 질문을 던짐)**

***

### **페이지 235**

**하이퍼파라미터 설정 방법**

*   실제로 하이퍼파라미터를 어떻게 설정하는지 봅시다.
    *   하이퍼파라미터를 변경하면서 모델 성능이 어떻게 변하는지 확인하고, 어떤 설정이 가장 잘 작동하는지 결정하고 싶습니다.
*   **훈련 → 검증 → 테스트** 순서를 사용합니다: 검증 세트에서 하이퍼파라미터를 조정하고, 테스트 데이터에서 최종 성능을 보고합니다.

**(이미지: 하이퍼파라미터 설정의 세 가지 아이디어와 그 문제점을 설명하는 그림. 훈련 데이터에만 맞추는 것(나쁨), 테스트 데이터에 맞추는 것(나쁨), 데이터를 훈련/검증/테스트로 분할하는 것(더 좋음)을 비교)**

***

### **페이지 236**

**k-NN의 한계 — 데이터가 충분하지 않을 때**

*   k-NN은 각 클래스에 대해 **충분한 데이터 포인트**가 있을 때 잘 작동합니다.
*   첫 번째 예시에서는, "?"의 레이블을 예측하는 것이 매우 **명확**합니다. 투표할 근처 샘플이 많기 때문입니다.
*   하지만, 마지막 예시처럼 각 클래스가 **하나 또는 몇 개의 샘플**만 가지고 있는 경우, 새로운 점에 대한 결정은 매우 **모호**해집니다.

**(이미지: 데이터가 충분할 때와 부족할 때의 k-NN 분류의 명확성 차이를 보여주는 그림)**

***

### **페이지 237**

**k-NN의 한계 — 데이터가 충분하지 않을 때**

*   k-NN은 각 클래스에 대해 **충분한 데이터 포인트**가 있을 때 잘 작동합니다.
*   첫 번째 예시에서는, "?"의 레이블을 예측하는 것이 매우 **명확**합니다. 투표할 근처 샘플이 많기 때문입니다.
*   하지만, 마지막 예시처럼 각 클래스가 **하나 또는 몇 개의 샘플**만 가지고 있는 경우, 새로운 점에 대한 결정은 매우 **모호**해집니다.

**(이미지: 데이터가 충분할 때와 부족할 때의 k-NN 분류의 명확성 차이를 보여주는 그림)**

***

### **페이지 238**

**k-NN의 한계 — 데이터가 충분하지 않을 때**

*   k-NN은 각 클래스에 대해 **충분한 데이터 포인트**가 있을 때 잘 작동합니다.
*   첫 번째 예시에서는, "?"의 레이블을 예측하는 것이 매우 **명확**합니다. 투표할 근처 샘플이 많기 때문입니다.
*   하지만, 마지막 예시처럼 각 클래스가 **하나 또는 몇 개의 샘플**만 가지고 있는 경우, 새로운 점에 대한 결정은 매우 **모호**해집니다.

**(이미지: 데이터가 극도로 부족하여 분류가 거의 불가능한 상황을 보여주는 그림)**

***

### **페이지 239**

**k-NN의 한계 — 계산 비용**

*   k-NN은 **계산 비용이 많이 듭니다**.
    *   새로운 샘플을 데이터셋의 **모든 데이터 포인트**와 비교해야 합니다.
*   **샘플의 수가 증가**함에 따라, 계산 비용은 데이터셋 크기에 따라 **선형적으로 (또는 그 이상으로)** 증가합니다.
*   따라서, k-NN은 대규모 이미지 분류에는 **비실용적**이 됩니다.

**(이미지: k-NN의 계산 복잡성을 보여주는 그림. 하나의 이미지를 수많은 다른 이미지와 비교해야 함)**

***

### **페이지 240**

**선형 분류기**

**(이미지: 데이터 과학을 상징하는 그래픽)**

***

### **페이지 241**

**k-NN에서 선형 분류기로**

*   지금까지 우리는 k-NN이 저장된 예시와 비교하여 이미지를 분류할 수 있다는 것을 보았습니다.
*   하지만, 두 가지 주요 문제가 있습니다:
    *   안정적으로 작동하려면 **많은 레이블된 샘플**이 필요합니다.
    *   모든 새로운 이미지를 기존의 모든 샘플과 비교해야 하므로 **계산 비용이 많이 듭니다**.
*   그래서, 모든 예시를 기억하고 비교하는 대신,
    *   클래스를 직접 **분리하는 함수를 학습**할 수 있다면 어떨까요?
    *   모든 예시를 저장하지 않고도 보지 못한 데이터에 **일반화**할 수 있는 모델.
*   이것이 우리를 **선형 분류기**라는 아이디어로 이끕니다.

**(이미지: k-NN의 복잡한 결정 경계와 선형 분류기의 단순한 결정 경계(선 하나)를 비교하는 그림)**

***

### **페이지 242**

**k-NN에서 선형 분류기로**

*   이것이 우리를 **선형 분류기**라는 아이디어로 이끕니다.

**(이미지: 특징 공간에서 비행기, 자동차, 사슴 클래스를 각각의 선형 결정 경계로 분리하는 예시)**

***

### **페이지 243**

**매칭에서 학습된 분류기로**

*   이미지의 특징 벡터를 x라고 합시다 (기술자로부터 구축됨).
*   특징을 클래스 **점수/확률**로 매핑하는 함수 f가 존재한다고 가정합시다:
    *   f(x) → 점수 (클래스 레이블 y) 또는 p(y|x)

**(이미지: k-NN의 다수결 투표 방식과, 함수 f를 통해 직접 클래스 점수를 계산하는 방식을 비교)**

***

### **페이지 244**

**매칭에서 학습된 분류기로**

*   이미지의 특징 벡터를 x라고 합시다 (기술자로부터 구축됨).
*   특징을 클래스 **점수/확률**로 매핑하는 함수 f가 존재한다고 가정합시다:
    *   f(x) → 점수 (클래스 레이블 y) 또는 p(y|x)

**(이미지: 이미지에서 추출된 특징 벡터 x가 함수 f(x)를 통과하여 각 클래스(고양이, 개)에 대한 점수(0.8, 0.2)를 출력하는 과정을 보여주는 그림)**

***

### **페이지 245**

**매칭에서 학습된 분류기로**

*   이미지의 특징 벡터를 x라고 합시다 (기술자로부터 구축됨).
*   특징을 클래스 **점수/확률**로 매핑하는 함수 f가 존재한다고 가정합시다:
    *   f(x) → 점수 (클래스 레이블 y) 또는 p(y|x)
*   **함수 f를 어떻게 알 수 있을까요?**

**(이미지: 이전 그림의 함수 f(x)에 물음표를 추가하여 함수를 어떻게 찾는지 질문)**

***

### **페이지 246**

**함수 f를 어떻게 알 수 있는가?**

*   우리는 f를 미리 알지 못합니다 — 대신, **데이터로부터 학습**해야 합니다.
*   많은 레이블된 예시 (xᵢ, yᵢ)가 주어지면,
*   f의 예측 f(xᵢ)가 실제 레이블 yᵢ와 일치하도록 f의 **파라미터를 조정**합니다.

**(이미지: 여러 레이블된 예시 데이터(고양이, 개 사진)를 사용하여 함수 f(x)를 학습시켜, 입력에 대한 클래스 점수를 예측하게 만드는 과정을 보여줌)**

***

### **페이지 247**

**함수 f를 어떻게 알 수 있는가?**

*   우리는 f를 미리 알지 못합니다 — 대신, **데이터로부터 학습**해야 합니다.
*   많은 레이블된 예시 (xᵢ, yᵢ)가 주어지면,
*   f의 예측 f(xᵢ)가 실제 레이블 yᵢ와 일치하도록 f의 **파라미터를 조정**합니다.

**(이미지: 고양이 이미지(x₁)를 입력했을 때, 예측값(ŷ₁)이 실제 레이블(y₁)과 거의 일치하도록(고양이 점수 0.9) 함수 f(x)가 작동하는 모습을 보여줌)**

***

### **페이지 248**

**함수 f를 어떻게 알 수 있는가?**

*   우리는 f를 미리 알지 못합니다 — 대신, **데이터로부터 학습**해야 합니다.
*   많은 레이블된 예시 (xᵢ, yᵢ)가 주어지면,
*   f의 예측 f(xᵢ)가 실제 레이블 yᵢ와 일치하도록 f의 **파라미터를 조정**합니다.

**(이미지: 개 이미지(x₂)를 입력했을 때, 예측값(ŷ₂)이 실제 레이블(y₂)과 거의 일치하도록(개 점수 0.8) 함수 f(x)가 작동하는 모습을 보여줌)**

***

### **페이지 249**

**함수 f를 어떻게 알 수 있는가?**

*   우리는 f를 미리 알지 못합니다 — 대신, **데이터로부터 학습**해야 합니다.
*   많은 레이블된 예시 (xᵢ, yᵢ)가 주어지면,
*   f의 예측 f(xᵢ)가 실제 레이블 yᵢ와 일치하도록 f의 **파라미터를 조정**합니다.

**(이미지: 레이블된 이미지들에서 특징 벡터(X)를 추출하고, 이를 함수 f(x)에 입력하여 클래스 레이블(y)에 대한 점수를 예측하는 학습 과정을 보여줌)**

***

### **페이지 250**

**함수 f를 어떻게 알 수 있는가?**

*   우리는 f를 미리 알지 못합니다 — 대신, **데이터로부터 학습**해야 합니다.
*   많은 레이블된 예시 (xᵢ, yᵢ)가 주어지면,
*   f의 예측 f(xᵢ)가 실제 레이블 yᵢ와 일치하도록 f의 **파라미터를 조정**합니다.

**(이미지: 초기 함수 f(x)가 고양이 이미지(x₁)에 대해 잘못된 예측(개 점수 0.7)을 하는 모습을 보여줌. 실제 레이블(y)과 예측(ŷ)이 일치하지 않음)**

***

### **페이지 251**

**함수 f를 어떻게 알 수 있는가?**

*   우리는 f를 미리 알지 못합니다 — 대신, **데이터로부터 학습**해야 합니다.
*   많은 레이블된 예시 (xᵢ, yᵢ)가 주어지면,
*   f의 예측 f(xᵢ)가 실제 레이블 yᵢ와 일치하도록 f의 **파라미터를 조정**합니다.

**(이미지: 함수 f를 '조정(Adjust)'하여 고양이 이미지(x₁)에 대한 예측이 올바르게(고양이 점수 0.51) 되도록 만드는 과정을 보여줌)**

***

### **페이지 252**

**함수 f를 어떻게 알 수 있는가?**

*   우리는 f를 미리 알지 못합니다 — 대신, **데이터로부터 학습**해야 합니다.
*   많은 레이블된 예시 (xᵢ, yᵢ)가 주어지면,
*   f의 예측 f(xᵢ)가 실제 레이블 yᵢ와 일치하도록 f의 **파라미터를 조정**합니다.

**(이미지: 초기 함수 f(x)가 개 이미지(x₂)에 대해 잘못된 예측(고양이 점수 0.7)을 하는 모습을 보여줌)**

***

### **페이지 253**

**함수 f를 어떻게 알 수 있는가?**

*   우리는 f를 미리 알지 못합니다 — 대신, **데이터로부터 학습**해야 합니다.
*   많은 레이블된 예시 (xᵢ, yᵢ)가 주어지면,
*   f의 예측 f(xᵢ)가 실제 레이블 yᵢ와 일치하도록 f의 **파라미터를 조정**합니다.

**(이미지: 함수 f를 '조정(Adjust)'하여 개 이미지(x₂)에 대한 예측이 올바르게(개 점수 0.55) 되도록 만드는 과정을 보여줌)**

***

### **페이지 254**

**f(x) 학습하기 — 로지스틱 회귀로 시작하기**

*   각 이미지에 대해, 우리는 **특징 벡터 x**를 가지고 있습니다.
    *   우리는 y = f(x)가 되는 함수 f를 찾고 싶습니다.
*   이를 모델링하는 가장 간단한 방법은 **로지스틱 회귀**를 사용하는 것입니다.
    *   로지스틱 회귀는 입력 특징들을 가중치 w와 선형적으로 결합합니다:
        *   s = wᵀx + b
    *   그런 다음 **시그모이드** 또는 **소프트맥스** 함수를 적용하여 클래스 확률을 생성합니다.
    *   이것은 단일 결정 경계로 클래스를 분리하는 **선형 분류기** 역할을 합니다.

**(이미지: 특징 벡터 x가 함수 f(x)를 통해 클래스 점수를 예측하는 과정을 보여주는 그림)**

***

### **페이지 255**

**복습. 로지스틱 회귀란 무엇인가?**

*   **로지스틱 회귀**는 **이진 분류**에 사용되는 통계 모델입니다 — 목표가 두 가지 가능한 결과 중 하나를 예측하는 문제, 예를 들면:
    *   합격/불합격, 스팸/스팸 아님, 질병/질병 없음, 1/0
    *   하지만 클래스 레이블을 직접 예측하는 대신, 로지스틱 회귀는 인스턴스가 **클래스 1에 속할 확률**을 예측합니다.
*   먼저 입력 특징들의 **선형 결합**을 계산합니다:
    *   z = w₁x₁ + w₂x₂ + ... + b
*   그런 다음 이 값을 **시그모이드 함수**를 통과시킵니다:
    *   σ(z) = 1 / (1 + e⁻ᶻ)
*   출력은 항상 0과 1 사이이며, 다음과 같이 해석됩니다:
    *   P(y = 1|x)

**(이미지 및 수식: 선형 모델과 로지스틱 모델(시그모이드 함수)의 차이를 보여주는 그래프와 관련 수식)**

***

### **페이지 256**

**f(x) 학습하기 — 로지스틱 회귀로 시작하기**

*   각 이미지에 대해, 우리는 **특징 벡터 x**를 가지고 있습니다.
    *   우리는 y = f(x)가 되는 함수 f를 찾고 싶습니다.
*   이를 모델링하는 가장 간단한 방법은 **로지스틱 회귀**를 사용하는 것입니다.
    *   로지스틱 회귀는 입력 특징들을 가중치 w와 선형적으로 결합합니다:
        *   z = wᵀx + b
    *   그런 다음 **시그모이드** 또는 **소프트맥스** 함수를 적용하여 클래스 확률을 생성합니다.
    *   이것은 단일 결정 경계로 클래스를 분리하는 **선형 분류기** 역할을 합니다.

**(이미지: 특징 벡터 x가 선형 결합(wᵀx + b)과 시그모이드/소프트맥스 함수를 거쳐 최종 클래스 확률(고양이 0.6, 개 0.4)을 출력하는 과정을 보여주는 다이어그램)**

***

### **페이지 257**

**로지스틱 회귀를 통한 f(x) 학습**

*   우리는 로지스틱 회귀 모델의 파라미터 **W**와 **b**를 학습시킵니다.
    *   예측된 출력 ŷᵢ = f(xᵢ)가 실제 레이블 yᵢ와 일치하도록.
*   다시 말해, 예측 오차를 최소화하기 위해 **W**와 **b**를 최적화함으로써,
    *   이미지 특징 X를 클래스 레이블 y로 매핑하는 함수 **f**를 학습할 수 있습니다.

**(이미지: 여러 레이블된 예시 데이터를 사용하여, 예측(ŷᵢ)과 실제 레이블(y) 간의 교차 엔트로피 손실을 최소화하도록 파라미터(W, b)를 학습하는 과정을 보여주는 다이어그램)**

***

### **페이지 258**

**한계 — 고정된 특징 추출기**

*   지금까지 우리는 **고정된 특징 추출기**를 사용했습니다.
    *   우리는 엣지를 탐지하고, 키포인트(코너)를 찾았으며, 그들의 기술자를 추출했습니다.
*   하지만 이러한 **수작업으로 만든 특징**들이 복잡한 실제 이미지를 분류하는 데 정말로 **충분하고 의미가 있을까요?**
    *   이 고정된 패턴들이 각 클래스의 **본질적인 특성**을 포착할까요?

**(이미지: 이미지에서 고정된 특징 추출(엣지, 코너, 기술자)을 거쳐 분류기로 들어가는 과정을 보여주며, 특징 추출 단계가 고정되어 있음을 강조)**

***

### **페이지 259**

**한계 — 고정된 특징 추출기**

*   지금까지 우리는 **고정된 특징 추출기**를 사용했습니다.
    *   우리는 엣지를 탐지하고, 키포인트(코너)를 찾았으며, 그들의 기술자를 추출했습니다.
*   하지만 이러한 **수작업으로 만든 특징**들이 복잡한 실제 이미지를 분류하는 데 정말로 **충분하고 의미가 있을까요?**
    *   이 고정된 패턴들이 각 클래스의 **본질적인 특성**을 포착할까요?

**(이미지: 고정된 특징 추출 단계에 큰 물음표를 표시하여 그 유효성에 의문을 제기)**

***

### **페이지 260**

**수작업 특징의 정보 손실**

*   예를 들어, 우리의 현재 특징들은 **색상**과 같은 중요한 정보를 잃습니다.
    *   **HOG**와 같은 기술자에서는 **엣지 방향** 정보만 유지되고 — 색상이나 질감 세부 정보는 **버려집니다**.
    *   결과적으로, 특징들은 한 클래스를 다른 클래스와 진정으로 구별하는 것을 포착하지 못할 수 있습니다.
        *   **고양이**와 **개**는 색상과 질감이 매우 다름에도 불구하고 유사한 엣지 패턴을 가질 수 있습니다.

**(이미지: 원본 개구리 이미지와 HOG 특징을 시각화한 이미지를 비교하며, 색상 정보가 손실됨을 보여줌)**

***

### **페이지 261**

**보완적인 특징 — 색상 히스토그램**

*   색상 정보의 손실을 보상하기 위해, 연구자들은 **색상 히스토그램**을 추가적인 특징으로 사용해왔습니다.
*   색상 히스토그램은 이미지의 **색상 분포**를 나타내며, 각 색상 범위(빈)에 얼마나 많은 픽셀이 속하는지를 셉니다.
*   이러한 히스토그램은 분류를 위한 **특징 벡터**로 사용될 수 있습니다.

**(이미지: 개구리 이미지의 특정 픽셀 색상이 색상 히스토그램의 해당 빈에 +1로 기여하는 과정을 보여주는 그림)**

***

### **페이지 262**

**여러 수작업 특징 결합하기**

*   이러한 방식으로, 연구자들은 분류에 유용해 보이는 다양한 **수작업 특징**을 설계하고 결합했습니다.
*   예를 들어, **HOG, SIFT, 색상 히스토그램**과 같은 특징들은 종종 단일 특징 벡터로 **연결(concatenated)**되었습니다.
*   희망은 다른 유형의 특징을 결합하면 이미지에 대한 더 완전한 표현을 제공할 것이라는 것이었습니다.

**(이미지: 개구리 이미지에서 여러 종류의 특징(히스토그램)을 추출하고 이를 하나로 결합하는 과정을 보여주는 그림)**

***

### **페이지 263**

**색상 히스토그램의 한계 — 공간 정보 없음**

*   색상 히스토그램은 이미지의 **전체적인 색상 분포**를 보여줍니다.
*   하지만, **공간 정보는 보존하지 않습니다** — 특정 색상이 어디에 나타나는지 알 수 없습니다.
    *   예를 들어, 이미지에 **빨간색**이 존재한다는 것은 알 수 있지만, 그것이 **눈**, **배경**, 또는 **객체**에 나타나는지는 알 수 없습니다.
    *   결과적으로, 색상 히스토그램은 유사한 색상 분포를 가졌지만 **다른 공간적 배열**을 가진 이미지를 구별할 수 없습니다.

**(이미지: 이전 페이지의 색상 히스토그램 그림을 다시 보여주며, 공간 정보가 없다는 한계를 지적)**

***

### **페이지 264**

**더 나은 특징을 향한 끝없는 탐색**

*   그렇다면 다음에 무엇을 해야 할까요?
    *   각 색상이 이미지의 어디에 나타나는지를 포착하는 또 다른 특징을 설계해야 할까요?
*   만약 우리가 모든 누락된 측면에 대해 계속해서 새로운 특징을 추가한다면, 이 과정은 **결코 끝나지 않을 것**입니다.
*   특징을 하나씩 수작업으로 만드는 것은 **확장 가능하지도 않고 일반화 가능하지도 않습니다**.

**(이미지: 이전 그림에 물음표를 추가하여, 다음 단계에 대한 막막함을 표현)**

***

### **페이지 265**

**수작업 특징은 항상 신뢰할 수 있는가?**

*   더욱이, **엣지 탐지**나 **코너 탐지** 결과가 항상 완벽하다고 말할 수 있을까요?
    *   이 알고리즘들이 다른 조명, 스케일, 또는 노이즈 조건 하에서 모든 **실제 엣지와 코너**를 일관되게 명확하게 찾을 수 있을까요?
    *   실제로는, 이러한 탐지는 종종 **노이즈가 많고, 불완전하며, 작은 변화에 민감**합니다.
*   따라서, 추출된 특징 자체도 완전히 신뢰할 수 없을 수 있습니다.

**(이미지: 다양한 이미지에 대한 엣지 및 코너 탐지 결과가 불완전하거나 노이즈에 민감한 여러 예시)**

***

### **페이지 266**

**도전 과제: 조명**

*   **수작업 특징 추출기**가 **다양한 조명 조건** 하에서 의미 있는 패턴을 일관되게 포착하는 것은 극도로 어렵습니다.

**(이미지: 다양한 조명 조건(어두움, 밝음, 역광 등) 하에 있는 네 마리의 고양이 사진)**

***

### **페이지 267**

**도전 과제: 배경 혼잡**

*   **배경 혼잡** 또한 상당한 도전 과제입니다. 배경이 **복잡한 질감이나 패턴**을 포함할 때, **수작업 특징 추출기**가 **실제 관심 객체**에 집중하기 어려워집니다.

**(이미지: 복잡한 배경(낙엽, 눈) 속에 있는 두 마리의 고양이 사진)**

***

### **페이지 268**

**도전 과제: 가려짐 (Occlusion)**

*   또 다른 주요 도전 과제는 **가려짐**입니다. 관심 객체가 **부분적으로 숨겨지거나 덮여** 있을 때, **수작업 특징**이 객체의 전체 구조를 포착하기가 극도로 어려워집니다.

**(이미지: 담요, 풀, 소파 쿠션에 의해 부분적으로 가려진 세 마리의 고양이 사진)**

***

### **페이지 269**

**도전 과제: 변형 (Deformation)**

*   또 다른 주요 도전 과제는 **변형**입니다. 동일한 클래스의 객체라도 **매우 다른 모양, 자세, 구성**으로 나타날 수 있습니다. 이러한 변화에도 일관성을 유지하는 수작업 특징을 설계하는 것은 극도로 어렵습니다.

**(이미지: 매우 다른 자세(앉아있기, 누워있기, 기지개 켜기 등)를 취하고 있는 네 마리의 고양이 사진. "이들은 모두 고양이입니다 — 단지... 매우 다른 기분과 자세로 있을 뿐입니다."라는 설명 포함)**

***

### **페이지 270**

**도전 과제: 클래스 내 변이 (Intra-class Variation)**

*   또 다른 도전 과제는 **클래스 내 변이**입니다. 동일한 클래스 내에서도 객체들은 색상, 질감, 모양, 또는 크기 면에서 **매우 다르게** 보일 수 있습니다.
*   이 모든 변이를 포착하는 수작업 특징을 설계하는 것은 극도로 어렵고 종종 비현실적입니다.

**(이미지: 색상과 무늬가 모두 다른 다섯 마리의 새끼 고양이 사진)**

***

### **페이지 271**

**도전 과제: 맥락 (Context)**

*   **맥락**에 따라, 이전에 추출된 특징이 **오해를 불러일으키거나 부정확**해질 수 있습니다.
*   지역적 패턴은 **호랑이 줄무늬**처럼 보일 수 있지만, 전체 맥락에서 보면 그저 **그늘에 누워있는 개**일 뿐이라는 것을 깨닫게 됩니다.

**(이미지: 부분적으로만 보일 때는 호랑이처럼 보이지만, 전체를 보면 개임을 알 수 있는 사진. "지역적 질감만 보면 정말 호랑이처럼 보이지만 — 맥락은 우리에게 다른 것을 알려줍니다."라는 설명 포함)**

***

### **페이지 272**

**수작업 특징의 근본적인 한계**

*   따라서, 특징을 **수동으로 정의, 설계, 엔지니어링**하는 접근 방식에는 **근본적인 한계**가 있습니다.
*   수십 년의 노력에도 불구하고, 이러한 문제들을 일관되게 극복하는 획기적인 방법은 발견되지 않았습니다.
*   대략 **2011년**까지, 이러한 수작업 특징 접근법을 넘어서는 **중요한 진전**이 없었습니다.

**(이미지: 수작업 특징 추출 과정에 사람이 개입(도메인 전문가)해야 하는 한계를 보여주는 그림)**

***

### **페이지 273**

**패러다임 전환 — 특징을 자동으로 학습하기**

*   여기서 **패러다임 전환**이 일어납니다.
    *   만약 모델이 **특징 추출 과정 자체**를 포함하여 **전체 함수 F**를 **학습**하게 할 수 있다면 어떨까요?
*   어떤 특징이 "최고" 또는 "최적"인지 수동으로 정의하는 대신, 모델이 데이터로부터 **유용한 특징을 자동으로 발견**하게 합니다.
*   다시 말해, 우리는 **수작업 특징**에서 **학습된 표현**으로 이동합니다.

**(이미지: 고정된 특징 추출 대신, 학습 가능한(Learned) 특징 추출 과정을 보여주는 그림. 이 단계에 느낌표가 표시되어 패러다임 전환을 강조)**

***

### **페이지 274**

**딥러닝 시대의 시작 — AlexNet (2012)**

*   이 패러다임 전환은 2012년에 소개된 **AlexNet**으로 처음 실현되었습니다.
*   **특징 추출과 분류를 모두 함께**, **종단간(end-to-end)** 방식으로 학습한 최초의 모델이었습니다.
*   그 결과는 이미지 분류 성능에서 **획기적인 향상**이었으며 — 이전의 모든 수작업 특징 방법을 훨씬 능가했습니다.
*   이는 컴퓨터 비전에서 **현대 딥러닝 시대의 시작**을 알렸습니다.

**(이미지: AlexNet의 구조를 보여주는 다이어그램. 특징 추출과 분류기가 모두 학습 가능(Learned)하다고 표시됨)**

***

### **페이지 275**

**딥러닝 시대의 시작 — AlexNet (2012)**

*   이 패러다임 전환은 2012년에 소개된 **AlexNet**으로 처음 실현되었습니다.
*   **특징 추출과 분류를 모두 함께**, **종단간(end-to-end)** 방식으로 학습한 최초의 모델이었습니다.
*   그 결과는 이미지 분류 성능에서 **획기적인 향상**이었으며 — 이전의 모든 수작업 특징 방법을 훨씬 능가했습니다.
*   이는 컴퓨터 비전에서 **현대 딥러닝 시대의 시작**을 알렸습니다.

**(이미지: 2010년부터 2017년까지 이미지 분류 오류율의 변화를 보여주는 그래프. AlexNet(2012) 이후 오류율이 급격히 감소함을 보여줌. AlexNet 개발팀 사진도 함께 표시됨)**

***

### **페이지 276**

**AlexNet의 부상을 가능하게 한 것은 무엇인가?**

*   AlexNet의 성공은 우연이 아니었습니다 — 몇 가지 핵심적인 발전 덕분에 가능했습니다:
    *   **인터넷 혁명** → 온라인 이미지에 대한 대규모 접근.
    *   대규모 **데이터셋**의 생성, 특히 1,000개 이상의 카테고리에 걸쳐 백만 개 이상의 레이블된 이미지를 포함하는 **ImageNet**.
    *   딥 뉴럴 네트워크 훈련을 계산적으로 가능하게 만든 강력한 **GPU**의 출현.
*   이러한 요인들이 함께 작용하여 마침내 딥러닝이 **확장되고 성공**할 수 있게 했습니다.

**(이미지: ImageNet 데이터셋을 시각화한 이미지와 여러 개의 GPU가 장착된 서버 랙 이미지)**

***

### **페이지 277**

**특징 추출 없는 선형 분류기**

*   먼저 별도의 특징 추출기 없이 **종단간 선형 분류기**를 고려해 봅시다.
*   이 모델에서는 명시적인 특징 추출 단계 없이 **원시 이미지 픽셀**로부터 직접 학습하여, 이를 바로 **클래스 점수**로 매핑합니다.

**(이미지: 입력 이미지(원시 픽셀)가 학습된 선형 분류기(f)를 직접 통과하여 클래스 점수를 출력하는 다이어그램)**

***

### **페이지 278**

**입력으로서의 원시 이미지 — 펼쳐진 특징 벡터**

*   이제 별도의 특징 추출기가 없으므로, **원시 이미지 자체**가 입력으로 사용됩니다.
*   이미지는 단순히 **숫자의 배열**입니다. 예를 들어 컬러 이미지는 **32×32×3**입니다.
*   이 배열을 **펼치면(flatten)**, **3,072개의 숫자로 이루어진 1D 벡터**가 되며, 이는 우리가 이전에 사용했던 **특징 벡터**와 똑같이 취급할 수 있습니다.

**(이미지: 고양이 이미지가 32x32x3 숫자 배열로 표현되고, 이를 펼쳐서 3072차원의 1D 벡터로 만드는 과정을 보여주는 그림)**

***

### **페이지 279**

**입력으로서의 원시 이미지 — 펼쳐진 특징 벡터**

*   이제 별도의 특징 추출기가 없으므로, **원시 이미지 자체**가 입력으로 사용됩니다.
*   이미지는 단순히 **숫자의 배열**입니다. 예를 들어 컬러 이미지는 **32×32×3**입니다.
*   이 배열을 **펼치면(flatten)**, **3,072개의 숫자로 이루어진 1D 벡터**가 되며, 이는 우리가 이전에 사용했던 **특징 벡터**와 똑같이 취급할 수 있습니다.

**(이미지: 이미지를 1D 벡터(x)로 변환한 후, 이를 선형 분류기 f(x)에 입력하여 클래스 점수를 얻는 과정을 보여주는 다이어그램)**

***

### **페이지 280**

**로지스틱 회귀와 동일한 설정**

*   이 경우, 상황은 우리가 이전에 논의했던 **로지스틱 회귀 문제**와 동일합니다.
*   입력 이미지 x와 분류기 f가 주어지면, 모델은 입력의 **선형 결합**을 계산하고, 이를 **시그모이드**(이진 분류용) 또는 **소프트맥스**(다중 클래스 분류용) 함수를 통과시켜 최종 **클래스 점수 또는 확률**을 생성합니다.

**(이미지: 입력 벡터 x가 가중치 행렬 W와 곱해지고 편향 b가 더해진 후, 시그모이드 함수를 통과하여 최종 점수를 출력하는 과정을 보여주는 다이어그램)**

***

### **페이지 281**

**로지스틱 회귀와 동일한 설정**

*   이 경우, 상황은 우리가 이전에 논의했던 **로지스틱 회귀 문제**와 동일합니다.
*   입력 이미지 x와 분류기 f가 주어지면, 모델은 입력의 **선형 결합**을 계산하고, 이를 **시그모이드**(이진 분류용) 또는 **소프트맥스**(다중 클래스 분류용) 함수를 통과시켜 최종 **클래스 점수 또는 확률**을 생성합니다.

**(이미지: 다중 클래스 분류를 위해 여러 개의 가중치 벡터(W₁, W₂, W₃)를 사용하고, 소프트맥스 함수를 통과하여 각 클래스(고양이, 개, 배)에 대한 확률을 출력하는 과정을 보여주는 다이어그램)**

***

### **페이지 282**

**선형 분류기의 차원**

*   각 구성 요소의 차원을 살펴봅시다:
    *   x ∈ ℝ³⁰⁷²ˣ¹, W ∈ ℝ³ˣ³⁰⁷², b ∈ ℝ³ˣ¹
*   따라서 선형 변환의 출력은 다음과 같습니다:
    *   Wx + b ∈ ℝ³ˣ¹
*   이는 **세 개의 클래스 점수**(예: 고양이, 개, 차)를 생성합니다.
*   f는 파라미터 W에 의존하므로, 이를 명시적으로 f(x, W)로 쓸 수 있습니다.

**(이미지: 이전 페이지의 다이어그램에 각 벡터와 행렬의 차원을 명시)**

***

### **페이지 283**

**대수적 관점 — 행렬 곱셈**

*   **대수적 관점**에서, 이 연산은 단순히 가중치 행렬 **W**와 입력 벡터 **X** 사이의 **행렬 곱셈**입니다.
*   **W**의 각 행(W₁, W₂, W₃)은 모든 입력 픽셀에 대한 **가중 합**을 계산하여 클래스당 하나의 **점수**를 생성합니다.

**(이미지: 입력 이미지를 펼친 벡터 x와 가중치 행렬 W를 곱하고 편향 b를 더하여 최종 점수 벡터 z를 계산하는 과정을 행렬 연산으로 시각화한 그림)**

***

### **페이지 284**

**점수(로짓)에서 확률로 — 소프트맥스 함수**

*   우리는 **소프트맥스 함수**를 사용하여 **점수(로짓)**를 **확률**로 변환합니다.
*   각 로짓 zᵢ는 지수화된 다음, 모든 출력 확률의 합이 1이 되도록 **정규화**됩니다.
*   이러한 방식으로, 모델은 모든 가능한 클래스에 대한 **확률 분포**를 출력합니다.

**(이미지: 정규화되지 않은 점수(로짓)가 지수 함수(exp)와 정규화를 거쳐 합이 1인 확률로 변환되는 과정을 보여주는 그림)**

***

### **페이지 285**

**오차 측정 — 교차 엔트로피 손실**

*   이제, 고양이에 대한 예측 확률이 **0.13**에 불과하고, 심지어 자동차보다도 낮다고 가정해 봅시다 — 분명히 모델이 실수를 하고 있습니다.
    *   이 오차를 어떻게 **측정**할까요?
*   우리는 **교차 엔트로피 손실**을 사용합니다. 이는 예측된 확률 분포가 실제 원-핫 레이블과 얼마나 다른지를 정량화합니다.
*   손실이 높을수록 더 큰 오차를 의미하며, 이 손실을 최소화하면 모델이 **정확한 클래스**에 더 높은 확률을 할당하도록 유도합니다.

**(이미지: 예측된 확률(0.13)이 잘못되었음을 물음표로 표시)**

***

### **페이지 286**

**오차 측정 — 교차 엔트로피 손실**

*   이제, 고양이에 대한 예측 확률이 **0.13**에 불과하고, 심지어 자동차보다도 낮다고 가정해 봅시다 — 분명히 모델이 실수를 하고 있습니다.
    *   이 오차를 어떻게 **측정**할까요?
*   우리는 **교차 엔트로피 손실**을 사용합니다. 이는 예측된 확률 분포가 실제 원-핫 레이블과 얼마나 다른지를 정량화합니다.
*   손실이 높을수록 더 큰 오차를 의미하며, 이 손실을 최소화하면 모델이 **정확한 클래스**에 더 높은 확률을 할당하도록 유도합니다.

**(이미지: 예측된 확률 분포와 실제 정답(원-핫 인코딩)을 비교하는 과정을 보여줌)**

***

### **페이지 287**

**오차 측정 — 교차 엔트로피 손실**

*   이제, 고양이에 대한 예측 확률이 **0.13**에 불과하고, 심지어 자동차보다도 낮다고 가정해 봅시다 — 분명히 모델이 실수를 하고 있습니다.
    *   이 오차를 어떻게 **측정**할까요?
*   우리는 **교차 엔트로피 손실**을 사용합니다. 이는 예측된 확률 분포가 실제 원-핫 레이블과 얼마나 다른지를 정량화합니다.
*   손실이 높을수록 더 큰 오차를 의미하며, 이 손실을 최소화하면 모델이 **정확한 클래스**에 더 높은 확률을 할당하도록 유도합니다.

**(이미지: 정답 클래스(고양이)의 예측 확률(0.13)에 대해 -log를 취하여 손실 값(2.04)을 계산하는 과정을 보여줌)**

***

### **페이지 288**

**교차 엔트로피의 정보 이론적 관점**

*   **정보 이론** 관점에서, 실제 분포 P와 모델 분포 Q 사이의 교차 엔트로피는 다음과 같이 정의됩니다:
    *   H(P, Q) = H(P) + D_KL(P||Q)
    *   여기서 H(P)는 실제 레이블의 **내재적 불확실성**을 나타냅니다.
*   하지만, 분류 문제에서 P는 **원-핫**입니다 — 각 샘플은 이미 단일의 알려진 카테고리에 속합니다. → 따라서, H(P) = 0입니다.
*   따라서, **교차 엔트로피를 최소화**하는 것은 **KL 발산을 최소화**하는 것과 동일하며, 이는 모델 분포 Q를 실제 분포 P에 더 가깝게 만드는 것을 의미합니다.

**(이미지: 이전 그림에 교차 엔트로피와 KL 발산의 관계를 설명하는 수식을 추가)**

***

### **페이지 289**

**오차 측정 — 교차 엔트로피 손실**

*   **정보 이론** 관점에서, 실제 분포 P와 모델 분포 Q 사이의 교차 엔트로피는 다음과 같이 정의됩니다:
    *   H(P, Q) = H(P) + D_KL(P||Q)
    *   여기서 H(P)는 실제 레이블의 **내재적 불확실성**을 나타냅니다.
*   하지만, 분류 문제에서 P는 **원-핫**입니다 — 각 샘플은 이미 단일의 알려진 카테고리에 속합니다. → 따라서, H(P) = 0입니다.
*   따라서, **교차 엔트로피를 최소화**하는 것은 **KL 발산을 최소화**하는 것과 동일하며, 이는 모델 분포 Q를 실제 분포 P에 더 가깝게 만드는 것을 의미합니다.

**(이미지: 예측 확률과 실제 확률 간의 차이를 쿨백-라이블러 발산(Kullback-Leibler divergence)으로 설명하는 그림)**

***

### **페이지 290**

**교차 엔트로피 손실에 대해 생각해보기**

*   손실의 최소값은 얼마일까요?
*   최대값은 얼마일까요?
*   만약 모든 클래스가 동일한 확률로 예측된다면 어떻게 될까요?
    *   → C개의 클래스에 대해,

**(이미지: 이전 페이지의 그림을 반복하여 보여줌)**

***

### **페이지 291**

**교차 엔트로피 손실의 범위와 해석**

*   손실 L의 **최소값**은 **0**입니다.
    *   이는 모델이 **정확한 클래스**를 확률 **1**로 예측할 때 발생합니다.
    *   L = -log(1) = 0
*   **최대값**은 **무한대(∞)**입니다.
    *   모델이 정확한 클래스에 **0의 확률**을 할당하면,
    *   L = -log(0) = ∞
*   예를 들어, **세 개의 클래스**가 있고 모델이 모두를 **동일하게** 예측한다면 (pᵢ = 1/3), 손실은 다음과 같습니다:
    *   L = -log(1/3) = log(3)
*   이는 모든 클래스가 동일하게 가능할 때의 **평균 불확실성**에 해당합니다.

***

### **페이지 292**

**선형 분류기의 기하학적 해석**

*   **기하학적 관점**에서, 선형 분류기는 **클래스별 선형 함수**의 집합으로 표현될 수 있습니다.
*   각 클래스 i에 대해, 방정식은 특징 공간에서 **결정 평면(초평면)**을 정의합니다.
*   이 평면에 대한 샘플의 **측면과 거리**는 그 로짓의 **부호와 크기**에 해당하며 — 모델이 해당 샘플이 그 클래스에 속한다고 얼마나 강하게 믿는지를 나타냅니다.

**(이미지: 3차원 특징 공간에서 여러 클래스(비행기, 자동차, 사슴)를 분리하는 초평면들을 시각화한 그림)**

***

### **페이지 293**

**선형 분류기의 기하학적 해석**

*   **기하학적 관점**에서, 선형 분류기는 **클래스별 선형 함수**의 집합으로 표현될 수 있습니다.
*   각 클래스 i에 대해, 방정식은 특징 공간에서 **결정 평면(초평면)**을 정의합니다.
*   이 평면에 대한 샘플의 **측면과 거리**는 그 로짓의 **부호와 크기**에 해당하며 — 모델이 해당 샘플이 그 클래스에 속한다고 얼마나 강하게 믿는지를 나타냅니다.

**(이미지: 이전 그림에 각 결정 평면의 방정식과, 샘플이 평면의 어느 쪽에 있는지에 따라 점수가 어떻게 변하는지를 보여주는 화살표를 추가)**

***

### **페이지 294**

**학습된 가중치는 어떻게 생겼을까?**

*   이제, 학습된 파라미터 W와 b는 실제로 어떻게 생겼을까요?
    *   우리는 각 이미지를 긴 벡터로 **펼치고** W를 곱한다는 것을 기억하세요.
*   입력 이미지는 원래 2D(또는 컬러 채널이 있는 3D)이고, W는 동일한 수의 요소를 가지므로, W의 각 행은 **동일한 2D 이미지 모양으로 다시 변형**될 수 있습니다.
*   따라서, 각 클래스(예: 고양이, 개, 배)는 해당 가중치 벡터 Wᵢ를 가지며, 이는 모델이 해당 클래스에서 "찾는" 것을 보여주는 **이미지 같은 패턴**으로 시각화될 수 있습니다.

**(이미지: 가중치 행렬 W의 계산 과정을 보여주는 그림)**

***

### **페이지 295**

**학습된 가중치는 어떻게 생겼을까?**

*   이제, 학습된 파라미터 W와 b는 실제로 어떻게 생겼을까요?
    *   우리는 각 이미지를 긴 벡터로 **펼치고** W를 곱한다는 것을 기억하세요.
*   입력 이미지는 원래 2D(또는 컬러 채널이 있는 3D)이고, W는 동일한 수의 요소를 가지므로, W의 각 행은 **동일한 2D 이미지 모양으로 다시 변형**될 수 있습니다.
*   따라서, 각 클래스(예: 고양이, 개, 배)는 해당 가중치 벡터 Wᵢ를 가지며, 이는 모델이 해당 클래스에서 "찾는" 것을 보여주는 **이미지 같은 패턴**으로 시각화될 수 있습니다.

**(이미지: 가중치 행렬 W의 첫 번째 행을 2D 패턴으로 재구성하는 과정을 보여주는 그림)**

***

### **페이지 296**

**학습된 가중치는 어떻게 생겼을까?**

*   이제, 학습된 파라미터 W와 b는 실제로 어떻게 생겼을까요?
    *   우리는 각 이미지를 긴 벡터로 **펼치고** W를 곱한다는 것을 기억하세요.
*   입력 이미지는 원래 2D(또는 컬러 채널이 있는 3D)이고, W는 동일한 수의 요소를 가지므로, W의 각 행은 **동일한 2D 이미지 모양으로 다시 변형**될 수 있습니다.
*   따라서, 각 클래스(예: 고양이, 개, 배)는 해당 가중치 벡터 Wᵢ를 가지며, 이는 모델이 해당 클래스에서 "찾는" 것을 보여주는 **이미지 같은 패턴**으로 시각화될 수 있습니다.

**(이미지: 가중치 행렬 W의 두 번째 행을 2D 패턴으로 재구성하는 과정을 보여주는 그림)**

***

### **페이지 297**

**선형 분류기가 "보는" 것**

*   **시각화**
    *   **비행기**: 전체적으로 파란색 영역 → 모델이 **하늘 배경 패턴**을 학습함
    *   **자동차**: 중앙의 둥근 보라색 모양 → **자동차 몸체 모양**에 집중함
    *   **개구리**: 녹색과 노란색의 혼합 → 개구리의 **질감과 색상 패턴**을 포착함
    *   **배**: 수평의 파란색 영역 → **수면 배경 패턴**을 학습함

*   **흐릿한 가중치**
    *   모델은 데이터로부터 **평균적인 패턴**만 학습하기 때문에, 대부분의 **미세한 세부 사항은 사라지고**, 흐릿한 **원형(prototypical) 패턴**만 남습니다.

**(이미지: CIFAR-10 데이터셋의 각 클래스에 대해 학습된 선형 분류기의 가중치를 시각화한 결과. 각 클래스에 대한 흐릿한 평균 이미지가 나타남)**

***

### **페이지 298**

**선형적으로 분리 가능한 예시**

*   **선형적으로 분리 가능한** 예시를 살펴봅시다.
    *   여기서 각 이미지는 두 가지 특징 — **복슬복슬함**(x축)과 **털의 밝기**(y축) — 으로 표현됩니다.
*   **판다**와 **판다가 아닌 동물**을 분리하는 **단일 선**을 그릴 수 있나요?

**(이미지: '복슬복슬함'과 '털의 밝기'를 축으로 하는 2D 공간에 여러 동물 이미지가 분포되어 있는 그래프)**

***

### **페이지 299**

**선형적으로 분리 가능한 예시**

*   **선형적으로 분리 가능한** 예시를 살펴봅시다.
    *   여기서 각 이미지는 두 가지 특징 — **복슬복슬함**(x축)과 **털의 밝기**(y축) — 으로 표현됩니다.
*   **판다**와 **판다가 아닌 동물**을 분리하는 **단일 선**을 그릴 수 있나요?
    *   **예** — 매우 간단합니다: 판다는 왼쪽 위 영역에 모여 있어 다른 동물들과 명확하게 분리됩니다.

**(이미지: 이전 그래프에 판다와 다른 동물을 분리하는 빨간색 선을 추가한 그림)**

***

### **페이지 300**

**선형적으로 분리 가능한 예시**

*   **판다**와 **판다가 아닌 동물**을 분리하는 **단일 선**을 그릴 수 있나요?
    *   **예** — 매우 간단합니다: 판다는 왼쪽 위 영역에 모여 있어 다른 동물들과 명확하게 분리됩니다.
*   동일한 아이디어가 **고양이 vs. 개**에도 적용됩니다 — 만약 그들의 특징이 뚜렷한 클러스터를 형성한다면, **선형 분류기**는 그들 사이의 경계를 쉽게 찾을 수 있습니다.

**(이미지: 이전 그래프에서 고양이와 다른 동물을 분리하는 선을 보여주는 그림)**

***

### **페이지 301**

**선형적으로 분리 가능한 예시**

*   **판다**와 **판다가 아닌 동물**을 분리하는 **단일 선**을 그릴 수 있나요?
    *   **예** — 매우 간단합니다: 판다는 왼쪽 위 영역에 모여 있어 다른 동물들과 명확하게 분리됩니다.
*   동일한 아이디어가 **고양이 vs. 개**에도 적용됩니다 — 만약 그들의 특징이 뚜렷한 클러스터를 형성한다면, **선형 분류기**는 그들 사이의 경계를 쉽게 찾을 수 있습니다.

**(이미지: 이전 그래프에서 개와 다른 동물을 분리하는 선을 보여주는 그림)**

***

### **페이지 302**

**선형 분류기의 어려운 경우**

*   일부 데이터셋은 **선형적으로 분리 가능하지 않습니다**.
*   스스로에게 물어보세요: 다음 클래스 배치에서, **단일 선(초평면)**이 클래스들을 분리할 수 있나요?
    *   **XOR 패턴** (1, 3사분면 vs. 2, 4사분면) → **아니오**
    *   **링/고리 모양** (1 ≤ ||x||₂ ≤ 2인 점들 vs. 나머지) → **아니오**
    *   **다중 모드 클러스터** (클래스 1에 대한 여러 분리된 덩어리) → **아니오**
*   선형 분류기는 이러한 분포를 분리하기 위해 하나의 경계를 그릴 수 없습니다.
    *   → 우리는 **비선형 결정 경계**(특징 맵/커널, MLP, CNN)가 필요합니다.

**(이미지: XOR 패턴을 시각화한 그림)**

***

### **페이지 303**

**선형 분류기의 어려운 경우**

*   일부 데이터셋은 **선형적으로 분리 가능하지 않습니다**.
*   스스로에게 물어보세요: 다음 클래스 배치에서, **단일 선(초평면)**이 클래스들을 분리할 수 있나요?
    *   **XOR 패턴** (1, 3사분면 vs. 2, 4사분면) → **아니오**
    *   **링/고리 모양** (1 ≤ ||x||₂ ≤ 2인 점들 vs. 나머지) → **아니오**
    *   **다중 모드 클러스터** (클래스 1에 대한 여러 분리된 덩어리) → **아니오**
*   선형 분류기는 이러한 분포를 분리하기 위해 하나의 경계를 그릴 수 없습니다.
    *   → 우리는 **비선형 결정 경계**(특징 맵/커널, MLP, CNN)가 필요합니다.

**(이미지: XOR 패턴과 링/고리 모양 패턴을 나란히 보여주는 그림)**

***

### **페이지 304**

**선형 분류기의 어려운 경우**

*   일부 데이터셋은 **선형적으로 분리 가능하지 않습니다**.
*   스스로에게 물어보세요: 다음 클래스 배치에서, **단일 선(초평면)**이 클래스들을 분리할 수 있나요?
    *   **XOR 패턴** (1, 3사분면 vs. 2, 4사분면) → **아니오**
    *   **링/고리 모양** (1 ≤ ||x||₂ ≤ 2인 점들 vs. 나머지) → **아니오**
    *   **다중 모드 클러스터** (클래스 1에 대한 여러 분리된 덩어리) → **아니오**
*   선형 분류기는 이러한 분포를 분리하기 위해 하나의 경계를 그릴 수 없습니다.
    *   → 우리는 **비선형 결정 경계**(특징 맵/커널, MLP, CNN)가 필요합니다.

**(이미지: XOR, 링, 다중 모드 클러스터 패턴을 나란히 보여주는 그림)**

***

### **페이지 305**

**다른 특징 공간에서의 결정 경계**

*   **두 개의 특징**이 있을 때, 데이터 포인트는 2D 공간에 놓이고, 결정 경계는 **선**(1차원 초평면)으로 표현됩니다.
*   하지만, **세 개의 특징**이 있을 때, 데이터 포인트는 3D 공간에 존재합니다.
    이 경우, 결정 경계는 더 이상 선이 될 수 없으며 — 두 클래스를 분리하는 **평면**이 됩니다.
*   ℝ²에서 초평면은 선입니다. ℝ³에서 초평면은 평면입니다.
    *   초평면은 결정 경계의 수학적이고 기하학적인 표현입니다.

**(이미지: 2D 공간에서의 선형 결정 경계(선)와 3D 공간에서의 선형 결정 경계(평면)를 비교하는 그림)**

***

### **페이지 306**

**비선형 데이터 처리하기**

*   데이터가 **선형적으로 분리 가능하지 않을 때**,
    우리는 입력 공간을 더 높은 차원으로 변환하는 **새로운 특징**을 만들 수 있습니다.
*   예를 들어,
    새로운 특징 z = x² + y²를 도입하면, 2D(x-y 평면)에서 분리할 수 없었던 데이터가
    이제 3D 공간(x, y, z)에서 언덕이나 그릇 같은 **곡면**을 형성합니다.
*   이 변환된 공간에서는 **초평면(선형 경계)**이 이제 클래스를 분리할 수 있습니다.

**(이미지: 2D에서 선형적으로 분리 불가능한 데이터(원형 분포)를 보여주는 그림)**

***

### **페이지 307**

**비선형 데이터 처리하기**

*   데이터가 **선형적으로 분리 가능하지 않을 때**,
    우리는 입력 공간을 더 높은 차원으로 변환하는 **새로운 특징**을 만들 수 있습니다.
*   예를 들어,
    새로운 특징 z = x² + y²를 도입하면, 2D(x-y 평면)에서 분리할 수 없었던 데이터가
    이제 3D 공간(x, y, z)에서 언덕이나 그릇 같은 **곡면**을 형성합니다.
*   이 변환된 공간에서는 **초평면(선형 경계)**이 이제 클래스를 분리할 수 있습니다.

**(이미지: 2D 데이터를 3D로 매핑하여 평면으로 분리하는 과정을 보여주는 그림)**

***

### **페이지 308**

**비선형 데이터 처리하기**

*   데이터가 **선형적으로 분리 가능하지 않을 때**,
    우리는 입력 공간을 더 높은 차원으로 변환하는 **새로운 특징**을 만들 수 있습니다.
*   예를 들어,
    새로운 특징 z = x² + y²를 도입하면, 2D(x-y 평면)에서 분리할 수 없었던 데이터가
    이제 3D 공간(x, y, z)에서 언덕이나 그릇 같은 **곡면**을 형성합니다.
*   이 변환된 공간에서는 **초평면(선형 경계)**이 이제 클래스를 분리할 수 있습니다.

이러한 접근 방식은 바로 **서포트 벡터 머신(SVM)**의 기본 아이디어입니다.

**(이미지: 2D 비선형 데이터, 3D 매핑, 그리고 최종적으로 SVM으로 분리된 결과를 보여주는 3개의 그림)**

***

### **페이지 309**

**비선형 데이터 처리하기**

*   데이터가 **선형적으로 분리 가능하지 않을 때**,
    우리는 입력 공간을 더 높은 차원으로 변환하는 **새로운 특징**을 만들 수 있습니다.
*   예를 들어,
    새로운 특징 z = x² + y²를 도입하면, 2D(x-y 평면)에서 분리할 수 없었던 데이터가
    이제 3D 공간(x, y, z)에서 언덕이나 그릇 같은 **곡면**을 형성합니다.
*   이 변환된 공간에서는 **초평면(선형 경계)**이 이제 클래스를 분리할 수 있습니다.

이러한 접근 방식은 바로 **서포트 벡터 머신(SVM)**의 기본 아이디어입니다.

**(이미지: 이전 페이지의 그림을 반복하여 보여줌)**

***

### **페이지 310**

**SVM의 한계와 신경망의 필요성**

*   데이터를 더 높은 차원으로 매핑하면 선형적으로 분리 가능하게 만들 수 있지만,
    **올바른 특징 매핑을 설계하거나 선택하는 것은 매우 어렵습니다**.
*   다항식 또는 커널 기반 매핑은 제한된 유형의 비선형성만 처리할 수 있으며
    데이터에 자동으로 적응하지 않습니다.
*   이러한 한계를 극복하기 위해, 우리는 새로운 접근법 — **신경망**을 배울 것입니다. 신경망은 데이터로부터 **비선형 특징 매핑을 자동으로 학습**할 수 있습니다.

**(이미지: 이전 페이지의 그림을 반복하여 보여줌)**

***

### **페이지 311**

**K 최근접 이웃 분류기**

**(이미지: 데이터 과학을 상징하는 그래픽)**

***

### **페이지 312**

**특징 추출기를 사용하지 않을 때 — 최근접 이웃은 어떻게 작동하는가?**

*   일반적으로, 우리는 **특징 기술자**(예: SIFT, ORB) 간의 **유사성**을 계산합니다.
    *   → 이미지 → 특징 벡터 → 거리 → 최근접 클래스
*   특징 추출기 없이는, 분류기는 **원시 픽셀 값**에 직접 의존해야 합니다.
    *   → 각 이미지 자체가 **고차원 벡터**(예: 128×128 → 16,384-D)가 됩니다.

**(이미지: 특징 기술자를 비교하는 기존 방식의 다이어그램)**

***

### **페이지 313**

**특징 추출기를 사용하지 않을 때 — 최근접 이웃은 어떻게 작동하는가?**

*   일반적으로, 우리는 **특징 기술자**(예: SIFT, ORB) 간의 **유사성**을 계산합니다.
    *   → 이미지 → 특징 벡터 → 거리 → 최근접 클래스
*   특징 추출기 없이는, 분류기는 **원시 픽셀 값**에 직접 의존해야 합니다.
    *   → 각 이미지 자체가 **고차원 벡터**(예: 128×128 → 16,384-D)가 됩니다.
*   두 이미지 간의 **거리**는 그들의 픽셀 강도를 비교하여 측정됩니다.

**(이미지: 새로운 이미지와 예시 이미지들 간의 '직접적인 이미지 유사성(거리)'을 계산하는 과정을 보여주는 그림)**

***

### **페이지 314**

**특징 추출기를 사용하지 않을 때 — 최근접 이웃은 어떻게 작동하는가?**

*   일반적으로, 우리는 **특징 기술자**(예: SIFT, ORB) 간의 **유사성**을 계산합니다.
    *   → 이미지 → 특징 벡터 → 거리 → 최근접 클래스
*   특징 추출기 없이는, 분류기는 **원시 픽셀 값**에 직접 의존해야 합니다.
    *   → 각 이미지 자체가 **고차원 벡터**(예: 128×128 → 16,384-D)가 됩니다.
*   두 이미지 간의 **거리**는 그들의 픽셀 강도를 비교하여 측정됩니다.

**(이미지: 두 이미지 간의 L1 거리를 계산하는 예시. 각 픽셀 값의 차이의 절댓값을 모두 더함)**

***

### **페이지 315**

**특징 추출기를 사용하지 않을 때 — 최근접 이웃은 어떻게 작동하는가?**

*   두 이미지 간의 **거리**는 그들의 픽셀 강도를 비교하여 측정됩니다.
*   이는 최근접 이웃 분류기가 **원시 이미지 유사성**에 의해 작동한다는 것을 의미합니다 —
    단순히 픽셀 단위 거리가 가장 작은 이미지를 찾는 것입니다.

**(이미지: 새로운 이미지와 두 예시 이미지 간의 거리를 비교하여 더 가까운(거리가 작은) 예시를 선택하는 과정을 보여줌)**

***

### **페이지 316**

**예시: 최근접 이웃 분류기**

*   이 그림은 **최근접 이웃 분류기**의 결과를 보여줍니다.
*   각 **테스트 이미지**에 대해, 분류기는 **전체적인 외형이 가장 유사한** 훈련 이미지를 찾습니다.
*   여기서 볼 수 있듯이, 검색된 이미지들은 테스트 이미지와 **시각적으로 유사**하게 보이지만,
    **다른 클래스에 속할 수 있습니다**.

**(이미지: CIFAR-10 데이터셋에서 각 클래스의 테스트 이미지에 대해 최근접 이웃을 찾은 결과. 시각적으로는 비슷하지만 클래스가 다른 경우가 많음)**

***

### **페이지 317**

**예시: 최근접 이웃 분류기**

*   이 그림은 **최근접 이웃 분류기**의 결과를 보여줍니다.
*   각 **테스트 이미지**에 대해, 분류기는 **전체적인 외형이 가장 유사한** 훈련 이미지를 찾습니다.
*   여기서 볼 수 있듯이, 검색된 이미지들은 테스트 이미지와 **시각적으로 유사**하게 보이지만,
    **다른 클래스에 속할 수 있습니다 (빨간색)**.

**(이미지: 이전 그림에서 잘못 분류된 경우를 빨간색 상자로 강조)**

***

### **페이지 318**

**픽셀 거리를 사용하는 k-최근접 이웃의 한계**

*   이미지 분류에서 k-NN의 치명적인 문제점은 **픽셀 거리**에 기반하여 유사성을 측정한다는 것입니다.
*   하지만, **픽셀 거리는 유익하지 않습니다** — 의미 있는 시각적 유사성을 포착하는 데 실패합니다.
*   아래에서 볼 수 있듯이, 세 개의 변경된 이미지(가려짐, 이동, 색조 변경)는 모두 원본과 **동일한 픽셀 거리**를 가집니다.
*   심지어 **1픽셀 이동**만으로도 이미지가 거의 동일하게 보임에도 불구하고 큰 수치적 차이를 초래합니다.

**절대 사용되지 않음!!**

**(이미지: 원본 여성의 얼굴 사진과, 눈이 가려지거나, 1픽셀 이동하거나, 색조가 변경된 세 개의 사진을 비교. 세 변형 모두 원본과 동일한 픽셀 거리를 가짐을 설명)**

***

### **페이지 319**

**참고 자료**

*   Stanford CS231N | 2025년 봄 | 강의 2: 선형 분류기를 이용한 이미지 분류
    *   https://www.youtube.com/watch?v=pdqofxJeBN8&list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16&index=2

**(이미지: 2017년과 2025년의 CS231N 강의 장면 비교)**

***

### **페이지 320**

**컴퓨터 비전의 기초**
**2025년 2학기**
**- 엄광현 –**
**인공지능학과**

***

### **페이지 321**

**강의 6:**
**신경망 및 최적화**
*   신경망이란 무엇인가
*   신경망 훈련
*   최적화

***

### **페이지 322**

**고양이 vs. 개**

*   **"어떻게 구별할 수 있나요?"**
    *   아마도 귀, 눈, 털 패턴, 심지어 얼굴 모양을 볼 것입니다.

**(이미지: 고양이와 개의 얼굴 사진)**

***

### **페이지 323**

**"숫자만으로 고양이를 알아볼 수 있나요?"**

*   **숫자로서의 픽셀**
    *   **"하지만 이 고양이 이미지를 가져와서 픽셀 값만 보여준다면 — 큰 배열 속의 숫자들만 — 여전히 고양이인지 알 수 있을까요?"**
*   **그렇지 않습니다. 개별 픽셀 값은 우리에게 많은 것을 알려주지 않습니다.**

**(이미지: 컴퓨터가 이미지를 픽셀 값의 숫자 배열로 보는 것을 보여주는 그림)**

***

### **페이지 324**

**복습: 엣지 탐지에서 특징 매칭까지**

*   지금까지 우리는 이미지에서 엣지를 탐지하고, (코너와 같은) 특징을 탐지하며, 그 특징을 기술하고, 이미지 간에 특징 매칭을 수행하는 방법을 배웠습니다.

**(이미지: 원본 고양이 사진 → 캐니 엣지 맵 → 코너 탐지 및 각 코너에 대한 기술자 벡터 생성 과정을 보여주는 그림)**

***

### **페이지 325**

**복습: 매칭 결과에 기반한 클래스 결정**

*   우리는 새로운 이미지와 **더 많은 특징이 매칭된** 예시 이미지를 선택할 수 있습니다.
*   **더 많은 수의 매치**를 가진 예시가 **더 유사하다**고 간주됩니다.
*   따라서, 매칭 결과를 바탕으로 새로운 이미지가 **고양이**인지 **개**인지 결정할 수 있습니다.
*   **다음을 사용할 수 있습니다:**
    *   좋은 매치의 수
    *   평균 기술자 거리
    *   최소 기술자 거리

**(이미지: 고양이 예시와 더 많은 매치가 발생했으므로, "이것은 고양이다!"라고 결론 내리는 과정을 보여줌)**

***

### **페이지 326**

**복습: 최근접 이웃 분류**

*   우리는 모든 샘플 중에서 **가장 유사한 예시**를 찾아 새로운 이미지를 분류할 수 있습니다.
*   이 **단일 최근접 예시**의 클래스를 예측으로 사용할 수 있습니다.
*   또는, **상위 k개의 가장 유사한 예시**(예: k = 2, 3, ...)를 찾아 그들 사이의 **다수결 투표**로 최종 클래스를 결정할 수 있습니다.

**(이미지: 여러 예시 중 가장 잘 매칭되는(Best Match!) 이미지를 찾아 "이것은 고양이다!"라고 예측하는 과정을 보여줌)**

***

### **페이지 327**

**복습: f(x) 학습하기 — 로지스틱 회귀로 시작하기**

*   각 이미지에 대해, 우리는 **특징 벡터 x**를 가지고 있습니다.
    *   우리는 y = f(x)가 되는 함수 f를 찾고 싶습니다.
*   이를 모델링하는 가장 간단한 방법은 **로지스틱 회귀**를 사용하는 것입니다.
    *   로지스틱 회귀는 입력 특징들을 가중치 w와 선형적으로 결합합니다:
        *   s = wᵀx + b
    *   그런 다음 **시그모이드** 또는 **소프트맥스** 함수를 적용하여 클래스 확률을 생성합니다.
    *   이것은 단일 결정 경계로 클래스를 분리하는 **선형 분류기** 역할을 합니다.

**(이미지: 특징 벡터 x가 함수 f(x)를 통해 클래스 점수를 예측하는 과정을 보여주는 그림)**

***

### **페이지 328**

**복습: 선형 분류기의 차원**

*   각 구성 요소의 차원을 살펴봅시다:
    *   x ∈ ℝ³⁰⁷²ˣ¹, W ∈ ℝ³ˣ³⁰⁷², b ∈ ℝ³ˣ¹
*   따라서 선형 변환의 출력은 다음과 같습니다:
    *   Wx + b ∈ ℝ³ˣ¹
*   이는 **세 개의 클래스 점수**(예: 고양이, 개, 차)를 생성합니다.
*   f는 파라미터 W에 의존하므로, 이를 명시적으로 f(x, W)로 쓸 수 있습니다.

**(이미지: 다중 클래스 분류를 위한 선형 분류기의 구조와 각 벡터/행렬의 차원을 명시)**

***

### **페이지 329**

**복습: 선형 분류기의 기하학적 해석**

*   **기하학적 관점**에서, 선형 분류기는 **클래스별 선형 함수**의 집합으로 표현될 수 있습니다.
*   각 클래스 i에 대해, 방정식은 특징 공간에서 **결정 평면(초평면)**을 정의합니다.
*   이 평면에 대한 샘플의 **측면과 거리**는 그 로짓의 **부호와 크기**에 해당하며 — 모델이 해당 샘플이 그 클래스에 속한다고 얼마나 강하게 믿는지를 나타냅니다.

**(이미지: 3차원 특징 공간에서 여러 클래스(비행기, 자동차, 사슴)를 분리하는 초평면들을 시각화한 그림)**

***

### **페이지 330**

**복습: 선형 분류기가 "보는" 것**

*   **시각화**
    *   **비행기**: 전체적으로 파란색 영역 → 모델이 **하늘 배경 패턴**을 학습함
    *   **자동차**: 중앙의 둥근 보라색 모양 → **자동차 몸체 모양**에 집중함
    *   **개구리**: 녹색과 노란색의 혼합 → 개구리의 **질감과 색상 패턴**을 포착함
    *   **배**: 수평의 파란색 영역 → **수면 배경 패턴**을 학습함

*   **흐릿한 가중치**
    *   모델은 데이터로부터 **평균적인 패턴**만 학습하기 때문에, 대부분의 **미세한 세부 사항은 사라지고**, 흐릿한 **원형(prototypical) 패턴**만 남습니다.

**(이미지: CIFAR-10 데이터셋의 각 클래스에 대해 학습된 선형 분류기의 가중치를 시각화한 결과)**

***

### **페이지 331**

**복습: 선형 분류기의 어려운 경우**

*   일부 데이터셋은 **선형적으로 분리 가능하지 않습니다**.
*   스스로에게 물어보세요: 다음 클래스 배치에서, **단일 선(초평면)**이 클래스들을 분리할 수 있나요?
    *   **XOR 패턴** (1, 3사분면 vs. 2, 4사분면) → **아니오**
    *   **링/고리 모양** (1 ≤ ||x||₂ ≤ 2인 점들 vs. 나머지) → **아니오**
    *   **다중 모드 클러스터** (클래스 1에 대한 여러 분리된 덩어리) → **아니오**
*   선형 분류기는 이러한 분포를 분리하기 위해 하나의 경계를 그릴 수 없습니다.
    *   → 우리는 **비선형 결정 경계**(특징 맵/커널, MLP, CNN)가 필요합니다.

**(이미지: XOR, 링, 다중 모드 클러스터 패턴을 나란히 보여주는 그림)**

***

### **페이지 332**

**선형 분류기의 한계**

*   선형 분류기의 핵심 한계는 그 구조에 있습니다: 입력 x의 **단 하나의 선형 결합**만을 수행하여 점수를 생성합니다.
    *   각 클래스는 고유한 가중치 집합을 가지지만, 주어진 입력 x에 대해 모델은 여전히 **단일 선형 변환**을 적용합니다:
        *   z = Wx + b
*   끝에 있는 **소프트맥스 레이어**는 단지 이 점수들을 확률로 **정규화**할 뿐 — 어떠한 비선형 능력도 추가하지 않습니다.
*   이러한 **선형적 특성** 때문에, 모델은 특징 공간에서 **직선(또는 초평면)**을 사용해서만 데이터를 분리할 수 있습니다.

**(이미지: 선형 분류기의 연산 과정과 기하학적 한계를 보여주는 그림)**

***

### **페이지 333**

**선형 결합: 점수 계산**

*   우리는 **선형 분류기**를 간단한 단일 단계 변환으로 시각화할 수 있습니다.
*   이전 다이어그램을 x를 왼쪽에 두도록 재배열하면 다음과 같습니다.

**(이미지: 입력 벡터 x가 각 클래스의 가중치 벡터(W₁, W₂, W₃)와 곱해지고 편향(b₁, b₂, b₃)이 더해져 최종 점수(z₁, z₂, z₃)가 계산되는 과정을 시각적으로 보여주는 그림)**

***

### **페이지 334**

**선형 분류기의 한계**

*   이제 W의 **한 행**, 즉 W₁만 살펴봅시다.

**(이미지: 이전 그림에서 첫 번째 클래스(고양이)의 점수가 계산되는 과정만 분리하여 보여줌)**

***

### **페이지 335**

**행렬 곱셈: W₁x**

*   연산 W₁x는 x와 W₁의 각 요소를 곱한 다음, 모든 곱을 **합산**하는 것과 동일합니다.

**(이미지: W₁x의 계산 과정을 각 요소의 곱셈과 덧셈으로 분해하여 시각적으로 보여주는 그림)**

***

### **페이지 336**

**가중치 입력 시각화**

*   이제 가중치 W가 X의 각 요소에 연결된 **화살표 위에** 표시되도록 다이어그램을 수정해 봅시다.

**(이미지: 이전 그림에서 가중치 값을 화살표 위에 표시하여 시각화 방식을 변경)**

***

### **페이지 337**

**가중치 입력 시각화**

*   이제 가중치 W가 X의 각 요소에 연결된 **화살표 위에** 표시되도록 다이어그램을 수정해 봅시다.

**(이미지: 이전 그림에서 각 입력 노드에서 합산 노드로 향하는 화살표로 연결하여 신경망 노드처럼 보이게 변경)**

***

### **페이지 338**

**편향 항 포함하기**

*   다음으로, 입력 벡터에 상수 1을 추가하여 **편향 덧셈**을 **곱셈**으로 표현할 수 있습니다.

**(이미지: 입력 벡터 x에 1을 추가하고, 가중치 벡터 W₁에 편향 b₁을 추가하여 편향 덧셈을 곱셈 연산에 포함시키는 과정을 보여줌)**

***

### **페이지 339**

**편향 항 포함하기**

*   다음으로, 입력 벡터에 상수 1을 추가하여 **편향 덧셈**을 **곱셈**으로 표현할 수 있습니다.

**(이미지: 이전 그림을 더 단순화하여 하나의 노드로 표현)**

***

### **페이지 340**

**결합 단순화**

*   따라서, 이제 우리는 결합 부분을 단순화하여 **Wx + b**로 간결하게 표현할 수 있습니다.
*   지금부터는 편의상 **Wx**를 사용하여 **Wx + b**를 나타내겠습니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 341**

**전체 다이어그램으로 돌아가기**

*   이제 원래 그림으로 돌아가서, 이 단순화된 버전을 사용하여 **선형 결합** 부분을 나타내 봅시다.

**(이미지: 입력 노드들이 각 출력 노드(z₁, z₂, z₃)에 직접 연결되는 단순화된 다이어그램)**

***

### **페이지 342**

**전체 다이어그램으로 돌아가기**

*   이제 원래 그림으로 돌아가서, 이 단순화된 버전을 사용하여 **선형 결합** 부분을 나타내 봅시다.

**(이미지: 이전 그림에 가중치 행렬(W₁, W₂)을 나타내는 연결선을 추가)**

***

### **페이지 343**

**전체 다이어그램으로 돌아가기**

*   이제 원래 그림으로 돌아가서, 이 단순화된 버전을 사용하여 **선형 결합** 부분을 나타내 봅시다.

**(이미지: 이전 그림에 전체 가중치 행렬 W를 표시)**

***

### **페이지 344**

**완전 연결 구조**

*   이 그림에서 볼 수 있듯이, X의 각 입력 요소는 해당 가중치 W₁, W₂, W₃를 통해 모든 출력 유닛 z₁, z₂, z₃에 연결됩니다.
*   이러한 유형의 구조를 **완전 연결(fully connected)** (또는 **밀집(dense)**) 레이어라고 합니다.

**(이미지: 완전 연결 구조를 강조하는 다이어그램)**

***

### **페이지 345**

**완전 연결 구조**

*   이 그림에서 볼 수 있듯이, X의 각 입력 요소는 해당 가중치 W₁, W₂, W₃를 통해 모든 출력 유닛 z₁, z₂, z₃에 연결됩니다.
*   이러한 유형의 구조를 **완전 연결(fully connected)** (또는 **밀집(dense)**) 레이어라고 합니다.

*   우리는 현재 출력 Z를 최종 점수로 직접 사용할 수 없습니다 — 이것은 여전히 **선형 결합**일 뿐입니다.
*   모델을 더 표현력 있게 만들려면, **비선형성**을 도입해야 합니다!

**(이미지: 이전 다이어그램에 큰 물음표를 추가하여, 선형 결합만으로는 부족함을 암시)**

***

### **페이지 346**

**비선형성 도입: 활성화 함수**

*   그래서 사람들은 선형 출력 Z = Wx 바로 뒤에 **비선형 함수**를 붙이기 시작했습니다.
*   이 비선형 함수를 **활성화 함수**라고 합니다.

**(이미지: 선형 결합의 출력(z)이 활성화 함수(φ)를 통과하여 최종 출력(φ(z))이 되는 과정을 보여주는 다이어그램)**

***

### **페이지 347**

**활성화 함수의 예**

*   신경망에서 일반적으로 사용되는 다양한 유형의 활성화 함수가 있습니다, 예를 들면:
    *   **ReLU** (Rectified Linear Unit), Leaky ReLU, **Sigmoid**, **Tanh** (Hyperbolic Tangent), ELU (Exponential Linear Unit), GELU (Gaussian Error Linear Unit), SiLU (Sigmoid Linear Unit, Swish라고도 함)
*   각 활성화 함수는 고유한 모양과 속성을 가지고 있어 그래디언트가 어떻게 흐르고 모델이 어떻게 학습하는지에 영향을 미칩니다.

**(이미지: ReLU, Leaky ReLU, Sigmoid, Tanh 등 다양한 활성화 함수의 그래프를 보여주는 그림. "ReLU는 대부분의 문제에 좋은 기본 선택입니다"라는 문구가 강조됨)**

***

### **페이지 348**

**아직 충분하지 않다**

*   여기에 활성화 함수를 추가하는 것만으로는 모델을 근본적으로 바꾸지 않습니다 — 단지 확률 계산을 조금 더 복잡하게 만들 뿐입니다.
    *   모델은 여전히 입력에서 출력으로 **하나의 선형 변환**을 수행합니다.
    *   활성화는 단지 **점수 스케일을 왜곡**할 뿐입니다.

**(이미지: 활성화 함수를 추가해도 여전히 한계가 있음을 큰 물음표로 표현)**

***

### **페이지 349**

**비선형성을 위한 레이어 쌓기**

*   진정한 비선형성을 도입하기 위해, 우리는 **여러 레이어를 쌓을** 수 있습니다 — 즉, 활성화 이후에 또 다른 **선형 결합**을 수행합니다.

**(이미지: 두 개의 완전 연결 레이어와 그 사이의 활성화 함수로 구성된 2계층 신경망 구조를 보여주는 다이어그램)**

***

### **페이지 350**

**비선형성을 위한 레이어 쌓기**

*   첫 번째 변환이 φ(·)를 포함하기 때문에, x에서 z²로의 전체 매핑은 **비선형**이 됩니다.

**(이미지: 이전 페이지의 다이어그램을 반복하여 보여줌)**

***

### **페이지 351**

**활성화 함수가 없다면?**

*   이제, 만약 활성화 함수 없이 여러 레이어를 쌓으면 어떻게 될까요?

**(이미지: 이전 다이어그램에서 활성화 함수 부분을 흐리게 처리하고 큰 물음표를 추가하여, 활성화 함수가 없을 때의 결과를 질문)**

***

### **페이지 352**

**활성화 함수가 없다면?**

*   여러 선형 레이어를 쌓더라도, 그것들은 항상 **단일 선형 변환으로 축소**될 수 있습니다 → 결국 다시 선형 분류기가 됩니다.

**(이미지: 활성화 함수가 없는 2계층 신경망이 단일 선형 변환(W²W¹x)으로 축소될 수 있음을 수식으로 보여줌)**

***

### **페이지 353**

**활성화 함수가 없다면?**

*   여러 선형 레이어를 쌓더라도, 그것들은 항상 **단일 선형 변환으로 축소**될 수 있습니다 → 결국 다시 선형 분류기가 됩니다.

**(이미지: 이전 수식에 W³ = W²W¹을 추가하여, 여러 선형 레이어가 하나의 등가 행렬로 합쳐질 수 있음을 보여줌)**

***

### **페이지 354**

**활성화 함수가 없다면?**

*   여러 선형 레이어를 쌓더라도, 그것들은 항상 **단일 선형 변환으로 축소**될 수 있습니다 → 결국 다시 선형 분류기가 됩니다.

**(이미지: 2계층 선형 신경망이 입력에서 출력으로 직접 연결되는 단일 선형 변환과 동일함을 시각적으로 보여주는 그림)**

***

### **페이지 355**

**따라서, 우리는 활성화 함수가 필요하다**

*   따라서, 모델에 **비선형성**을 도입하기 위해 **활성화 함수**가 필요합니다.
*   비선형 활성화를 포함하면, 전체 변환은 더 이상 단일 행렬 W³으로 **축소될 수 없습니다**.

**(이미지: 활성화 함수가 포함된 2계층 신경망은 단일 행렬로 축소될 수 없음(≠ W³x)을 보여주는 다이어그램)**

***

### **페이지 356**

**선형 모델에서 신경망으로**

*   이 구조를 **2계층 신경망**이라고 합니다.
*   2개 이상의 레이어부터는 모델이 **비선형 활성화**를 포함하기 때문에 **신경망**이라고 부를 수 있습니다.
    *   1계층 모델은 단지 **선형 점수 함수**일 뿐 — 비선형성이 없으므로 진정한 신경망이 아닙니다.

**(이미지: 2계층 신경망 구조를 보여주는 다이어그램)**

***

### **페이지 357**

**선형 모델에서 신경망으로**

*   **1-레이어**: 선형 모델
*   **2-레이어**: 최초의 진정한 신경망 (비선형)
*   **3+ 레이어**: 더 깊은 네트워크 — 더 표현력이 풍부함

**(수식: 1, 2, 3계층 신경망의 출력을 계산하는 공식을 각각 보여줌)**

***

### **페이지 358**

**다이어그램 단순화하기**

*   이제 다이어그램을 더 단순화해 봅시다.
*   우리는 **선형 결합, 활성화, 그리고 출력**을 함께 **단일 유닛** — 하나의 **노드**로 취급할 수 있습니다.
    *   각 노드는 입력을 받고, 가중치를 적용하고, 편향을 더하고, 활성화를 통과시켜 출력을 생성하는 이 전체 과정을 나타냅니다.

**(이미지: 신경망의 한 뉴런(선형 결합 + 활성화)을 하나의 단위로 묶어 표시)**

***

### **페이지 359**

**다이어그램 단순화하기**

*   이제 다이어그램을 더 단순화해 봅시다.
*   우리는 **선형 결합, 활성화, 그리고 출력**을 함께 **단일 유닛** — 하나의 **노드**로 취급할 수 있습니다.
    *   각 노드는 입력을 받고, 가중치를 적용하고, 편향을 더하고, 활성화를 통과시켜 출력을 생성하는 이 전체 과정을 나타냅니다.

**(이미지: 이전 다이어그램의 각 뉴런을 단순한 원(노드)으로 표현한 그림)**

***

### **페이지 360**

**입력과 출력을 노드로 표현하기**

*   **입력**과 **출력**도 노드로 표현해 봅시다 — 각각에 대해 원을 사용합니다.

**(이미지: 입력과 출력까지 모두 원(노드)으로 표현한 신경망 다이어그램)**

***

### **페이지 361**

**레이어 이름 붙이기**

*   이제 우리는 레이어를 다음과 같이 부릅니다: **왼쪽**은 **입력 레이어**, **오른쪽**은 **출력 레이어**, 그리고 **가운데 부분**은 **은닉 레이어**라고 합니다.

**(이미지: 신경망 다이어그램의 각 부분을 입력 레이어, 은닉 레이어, 출력 레이어로 명명)**

***

### **페이지 362**

**각 레이어는 몇 개의 노드를 가져야 하는가?**

*   **입력 레이어**의 노드 수는 **입력 특징**의 수와 같습니다.
*   **출력 레이어**의 노드 수는 **클래스**의 수와 같습니다.
*   따라서, 입력 및 출력 레이어 크기는 데이터와 작업에 의해 **고정**됩니다.

**(이미지: 이전 다이어그램을 반복하여 보여줌)**

***

### **페이지 363**

**은닉 레이어는 어떤가?**

*   이 다이어그램에서, 은닉 레이어는 우연히 출력 레이어와 동일한 수의 노드를 가지고 있습니다. 하지만 꼭 그렇게 고정되어야 할까요?
*   → **전혀 그렇지 않습니다.**

**(이미지: 이전 다이어그램을 반복하여 보여줌)**

***

### **페이지 364**

**은닉 레이어는 어떤가?**

*   이 다이어그램에서, 은닉 레이어는 우연히 출력 레이어와 동일한 수의 노드를 가지고 있습니다. 하지만 꼭 그렇게 고정되어야 할까요?
*   → **전혀 그렇지 않습니다.**

**(이미지: 은닉 레이어의 노드 수를 늘린 다이어그램)**

***

### **페이지 365**

**은닉 레이어는 어떤가?**

*   이 다이어그램에서, 은닉 레이어는 우연히 출력 레이어와 동일한 수의 노드를 가지고 있습니다. 하지만 꼭 그렇게 고정되어야 할까요?
*   → **전혀 그렇지 않습니다.**
*   우리는 모델 용량, 계산 비용, 또는 실험적 목표에 따라 **임의의 수의 노드**를 선택할 수 있습니다.

**(이미지: 은닉 레이어의 노드 수가 가변적임을 점선으로 표현한 다이어그램)**

***

### **페이지 366**

**2계층 신경망의 행렬 차원**

*   각 레이어의 차원을 살펴봅시다.
*   **만약 우리가 정의한다면:**
    *   입력 차원: **D**
    *   은닉 레이어 크기: **H**
    *   출력 차원 (클래스 수): **C**
*   **그러면 가중치 행렬은 다음과 같은 모양을 가져야 합니다:**
    *   W¹: 입력(D) → 은닉(H)을 매핑
    *   W²: 은닉(H) → 출력(C)을 매핑

**(이미지: 2계층 신경망 다이어그램에 각 벡터와 행렬의 차원을 명시)**

***

### **페이지 367**

**3계층 신경망 (2개의 은닉 레이어)**

*   이제 **3계층 신경망**을 살펴봅시다. 이는 **2개의 은닉 레이어**를 가지고 있다는 것을 의미합니다.
    *   입력 차원 → **D**
    *   첫 번째 은닉 레이어 → **H₁**
    *   두 번째 은닉 레이어 → **H₂**
    *   출력 차원 (클래스 수) → **C**

**(이미지: 2개의 은닉 레이어를 가진 3계층 신경망 다이어그램과 각 행렬의 차원을 명시)**

***

### **페이지 368**

**다층 신경망**

*   각 레이어는 입력 차원을 다음 레이어의 출력 차원으로 변환합니다. 이러한 변환의 연결은 **다층 신경망**을 정의합니다.

**(이미지: 이전 페이지의 다이어그램을 반복하여 보여줌)**

***

### **페이지 369**

**3계층 신경망의 계산 흐름**

**(이미지: 3계층 신경망의 순전파 과정을 보여주는 다이어그램과 이를 구현한 Python 코드 스니펫)**

***

### **페이지 370**

**종단간 분류 모델**

*   이제 우리는 **입력 이미지**를 받아 **3계층 신경망**을 통해 출력에서 **클래스 점수**를 예측하는 모델을 구축했습니다.

**(이미지: 입력 이미지(고양이)가 3계층 신경망을 통과하여 각 클래스에 대한 점수(36, 7, 18)를 출력하는 전체 과정을 보여주는 다이어그램)**

***

### **페이지 371**

**은닉 레이어 크기의 효과**

*   은닉 레이어의 뉴런 수를 늘리면, **결정 경계**가 더 유연하고 복잡해집니다.
*   이는 네트워크가 **더 높은 용량**을 갖게 됨을 의미합니다 — 데이터에서 더 복잡한 패턴을 표현할 수 있습니다.
*   하지만, 더 큰 용량은 **과적합(overfitting)의 위험**도 증가시킨다는 것을 기억해야 합니다.

**(이미지: 은닉 뉴런의 수가 3개, 6개, 20개로 늘어남에 따라 결정 경계가 점점 더 복잡해지는 것을 보여주는 그림. "더 많은 뉴런 = 더 많은 용량"이라는 문구가 표시됨)**

***

### **페이지 372**

**왜 "신경망"이라고 불리는가?**

*   자, 왜 우리는 이것을 *신경망*이라고 부를까요?

**(이미지: 뇌의 뉴런들이 서로 연결되어 신호를 주고받는 모습을 형상화한 그림)**

***

### **페이지 373**

**왜 "신경망"이라고 불리는가?**

*   자, 왜 우리는 이것을 *신경망*이라고 부를까요?
*   하나의 단일 유닛 — **선형 결합**과 그 뒤의 **활성화** — 을 살펴봅시다.
*   이 유닛은 1950년대에 생물학적 뉴런이 작동하는 방식에서 영감을 받아 제안된 개념인 **퍼셉트론**으로 알려져 있습니다.

**(이미지: 인공 뉴런(퍼셉트론)의 계산 과정과 생물학적 뉴런(Perceptron)의 구조를 비교하는 그림)**

***

### **페이지 374**

**생물학적 영감**

*   인간의 뇌에서, 각 **뉴런**은 다른 많은 뉴런으로부터 신호를 받고, 이를 결합하며, 총 신호가 임계값을 초과하면 **발화(fire)**합니다.
*   **퍼셉트론**은 이 동작을 모방합니다:
    *   입력 xᵢ = 다른 뉴런으로부터의 신호
    *   가중치 wᵢ = 연결 강도
    *   활성화 f(·) = 뉴런의 "발화" 결정

**(이미지: 생물학적 뉴런의 구조(수상돌기, 축삭, 시냅스)와 인공 뉴런(퍼셉트론)의 다이어그램을 비교)**

***

### **페이지 375**

**생물학적 뉴런 vs. 인공 뉴런의 연결**

*   인간의 뇌에는 약 **860억 개의 뉴런**이 있습니다 — 전체 뇌 세포의 약 10%입니다.
*   생물학적 뉴런은 **매우 복잡하고, 불규칙한 패턴**으로 연결되어, 방대하고 동적인 상호작용 네트워크를 형성합니다.
*   **대조적으로**: 인공 신경망에서 뉴런은 계산 효율성을 위해 **규칙적이고, 구조화된 레이어**로 구성됩니다.

**(이미지: 복잡하게 얽힌 생물학적 뉴런의 연결망과, 규칙적인 층 구조를 가진 인공 신경망을 비교하는 그림)**

***

### **페이지 376**

**무작위로 연결된 신경망**

*   흥미롭게도, 신경망은 연결이 **무작위로 구성**될 때도 작동할 수 있습니다.
*   우리가 보통 설계하는 엄격한 계층 구조와 달리, 일부 네트워크는 **불규칙하거나 부분적으로 무작위적인 연결**을 가지며 — 그럼에도 불구하고 효과적으로 학습할 수 있습니다.
*   하지만 효율성을 위해, 우리는 보통 규칙적인 계층 구조를 사용합니다.

**(이미지: 생물학적 뉴런의 복잡한 연결성과, 무작위 연결을 가진 인공 신경망도 작동할 수 있음을 보여주는 그림)**

***

### **페이지 377**

**뇌 비유에 주의할 점**

*   **실제로는...** 생물학적 뉴런은 각각 독특한 구조와 기능을 가진 **매우 다양한 유형**으로 존재합니다.
*   **수상돌기**는 입력의 수동적인 합산뿐만 아니라 **복잡하고 비선형적인 계산**을 수행할 수 있습니다.
*   **시냅스**는 단일 스칼라 가중치가 아닙니다 — 화학적, 시간적, 공간적 요인에 따라 진화하는 **고도로 동적이고 비선형적인 시스템**입니다.
*   신경 활동은 단순한 행렬 곱셈이 포착할 수 있는 것보다 훨씬 더 복잡한 **시간 의존적인 스파이크, 신경 전달 물질, 그리고 가소성**을 포함합니다.
*   인공 신경망은 생물학적 시스템에서 **영감을 받았지만** — 뇌의 **충실한 모델은 아닙니다**.
    *   우리의 뉴런은 생물학적으로가 아니라 효율적으로 계산합니다.

***

### **페이지 378**

**신경망 훈련**

**(이미지: 데이터 과학을 상징하는 그래픽)**

***

### **페이지 379**

**신경망에서 빠진 것은 무엇인가?**

*   지금까지 우리는 신경망이 **무엇**이며, 정보가 앞으로 전달되는 레이어, 뉴런, 연결로 **어떻게 구조화**되어 있는지 보았습니다.
    *   또한 가중 합과 비선형 활성화의 연속을 통해 주어진 입력으로부터 **어떻게 출력을 계산**하는지도 살펴보았습니다.
*   **하지만... 무언가 빠졌습니다.**

**(이미지: 3계층 신경망의 전체 구조를 보여주는 다이어그램)**

***

### **페이지 380**

**신경망에서 빠진 것은 무엇인가?**

*   지금까지 우리는 신경망이 **무엇**이며, 정보가 앞으로 전달되는 레이어, 뉴런, 연결로 **어떻게 구조화**되어 있는지 보았습니다.
    *   또한 가중 합과 비선형 활성화의 연속을 통해 주어진 입력으로부터 **어떻게 출력을 계산**하는지도 살펴보았습니다.
*   **하지만... 무언가 빠졌습니다.**

**(이미지: 이전 다이어그램의 모든 가중치와 편향에 물음표를 표시하여, 이 값들을 어떻게 결정하는지에 대한 의문을 제기)**

***

### **페이지 381**

**신경망에서 빠진 것은 무엇인가?**

*   지금까지 우리는 **고정된** 가중치와 편향을 사용하여 출력만 계산했습니다.
*   우리가 진정으로 원하는 것은 네트워크가 데이터로부터 이러한 파라미터들을 **자동으로 학습**하는 것입니다.
*   그 빠진 조각은 **학습**입니다 — 더 나은 예측을 하기 위해 가중치와 편향을 조정하는 과정입니다.

**(이미지: 이전 다이어그램의 가중치 행렬들에 큰 물음표를 표시하여, 학습의 필요성을 강조)**

***

### **페이지 382**

**신경망은 어떻게 학습할 수 있는가?**

*   먼저, 네트워크는 **무언가 잘못되었다는 것을 깨달아야** 합니다.
    *   현재 예측이 **맞는지 틀리는지** 알아야 합니다.
*   이러한 **오류의 인식**이 **학습의 첫걸음**입니다.
*   학습하기 위해, 네트워크는 **얼마나 틀렸는지** 측정해야 합니다 — 그리고 이것이 바로 **손실 함수**가 하는 일입니다.

*   예를 들어, **고양이** 이미지를 입력했지만 네트워크가 **"배"**에 가장 높은 점수를 할당했다고 가정해 봅시다.
*   분명히 무언가 잘못되었고 — 네트워크는 이 **실수를 인식**할 수 있어야 합니다.

**(이미지: 잘못된 예측(배 점수가 가장 높음)을 하고 있는 신경망과, 이 실수를 나타내는 큰 물음표)**

***

### **페이지 383**

**네트워크가 얼마나 틀렸는지 측정하기**

*   네트워크의 예측이 얼마나 틀렸는지 알기 위해서는, 오차를 **측정**할 적절한 척도나 지표가 필요합니다.
*   네트워크가 **각 클래스에 대한 점수**를 생성하고, 이 점수들을 **소프트맥스 함수**를 사용하여 **확률**로 변환한다고 가정해 봅시다.

**(이미지: 신경망의 출력 점수가 소프트맥스 정규화를 거쳐 확률로 변환되는 과정을 보여줌)**

***

### **페이지 384**

**정답 표현하기**

*   자, 그렇다면 **정답**은 어떻게 생겼을까요?
*   만약 실제 레이블이 **"고양이"**라면, 목표 확률은 **"고양이"에 대해 1**이고 **다른 모든 클래스에 대해 0**이어야 합니다 —처럼.
    *   이러한 종류의 표현을 **원-핫 인코딩**이라고 합니다.
*   따라서 네트워크는 예측된 확률을 이 원-핫 목표와 비교하여 **얼마나 차이가 나는지** 볼 수 있습니다.

**(이미지: 예측된 확률과, 정답을 나타내는 원-핫 인코딩된 벡터를 비교하는 과정을 보여줌)**

***

### **페이지 385**

**정답 표현하기**

*   자, 그렇다면 **정답**은 어떻게 생겼을까요?
*   만약 실제 레이블이 **"고양이"**라면, 목표 확률은 **"고양이"에 대해 1**이고 **다른 모든 클래스에 대해 0**이어야 합니다 —처럼.
    *   이러한 종류의 표현을 **원-핫 인코딩**이라고 합니다.
*   따라서 네트워크는 예측된 확률을 이 원-핫 목표와 비교하여 **얼마나 차이가 나는지** 볼 수 있습니다.

**(이미지: 예측된 확률과 원-핫 인코딩된 정답 벡터의 각 요소를 비교하는 화살표를 추가)**

***

### **페이지 386**

**목표 클래스에 집중하기**

*   사실, 우리는 **모든 클래스**의 확률을 비교할 필요가 없습니다.
*   정말로 중요한 것은 **정확한 (목표) 클래스의 예측 확률**입니다 — 예를 들어, "고양이"에 할당된 확률.
    *   만약 우리가 이 목표 확률을 **증가**시키는 데 집중한다면, 다른 모든 클래스의 확률은 소프트맥스 함수 이후 모든 확률의 합이 1이 되어야 하므로 **자동으로 감소**할 것입니다.

**(이미지: 정답 클래스(고양이)의 확률만 비교하고, 나머지 클래스는 무시하는 개념을 시각적으로 보여줌)**

***

### **페이지 387**

**학습 목표**

*   따라서, 학습 목표는 간단해집니다: 정확한 클래스의 확률을 높이는 것입니다.
*   우리는 **정확한 (목표) 클래스**에 대해 **낮은 예측 확률**을 벌하는 **손실 함수**를 정의합니다. 이 손실 함수를 **교차 엔트로피 손실**이라고 합니다.
    *   이는 네트워크가 실제 클래스에는 **높은 확률**을, 다른 모든 클래스에는 **낮은 확률**을 할당하도록 유도합니다.

**(이미지: 교차 엔트로피 손실 함수를 사용하여 예측(ŷ)과 실제 레이블(y) 간의 오차를 계산하는 과정을 보여주는 다이어그램)**

***

### **페이지 388**

**교차 엔트로피 손실 공식**

*   **정확한 클래스**만이 y_c = 1을 가지므로, 손실은 효과적으로 다음과 같이 됩니다:
    *   L = -log(ŷ_correct)
*   이는 네트워크가 정확한 클래스에 대한 예측 확률이 **작을 때** 벌을 받고, **1에 가까울 때** 보상을 받는다는 것을 의미합니다.

**(이미지: 이전 다이어그램에 교차 엔트로피 손실 공식의 단순화된 형태를 추가)**

***

### **페이지 389**

**예측 확률이 낮을 때 무슨 일이 일어나는가?**

*   자, 만약 정확한 클래스에 대한 예측 확률이 **매우 낮다면**, 예를 들어 **0.1**이라고 해봅시다.
    *   결과는 **L = 2.3** — **큰 손실 값**입니다.
    *   이는 네트워크가 잘못된 답에 너무 확신을 가진 것에 대해 **심하게 벌을 받는다**는 것을 의미합니다.
*   이 손실은 단순히 확률 간의 차이(예: 1 - 0.1 = 0.9)보다 **훨씬 더 크다**는 점에 유의하세요.

**(이미지: 예측 확률이 0.1일 때 손실 값이 2.3으로 계산되는 예시)**

***

### **페이지 390**

**높은 확신도의 경우**

*   이제, **높은 확신도**의 경우를 살펴봅시다 — 정확한 클래스에 대한 예측 확률이 **0.9**일 때.
    *   결과는 **L = 0.105**이며, 이는 낮은 확신도의 경우에 비해 **매우 작습니다**.
    *   이는 네트워크가 **확신을 가지고 정확할 때**, 손실이 **작아지고** 적용될 **벌이 거의 없다**는 것을 보여줍니다.

**(이미지: 예측 확률이 0.9일 때 손실 값이 0.105로 계산되는 예시)**

***

### **페이지 391**

**교차 엔트로피 손실 곡선**

*   정확한 클래스에 대한 예측 확률이 증가함에 따라 **손실이 어떻게 변하는지** 시각화해 봅시다.
    *   곡선은 작은 확률 근처에서 **가파르고**, 1에 가까워질수록 **평평해집니다**.
*   이 모양은 네트워크가 **확신을 가졌지만 틀렸을 때**는 **심하게 벌을 받고**, **확신을 가지고 맞았을 때**는 **부드럽게 보상**받는다는 것을 명확하게 보여줍니다.

**(이미지: 예측 확률에 따른 교차 엔트로피 손실 값의 변화를 보여주는 그래프)**

***

### **페이지 392**

**학습의 목표**

*   따라서, 우리의 목표는 손실 L을 **가능한 한 작게**, 이상적으로는 **0에 가깝게** 만드는 가중치와 편향 — W¹, W², ... — 을 **찾는 것**입니다.
*   다시 말해, 우리는 네트워크의 예측이 **가능한 한 정확**해지고, **교차 엔트로피 손실이 최소화**되도록 파라미터를 조정하고 싶습니다.
    *   이 과정을 우리는 **학습** 또는 신경망 **훈련**이라고 부릅니다.

**(이미지: 손실(L)을 최소화하는 가중치(W)를 찾는 것이 목표임을 보여주는 다이어그램)**

***

### **페이지 393**

**네트워크 파라미터 표현하기**

*   우리는 네트워크의 파라미터를 다음과 같이 표현할 수 있습니다:
    *   W = {W¹, W², ..., Wᴸ}
*   **여기서**
    *   L은 네트워크의 레이어 수입니다.
    *   W¹, W², ...는 각 레이어의 가중치 행렬입니다.
*   따라서 지금부터 우리가 W라고 말할 때는, 모든 레이어에 걸친 **전체 파라미터 집합**을 의미합니다.

**(이미지: 전체 네트워크 파라미터 W를 최소화하는 것이 목표임을 나타내는 수식과 다이어그램)**

***

### **페이지 394**

**네트워크 함수 표현하기**

*   입력 이미지 x에서 출력 예측 ŷ까지의 전체 과정을 함수 **f**로 나타내 봅시다.
*   이전 강의에서 **선형 분류기**에 대해 이야기할 때 비슷한 다이어그램을 본 적이 있습니다.
    *   아이디어는 동일하지만, 이제 f는 **신경망**을 나타냅니다.
*   네트워크는 파라미터 W를 가지므로, 출력을 다음과 같이 표현할 수 있습니다:
    *   ŷ = f(x, W)

**(이미지: 전체 신경망을 하나의 함수 f로 추상화하여 표현한 다이어그램)**

***

### **페이지 395**

**복습: 특징 추출 없는 선형 분류기**

*   먼저 별도의 특징 추출기 없이 **종단간 선형 분류기**를 고려해 봅시다.
*   이 모델에서는 명시적인 특징 추출 단계 없이 **원시 이미지 픽셀**로부터 직접 학습하여, 이를 바로 **클래스 점수**로 매핑합니다.

**(이미지: 입력 이미지가 학습된 선형 분류기(f)를 직접 통과하여 클래스 점수를 출력하는 다이어그램)**

***

### **페이지 396**

**예측과 파라미터의 함수로서의 손실**

*   우리는 손실을 나타내기 위해 L을 사용해왔습니다. 더 자세히 살펴보면, 손실은 **예측**과 **실제 레이블** 모두에 의존합니다: L = L(ŷ, y)
*   하지만 예측 ŷ 자체는 네트워크로부터 나오므로 — 입력 x와 파라미터 W의 함수입니다 — 우리는 이를 다음과 같이 표현할 수 있습니다: L = L(f(x, W), y)
*   이는 손실이 네트워크 함수 f를 통해 **가중치 W에 간접적으로 의존**한다는 것을 보여줍니다.

**(이미지 및 수식: 손실 L이 예측 ŷ와 실제 레이블 y의 함수이며, ŷ는 다시 x와 W의 함수이므로, 결국 L은 x, W, y의 함수임을 보여주는 다이어그램과 수식)**

***

### **페이지 397**

**단일 샘플 참조하기**

*   이제, 우리가 보고 있는 입력 이미지가 데이터셋의 **첫 번째 이미지**라고 해봅시다.
    *   이를 x₁으로, 실제 레이블을 y₁, 예측된 출력을 ŷ₁, 이 샘플에 대한 손실을 L₁으로 나타낼 수 있습니다.
*   이는 훈련 데이터의 **첫 번째 샘플**에 대해 네트워크가 얼마나 틀렸는지를 나타냅니다.

**(이미지 및 수식: 단일 샘플(x₁, y₁)에 대한 손실(L₁) 계산 과정을 보여주는 다이어그램과 수식)**

***

### **페이지 398**

**뭔가 빠졌다…**

*   잠깐만요... 여기서 뭔가 빠지지 않았나요?
*   우리가 단 **하나의 고양이 이미지**만 보고 네트워크를 훈련시키나요? 물론 아닙니다.
*   단일 예시로부터 학습하는 것은 결코 충분하지 않을 것입니다. 네트워크는 "고양이"가 어떻게 생겼는지 진정으로 이해하기 위해 다른 자세, 조명 조건, 배경에 있는 **많은 이미지**를 봐야 합니다.
*   그리고 고양이뿐만 아니라 — 개, 배, 그리고 다른 많은 클래스들을 구별하는 법을 배우기 위해 그것들도 봐야 합니다.

**(이미지 및 수식: 단일 샘플(x₁, y₁)에 대한 손실(L₁) 계산 과정을 보여주는 다이어그램과 수식)**

***

### **페이지 399**

**데이터셋으로부터 학습하기**

*   그래서 우리는 **많은 데이터 샘플** — 다시 말해, **데이터셋**이 필요합니다.
*   데이터셋은 많은 (N개의) 입력과 레이블 쌍으로 구성됩니다: { (xᵢ, yᵢ) }ᵢ₌₁ᴺ
    *   여기서 각 xᵢ는 입력 이미지이고, yᵢ는 그에 해당하는 실제 레이블입니다.
*   우리는 이 데이터셋으로부터 학습하여 신경망을 훈련시키고, 이를 통해 새롭고 보지 못한 데이터에 일반화할 수 있습니다.

**(이미지: 여러 개의 레이블된 고양이와 개 이미지로 구성된 데이터셋을 보여주고, 이 데이터셋의 샘플(xᵢ)이 신경망(f)에 입력되는 과정을 보여주는 다이어그램)**

***

### **페이지 400**

**데이터셋 전체에 대한 총 손실**

*   각 샘플은 자체적인 손실에 기여합니다: Lᵢ = L(f(xᵢ, W), yᵢ)
*   네트워크를 훈련시키기 위해, 우리는 모든 개별 손실을 결합하여 전체 데이터셋에 대한 **총 (평균) 손실**을 계산합니다.

**(수식 및 이미지: 데이터셋의 모든 샘플에 대한 손실의 평균으로 총 손실(L_total)을 계산하는 공식과 과정을 보여주는 다이어그램)**

***

### **페이지 401**

**데이터셋 전체에 대한 총 손실**

*   이것은 네트워크가 단일 이미지가 아닌 **전체 훈련 세트**에서 얼마나 잘 수행하는지를 나타냅니다.
*   우리의 목표는 파라미터 W를 조정하여 이 **총 손실을 최소화**하는 것입니다.

**(이미지 및 수식: 총 손실을 최소화하는 가중치 W를 찾는 것이 목표임을 보여주는 다이어그램과 수식)**

***

### **페이지 402**

**데이터셋 전체에 대한 총 손실**

*   이것은 네트워크가 단일 이미지가 아닌 **전체 훈련 세트**에서 얼마나 잘 수행하는지를 나타냅니다.
*   우리의 목표는 파라미터 W를 조정하여 이 **총 손실을 최소화**하는 것입니다.

**(이미지 및 수식: argmin을 사용하여 총 손실을 최소화하는 최적의 가중치 W*를 찾는 것을 공식으로 표현)**

***

### **페이지 403**

**최적화**

**(이미지: 데이터 과학을 상징하는 그래픽)**

***

### **페이지 404**

**함수의 최솟값 찾기**

*   주어진 함수:
    *   f(x) = (x - 2)²
*   함수의 최솟값은 무엇인가요?
*   그리고 어떤 x 값에서 그 값이 발생하나요?

***

### **페이지 405**

**함수의 최솟값 찾기**

*   우리는 이미 f(x) = (x - 2)²가 볼록한(아래로 열린) **포물선**을 형성한다는 것을 알고 있습니다.
*   이 함수는 x = 2를 중심으로 대칭이고 제곱 항은 항상 음수가 아니므로, 제곱 항이 0이 될 때 함수는 **최솟값**에 도달합니다.
    *   모든 x에 대해 (x - 2)² ≥ 0
*   곡선은 볼록하며, 가장 낮은 지점(꼭짓점)은 (2,0)에 위치합니다.

**(수식 및 그래프: f(x) = (x - 2)² 함수의 그래프와 최솟값을 찾는 과정을 수식으로 표현)**

***

### **페이지 406**

**함수가 복잡해질 때**

*   이제 훨씬 더 **복잡한** 함수 — 방정식만 보고는 모양이나 속성을 쉽게 이해할 수 없는 함수 — 를 고려해 봅시다.
*   이러한 경우:
    *   함수는 여러 개의 **지역 최솟값**과 최댓값을 가질 수 있습니다.
    *   단순히 관찰만으로는 실제 (전역) 최솟값이 어디에 있는지 식별하기 어려워집니다.
    *   f(x)를 최소화하는 점 x를 찾기 위한 **체계적인 방법**이 필요합니다.

**(수식 및 그래프: 매우 복잡한 함수의 예시와 그 그래프 위에 물음표를 표시하여 최솟값을 찾는 것이 어려움을 보여줌)**

***

### **페이지 407**

**함수가 복잡해질 때**

*   함수가 매우 비선형적이고 복잡해지면:
    *   많은 골짜기와 봉우리(지역 최솟값/최댓값)를 가질 수 있습니다.
    *   실제 전역 최솟값을 찾으려면 이론적으로 무한한 영역을 탐색해야 하며, 이는 **계산적으로 불가능**합니다.
*   따라서, 우리는 **경사 하강법**과 같은 **반복적인 최적화 방법**을 사용합니다.

**(이미지: 복잡하고 여러 개의 지역 최솟값을 가진 함수의 그래프)**

***

### **페이지 408**

**산 위에 서 있다고 상상해보세요**

*   풍경이 이렇게 보입니다. 당신은 이 산 어딘가에 서 있습니다.
    *   어느 길로 내려갈지 어떻게 결정하시겠습니까?
    *   그냥 추측하시겠습니까 — 아니면 땅이 가장 가파르게 아래로 경사진 방향을 따라가시겠습니까?

**(이미지: 산의 풍경 사진)**

***

### **페이지 409**

**가장 가파른 길을 따라 내려가기**

*   당신은 아마도 발밑의 땅이 **가장 가파르게 아래로 경사진** 방향으로 걷기 시작할 것입니다.
*   그것이 당신의 높이를 줄이는 가장 빠른 방법입니다 — 그리고 최적화에서, 이것이 바로 **그래디언트**가 우리에게 하라고 알려주는 것입니다.

**(이미지: 산을 내려가는 사람의 모습과 가장 가파른 경사 방향을 나타내는 화살표)**

***

### **페이지 410**

**함수가 복잡해질 때**

*   당신은 산을 올라 이제 어떤 지점 x₀에 서 있습니다.
    *   이 위치에서, 당신은 당신의 높이를 측정할 수 있습니다 — 그것이 f(x₀)입니다.
*   하지만 당신은 f(x)의 **전체 풍경을 볼 수 없습니다**.
*   여기서부터 — 전체 산을 보지 않고 —
    *   **어떤 방향이 당신을 내리막길로 이끌지 어떻게 결정할 수 있을까요?**

**(그래프: 함수 위의 한 점 (x₀, f(x₀))을 표시)**

***

### **페이지 411**

**함수가 복잡해질 때**

*   당신은 산을 올라 이제 어떤 지점 x₀에 서 있습니다.
    *   이 위치에서, 당신은 당신의 높이를 측정할 수 있습니다 — 그것이 f(x₀)입니다.
*   하지만 당신은 f(x)의 **전체 풍경을 볼 수 없습니다**.
*   여기서부터 — 전체 산을 보지 않고 —
    *   **어떤 방향이 당신을 내리막길로 이끌지 어떻게 결정할 수 있을까요?**

**(그래프: 전체 함수 곡선을 보여주지 않고 현재 지점만 표시하여, 전체 풍경을 알 수 없음을 강조)**

***

### **페이지 412**

**함수가 복잡해질 때**

*   당신은 산을 올라 이제 어떤 지점 x₀에 서 있습니다.
    *   이 위치에서, 당신은 당신의 높이를 측정할 수 있습니다 — 그것이 f(x₀)입니다.
*   하지만 당신은 **전체 풍경을 볼 수 없습니다**.
*   여기서부터 — 전체 산을 보지 않고 —
    *   **어떤 방향이 당신을 내리막길로 이끌지 어떻게 결정할 수 있을까요?**

*   왼쪽 또는 오른쪽 — 어느 쪽이 당신을 내리막길로 이끌까요?

**(그래프: 현재 지점에서 왼쪽과 오른쪽 방향을 화살표로 표시)**

***

### **페이지 413**

**오른쪽으로 한 걸음**

*   **오른쪽**으로 작은 한 걸음을 내디뎠다고 가정해 봅시다 — 거리 h만큼. 이제 당신은 x₀ + h에 있습니다.
*   높이를 다시 확인합니다.

**(그래프: x₀에서 오른쪽으로 h만큼 이동한 지점 x₀ + h를 표시)**

***

### **페이지 414**

**오른쪽으로 한 걸음**

*   **오른쪽**으로 작은 한 걸음을 내디뎠다고 가정해 봅시다 — 거리 h만큼. 이제 당신은 x₀ + h에 있습니다.
*   높이를 다시 확인합니다: f(x₀ + h)
*   그리고 값이 f(x₀)에 비해 **증가**했음을 알아차립니다.
*   이는 오른쪽으로 움직이는 것이 당신을 **오르막길**로 가게 했다는 것을 의미합니다.

**(그래프: x₀ + h에서의 함수 값 f(x₀ + h)가 f(x₀)보다 높음을 보여줌)**

***

### **페이지 415**

**왼쪽으로 한 걸음**

*   이제, **왼쪽**으로 작은 한 걸음을 내디뎌 봅시다, 다시 거리 h만큼. 당신은 x₀ - h에 있습니다.
*   높이를 확인합니다.

**(그래프: x₀에서 왼쪽으로 h만큼 이동한 지점 x₀ - h를 표시)**

***

### **페이지 416**

**왼쪽으로 한 걸음**

*   이제, **왼쪽**으로 작은 한 걸음을 내디뎌 봅시다, 다시 거리 h만큼. 당신은 x₀ - h에 있습니다.
*   높이를 확인합니다: f(x₀ - h)
*   그리고 이번에는 값이 f(x₀)에 비해 **감소**했습니다.
*   왼쪽으로 움직이는 것이 당신을 **내리막길**로 가게 했습니다 — 그래서 함수는 그 방향으로 감소합니다.

**(그래프: x₀ - h에서의 함수 값 f(x₀ - h)가 f(x₀)보다 낮음을 보여줌)**

***

### **페이지 417**

**왼쪽으로 한 걸음**

*   이제, **왼쪽**으로 작은 한 걸음을 내디뎌 봅시다, 다시 거리 h만큼. 당신은 x₀ - h에 있습니다.
*   높이를 확인합니다: f(x₀ - h)
*   그리고 이번에는 값이 f(x₀)에 비해 **감소**했습니다.
*   왼쪽으로 움직이는 것이 당신을 **내리막길**로 가게 했습니다 — 그래서 함수는 그 방향으로 감소합니다.

**(수식: f(x₀ + h) - f(x₀ - h)의 부호에 따라 왼쪽 또는 오른쪽으로 이동해야 함을 보여줌)**

***

### **페이지 418**

**양방향 비교하기**

*   f(x)가 x₀ 근처에서 어떻게 변하는지 알기 위해, 우리는 양쪽의 함수 값을 비교합니다.
*   높이의 차이는: f(x₀ + h) - f(x₀ - h)
*   그리고 그들 사이의 수평 거리는 2h입니다.
*   따라서, x₀ 주변의 **평균 변화율**은 다음과 같습니다:
    *   (f(x₀ + h) - f(x₀ - h)) / 2h

**(그래프 및 수식: 평균 변화율을 계산하는 과정을 보여줌)**

***

### **페이지 419**

**양방향 비교하기**

*   f(x)가 x₀ 근처에서 어떻게 변하는지 알기 위해, 우리는 양쪽의 함수 값을 비교합니다.
*   높이의 차이는: f(x₀ + h) - f(x₀ - h)
*   그리고 그들 사이의 수평 거리는 2h입니다.
*   따라서, x₀ 주변의 **평균 변화율**은 다음과 같습니다.
*   이제, 단계 h를 **점점 더 작게** 만들면 (h → 0⁺)

**(그래프 및 수식: 평균 변화율의 극한(미분)을 계산하는 과정을 보여줌)**

***

### **페이지 420**

**양방향 비교하기**

*   f(x)가 x₀ 근처에서 어떻게 변하는지 알기 위해, 우리는 양쪽의 함수 값을 비교합니다.
*   높이의 차이는: f(x₀ + h) - f(x₀ - h)
*   그리고 그들 사이의 수평 거리는 2h입니다.
*   따라서, x₀ 주변의 **평균 변화율**은 다음과 같습니다.
*   이제, 단계 h를 **점점 더 작게** 만들면 (h → 0⁺)
*   이 표현은 x₀에서의 함수의 **실제 기울기**에 접근합니다:
    *   이는 f(x)가 x₀에서 **얼마나 빨리** 그리고 **어떤 방향으로** 변하는지를 알려줍니다.

**(그래프 및 수식: 미분 계수가 함수의 기울기를 나타냄을 설명)**

***

### **페이지 421**

**f'(x₀)의 의미**

*   도함수 f'(x₀)는 우리에게 두 가지를 알려줍니다:
*   **변화의 방향**
    *   f'(x₀)의 **부호**는 함수가 증가하는지 감소하는지를 나타냅니다.
        *   f'(x₀) > 0 : 함수가 증가 → 내리막길로 가려면 **왼쪽**으로 가야 합니다.
        *   f'(x₀) < 0 : 함수가 감소 → 내리막길로 가려면 **오른쪽**으로 가야 합니다.
*   **가파름 (크기)**
    *   **절댓값** |f'(x₀)|는 기울기가 **얼마나 가파른지**를 나타냅니다 —
    *   즉, f(x)가 x₀ 근처에서 얼마나 빠르게 변하는지.

**(그래프: 그래디언트 방향과 그 크기(가파름)를 시각적으로 보여주는 그림)**

***

### **페이지 422**

**가장 가파른 하강 방향으로 이동하기**

*   f'(x₀)가 기울기의 **방향과 가파름**을 알려주므로, 가능한 한 빨리 내리막길로 가려면 다음과 같이 움직여야 합니다:
    *   그래디언트의 **반대 방향**으로 (f(x)를 감소시키고 싶기 때문에).
    *   그 가파름에 **비례하는 스텝 크기**로.

**(그래프: 그래디언트의 반대 방향(음의 그래디언트 방향)으로 이동해야 함을 보여주는 그림)**

***

### **페이지 423**

**가장 가파른 하강 방향으로 이동하기**

*   f'(x₀)가 기울기의 **방향과 가파름**을 알려주므로, 가능한 한 빨리 내리막길로 가려면 다음과 같이 움직여야 합니다:
    *   그래디언트의 **반대 방향**으로 (f(x)를 감소시키고 싶기 때문에).
    *   그 가파름에 **비례하는 스텝 크기**로.
*   **수학적으로:**
    *   x₁ = x₀ - ηf'(x₀)
*   **η**: 스텝 크기 (또는 학습률).

**(그래프 및 수식: 그래디언트 하강법의 한 스텝을 수식과 그래프로 보여줌)**

***

### **페이지 424**

**하강 반복하기**

*   우리는 한 걸음 내려왔습니다: x₁ = x₀ - ηf'(x₀)
*   하지만 이것은 단 **한 번의 업데이트**일 뿐 — 최솟값은 여전히 더 멀리 있을 수 있습니다.
*   따라서, 우리는 이 과정을 **반복**합니다:
    *   x_t+1 = x_t - ηf'(x_t)

**(그래프: 그래디언트 하강법을 여러 번 반복하여 점차 최솟값으로 이동하는 과정을 보여줌)**

***

### **페이지 425**

**하강 반복하기**

*   우리는 한 걸음 내려왔습니다.
*   하지만 이것은 단 **한 번의 업데이트**일 뿐 — 최솟값은 여전히 더 멀리 있을 수 있습니다.
*   따라서, 우리는 이 과정을 **반복**합니다.
*   각 업데이트는 우리를 골짜기에 **더 가깝게** 만듭니다.
*   기울기 f'(x_t)가 매우 작아지면 (0에 가까워지면),
*   우리는 땅이 평평해지는 **최솟값**에 도달한 것입니다.

**(그래프: 최솟값에 도달하여 기울기가 0에 가까워지는 지점을 보여줌)**

***

### **페이지 426**

**하강 반복하기**

*   우리는 한 걸음 내려왔습니다.
*   하지만 이것은 단 **한 번의 업데이트**일 뿐 — 최솟값은 여전히 더 멀리 있을 수 있습니다.
*   따라서, 우리는 이 과정을 **반복**합니다.
*   각 업데이트는 우리를 골짜기에 **더 가깝게** 만듭니다.
*   기울기 f'(x_t)가 매우 작아지면 (0에 가까워지면),
*   우리는 땅이 평평해지는 **최솟값**에 도달한 것입니다.

**(그래프: 복잡한 함수 위에서 그래디언트 하강법이 지역 최솟값으로 수렴하는 과정을 보여줌)**

***

### **페이지 427**

**경사 하강법의 한계**

*   경사 하강법은 기울기가 0이 되는 지점을 향해 단계적으로 이동합니다.
*   **하지만:**
    *   이 지점이 **전역 최솟값**이라는 보장은 없습니다.
    *   더 높은 지형으로 둘러싸인 작은 골짜기인 **지역 최솟값**에 쉽게 갇힐 수 있습니다.
    *   일단 갇히면, 그래디언트가 0이 되므로 알고리즘은 더 깊은 골짜기가 다른 곳에 존재할 수 있음에도 불구하고 **움직임을 멈춥니다**.

**(그래프: 전역 최솟값이 아닌 지역 최솟값에 수렴한 경우를 보여줌)**

***

### **페이지 428**

**데이터셋 전체에 대한 총 손실**

*   이것은 네트워크가 단일 이미지가 아닌 **전체 훈련 세트**에서 얼마나 잘 수행하는지를 나타냅니다.
*   우리의 목표는 파라미터 W를 조정하여 이 **총 손실을 최소화**하는 것입니다.

**(이미지 및 수식: 총 손실을 최소화하는 것이 목표임을 다시 한번 강조하는 다이어그램)**

***

### **페이지 429**

**모델 훈련에 경사 하강법 적용하기**

*   우리의 궁극적인 목표는 총 손실을 최소화하는 파라미터 집합 W*를 찾는 것입니다.
*   L_total(W)를 최소화하기 위해, 우리는 W에 대한 **도함수(그래디언트)**를 계산합니다.
*   그런 다음 **가장 가파른 하강** 방향으로 W를 **업데이트**합니다.

**(수식: 모델 훈련에 경사 하강법을 적용하는 전체 과정을 수식으로 표현)**

***

### **페이지 430**

**실제적인 도전 과제**

*   이론적으로, 우리는 **모든** 훈련 샘플을 사용하여 총 그래디언트를 계산합니다.
*   **하지만 실제로는:**
    *   현대 데이터셋은 **수백만 또는 수십억** 개의 샘플을 포함합니다 (N > 10⁹).
    *   모든 데이터를 한 번에 메모리에 로드하는 것은 **불가능**합니다.
    *   모든 업데이트마다 전체 그래디언트를 계산하는 것은 **극도로 느릴 것**입니다.
*   데이터셋이 커짐에 따라, **전체 배치 경사 하강법**은 계산적으로 불가능해집니다.

**(수식: 전체 배치 그래디언트 계산 공식)**

***

### **페이지 431**

**확률적 (미니배치) 경사 하강법**

*   그렇다면, 모든 데이터를 한 번에 사용하지 않고 어떻게 효율적으로 훈련할 수 있을까요?
    *   각 단계에서 데이터의 **작은 무작위 부분 집합**만 사용하여 실제 그래디언트를 **근사**합니다.
    *   이 접근법을 **확률적 경사 하강법(SGD)** 또는 **미니배치 경사 하강법**이라고 합니다.
*   그런 다음 다음과 같이 파라미터를 업데이트합니다.
*   **장점:**
    *   **훨씬 적은 메모리**를 필요로 합니다.
    *   **더 자주** 업데이트합니다 (더 빠른 학습).
    *   **무작위성**을 추가하여 지역 최솟값을 탈출하는 데 도움이 됩니다.
*   실제로, 거의 모든 현대 신경망은 이런 방식으로 훈련됩니다.

**(수식 및 이미지: 미니배치 경사 하강법의 개념과 장점을 설명하는 다이어그램)**

***

### **페이지 432**

**확률적 vs. 전체 배치 경사 하강법**

*   **확률적 경사 하강법(미니배치)**과 **경사 하강법(전체 배치)**을 비교할 때,
    *   **전체 배치 GD**는 모든 데이터로 계산된 실제 그래디언트를 따라 최솟값을 향해 부드럽게 이동합니다.
    *   반면에 **확률적(미니배치) GD**는 각 단계에서 데이터의 일부만 사용하여 업데이트하므로 더 **지그재그 또는 노이즈가 많은** 궤적을 유발합니다.
    *   흥미롭게도, 이 **노이즈가 많은 동작**은 때때로 **유리**할 수 있으며, 모델이 지역 최솟값을 탈출하고 **더 나은 전역 최솟값**에 도달하는 데 도움이 됩니다.

**(이미지: 전체 배치 GD의 부드러운 경로와 확률적 GD의 지그재그 경로를 비교하는 그림)**

***

### **페이지 433**

**SGD의 지역 최솟값**

*   **확률적 경사 하강법(SGD)**의 **지역 최솟값 문제**를 살펴봅시다.
*   그림에서 볼 수 있듯이, 모델은 지역 최솟값에 갇힐 수 있습니다.
*   여기서 왼쪽으로 움직여야 할지 오른쪽으로 움직여야 할지 결정할 수 있을까요?

**(그래프: 지역 최솟값에 갇힌 지점을 보여주는 그림)**

***

### **페이지 434**

**SGD가 지역 최솟값에 갇히는 이유**

*   SGD는 지역 최솟값에서 움직일 수 없습니다 — 왜냐하면 그 지점에서 **그래디언트(도함수)가 0**이기 때문입니다.
*   결과적으로, 알고리즘은 **멈추고 갇히게** 되어, 스스로 지역 최솟값을 탈출할 수 없게 됩니다.

**(그래프: 지역 최솟값에서 그래디언트가 0이 되어 움직일 수 없음을 보여주는 그림)**

***

### **페이지 435**

**지역 최솟값과 안장점의 문제**

*   **지역 최솟값**과 **안장점** 모두 최적화 중에 문제를 일으킵니다.
*   이 지점들에서 **그래디언트가 0**이 되어, 알고리즘이 전역 최솟값에 도달하지 않았음에도 불구하고 **파라미터 업데이트를 중단**하게 만듭니다.

**(이미지: 손실 함수 표면의 지역 최솟값과 안장점에서 그래디언트가 0이 되어 갇히는 모습을 보여주는 그림)**

***

### **페이지 436**

**지역 최솟값과 안장점의 문제**

*   **지역 최솟값**과 **안장점** 모두 최적화 중에 문제를 일으킵니다.
*   이 지점들에서 **그래디언트가 0**이 되어, 알고리즘이 전역 최솟값에 도달하지 않았음에도 불구하고 **파라미터 업데이트를 중단**하게 만듭니다.

**(이미지 및 수식: 2차원에서의 안장점(saddle point)을 보여주는 3D 그래프와 그 지점에서 편도함수가 0이 됨을 보여주는 수식)**

***

### **페이지 437**

**미니배치 경사 하강법의 노이즈가 많은 업데이트**

*   그래디언트가 전체 데이터셋 대신 **미니배치**로부터 계산되기 때문에, 추정된 그래디언트는 **노이즈가 많을** 수 있습니다.
    *   단계마다 변동합니다. 이 무작위성은 최적화 경로를 **지그재그**로 만들고 수렴을 **덜 안정적**으로 만들 수 있습니다.

**(이미지: 미니배치로 인한 노이즈가 많은 그래디언트 때문에 최적화 경로가 불안정하게 움직이는 모습을 보여주는 그림)**

***

### **페이지 438**

**불균일한 곡률이 최적화에 미치는 영향**

*   손실 표면이 각 방향에 따라 **다른 곡률**을 가질 때 — 한 축에서는 가파르고 다른 축에서는 평평(얕음)할 때 — 최적화는 불균일하게 동작합니다:
    *   얕은 방향을 따라서는 **느리게** 움직입니다.
    *   가파른 방향을 따라서는 **빠르게 진동**합니다 (지터링).

**(이미지: 타원형 등고선을 가진 손실 표면에서 최적화가 어떻게 진행될지 보여주는 그림)**

***

### **페이지 439**

**불균일한 곡률이 최적화에 미치는 영향**

*   손실 표면이 각 방향에 따라 **다른 곡률**을 가질 때 — 한 축에서는 가파르고 다른 축에서는 평평(얕음)할 때 — 최적화는 불균일하게 동작합니다:
    *   얕은 방향을 따라서는 **느리게** 움직입니다.
    *   가파른 방향을 따라서는 **빠르게 진동**합니다 (지터링).

**(이미지: 이전 그림에 진동하며 느리게 진행하는 최적화 경로를 추가한 그림)**

***

### **페이지 440**

**모멘텀: 지역 최솟값과 안장점 극복하기**

*   지역 최솟값이나 안장점에서 **그래디언트가 0**이 되면, 옵티마이저는 움직임을 멈춥니다.
*   진행을 계속하기 위해, 우리는 **모멘텀**을 추가할 수 있습니다 — 파라미터를 **이전의 움직임 방향**으로 계속 나아가게 하는 물리적 운동에서 영감을 받은 개념입니다.

**(이미지: 지역 최솟값과 안장점에서 갇히는 문제를 보여주는 그림)**

***

### **페이지 441**

**모멘텀: 지역 최솟값과 안장점 극복하기**

*   지역 최솟값이나 안장점에서 **그래디언트가 0**이 되면, 옵티마이저는 움직임을 멈춥니다.
*   진행을 계속하기 위해, 우리는 **모멘텀**을 추가할 수 있습니다 — 파라미터를 **이전의 움직임 방향**으로 계속 나아가게 하는 물리적 운동에서 영감을 받은 개념입니다.

**(이미지: 이전 움직임의 방향(관성)을 화살표로 표시)**

***

### **페이지 442**

**모멘텀: 지역 최솟값과 안장점 극복하기**

*   지역 최솟값이나 안장점에서 **그래디언트가 0**이 되면, 옵티마이저는 움직임을 멈춥니다.
*   진행을 계속하기 위해, 우리는 **모멘텀**을 추가할 수 있습니다 — 파라미터를 **이전의 움직임 방향**으로 계속 나아가게 하는 물리적 운동에서 영감을 받은 개념입니다.

**(이미지: 이전 움직임의 관성(모멘텀) 덕분에 그래디언트가 0인 지점을 통과하여 계속 움직이는 모습을 보여줌)**

***

### **페이지 443**

**0 그래디언트를 넘어서는 모멘텀의 이점**

*   모멘텀은 **지역 최솟값**과 **안장점**을 탈출하는 데 도움이 될 뿐만 아니라, **그래디언트 노이즈**와 **나쁜 조건(poor conditioning)** 하에서의 최적화도 개선합니다.
*   노이즈가 많은 업데이트를 부드럽게 하고 얕은 방향을 따른 수렴을 가속화합니다.

**(이미지: SGD+모멘텀이 지역 최솟값, 안장점, 나쁜 조건, 그래디언트 노이즈 문제를 어떻게 완화하는지 시각적으로 보여주는 그림)**

***

### **페이지 444**

**모멘텀을 사용한 SGD: 속도 누적하기**

*   표준 **확률적 경사 하강법(SGD)**과 비교하여, **모멘텀을 사용한 SGD**는 **현재 그래디언트**와 **이전 속도**를 결합하여 파라미터를 업데이트합니다.
*   속도 항은 일관된 방향과 속도를 유지하기 위해 과거의 움직임을 누적합니다.

**(수식: 표준 SGD의 업데이트 공식)**

***

### **페이지 445**

**모멘텀을 사용한 SGD: 속도 누적하기**

*   표준 **확률적 경사 하강법(SGD)**과 비교하여, **모멘텀을 사용한 SGD**는 **현재 그래디언트**와 **이전 속도**를 결합하여 파라미터를 업데이트합니다.
*   속도 항은 일관된 방향과 속도를 유지하기 위해 과거의 움직임을 누적합니다.

**(수식 및 코드: 표준 SGD의 업데이트 공식과 이를 구현한 간단한 Python 코드)**

***

### **페이지 446**

**모멘텀을 사용한 SGD: 속도 누적하기**

*   표준 **확률적 경사 하강법(SGD)**과 비교하여, **모멘텀을 사용한 SGD**는 **현재 그래디언트**와 **이전 속도**를 결합하여 파라미터를 업데이트합니다.
*   속도 항은 일관된 방향과 속도를 유지하기 위해 과거의 움직임을 누적합니다.
    *   v_t: 속도 (누적된 그래디언트 방향)
    *   ρ: 모멘텀 계수 (예: 0.9 또는 0.99)
    *   η: 학습률

**(수식 및 코드: SGD와 SGD+모멘텀의 업데이트 공식을 나란히 비교)**

***

### **페이지 447**

**모멘텀을 사용한 SGD: 속도 누적하기**

*   표준 **확률적 경사 하강법(SGD)**과 비교하여, **모멘텀을 사용한 SGD**는 **현재 그래디언트**와 **이전 속도**를 결합하여 파라미터를 업데이트합니다.
*   속도 항은 일관된 방향과 속도를 유지하기 위해 과거의 움직임을 누적합니다.

**(수식: SGD+모멘텀의 업데이트 공식을 전개하여 현재 그래디언트 항과 모멘텀 항으로 분리해서 보여줌)**

***

### **페이지 448**

**모멘텀을 사용한 SGD: 속도 누적하기**

*   표준 **확률적 경사 하강법(SGD)**과 비교하여, **모멘텀을 사용한 SGD**는 **현재 그래디언트**와 **이전 속도**를 결합하여 파라미터를 업데이트합니다.
*   속도 항은 일관된 방향과 속도를 유지하기 위해 과거의 움직임을 누적합니다.

**(수식 및 코드: SGD와 SGD+모멘텀의 업데이트 공식과 각각을 구현한 Python 코드를 나란히 비교)**

***

### **페이지 449**

**모멘텀을 사용한 SGD: 속도 누적하기**

*   표준 **확률적 경사 하강법(SGD)**과 비교하여, **모멘텀을 사용한 SGD**는 **현재 그래디언트**와 **이전 속도**를 결합하여 파라미터를 업데이트합니다.
*   속도 항은 일관된 방향과 속도를 유지하기 위해 과거의 움직임을 누적합니다.

**(이미지: 모멘텀이 어떻게 작동하는지 보여주는 다이어그램. 실제 움직임은 음의 그래디언트 방향과 모멘텀 방향의 합으로 결정됨)**

***

### **페이지 500**

**모멘텀을 사용한 SGD: 속도 누적하기**

*   표준 **확률적 경사 하강법(SGD)**과 비교하여, **모멘텀을 사용한 SGD**는 **현재 그래디언트**와 **이전 속도**를 결합하여 파라미터를 업데이트합니다.

**(이미지: 일반적인 경사 하강법의 진동하는 경로와, 모멘텀을 사용했을 때 수직 방향의 진동은 상쇄되고 수평 방향으로의 진행이 가속화되는 경로를 비교)**

***

### **페이지 501**

**RMSProp: 적응형 학습률 방법**

*   최적화 방법의 또 다른 갈래는 **RMSProp**이며, 이는 **학습률을 적응적으로 조정**함으로써 **불균일한 그래디언트 스케일**과 **진동**의 문제를 해결합니다.
*   각 파라미터에 대해, RMSProp은 업데이트 단계를 **최근 그래디언트 크기**의 **제곱 평균 제곱근(RMS)**으로 나눕니다. 따라서 큰 그래디언트를 가진 파라미터는 더 작은 스텝을, 작은 그래디언트를 가진 파라미터는 더 큰 스텝을 밟게 됩니다.
    *   **아이디어:**
        *   **가파른 축**: 큰 그래디언트 → 작은 스텝
        *   **평평한 축**: 작은 그래디언트 → 큰 스텝

**(수식: SGD+모멘텀과 RMSProp의 업데이트 공식을 나란히 비교)**

***

### **페이지 502**

**RMSProp: 적응형 학습률 방법**

*   최적화 방법의 또 다른 갈래는 **RMSProp**이며, 이는 **학습률을 적응적으로 조정**함으로써 **불균일한 그래디언트 스케일**과 **진동**의 문제를 해결합니다.
*   각 파라미터에 대해, RMSProp은 업데이트 단계를 **최근 그래디언트 크기**의 **제곱 평균 제곱근(RMS)**으로 나눕니다.

**(수식 및 코드: SGD+모멘텀과 RMSProp의 업데이트 공식과 각각을 구현한 Python 코드를 나란히 비교)**

***

### **페이지 503**

**RMSProp: 적응형 학습률 방법**

*   수직 방향의 그래디언트는 더 높고 수평 방향의 그래디언트는 더 낮아서 최적의 해를 찾는 전체 검색 과정이 느려지는 것을 볼 수 있습니다.
*   RMSprop을 사용하면 더 나은 검색 "궤적"을 찾아 이 문제를 해결합니다.
    *   **아이디어:**
        *   **가파른 축**: 큰 그래디언트 → 작은 스텝
        *   **평평한 축**: 작은 그래디언트 → 큰 스텝

**(이미지: 일반적인 경사 하강법의 진동하는 경로와 RMSProp의 더 직접적인 경로를 비교)**

***

### **페이지 504**

**Adam 옵티마이저의 동기**

*   **모멘텀**과 **RMSProp**은 모두 SGD를 개선했지만 — **다른 방식**으로 개선했습니다.
    *   **모멘텀**은 **관성**을 추가하여, 과거 그래디언트를 누적하여 노이즈가 많은 업데이트를 부드럽게 하고 평평한 영역을 통과하는 데 도움이 됩니다.
    *   **RMSProp**은 최근 그래디언트의 크기에 따라 **학습률을 조정**하여 수렴을 안정화하는 데 도움이 됩니다.
*   하지만, 각 방법은 문제의 **한쪽 측면**만 다룹니다.
    → **Adam (Adaptive Moment Estimation)**은 **두 가지 이점을 모두 결합**하기 위해 제안되었습니다.
        *   그래디언트의 **1차 모멘트(평균)**와 **2차 모멘트(분산)**를 모두 추적합니다.

***

### **페이지 505**

**Adam 옵티마이저의 동기**

*   **Adam (Adaptive Moment Estimation)**은 두 가지 이점을 모두 결합하기 위해 제안되었습니다.
    *   그래디언트의 **1차 모멘트(평균)**와 **2차 모멘트(분산)**를 모두 추적합니다.

**(이미지: GD, SGD, Momentum, Adagrad, RMSProp, Adam 등 다양한 옵티마이저들의 관계와 발전을 보여주는 한국어 다이어그램)**

***

### **페이지 506**

**Adam 옵티마이저: 모멘텀과 RMSProp 결합하기**

*   Adam은 다음을 결합합니다:
    *   **모멘텀** → 그래디언트의 이동 평균을 유지 (1차 모멘트)
    *   **RMSProp** → 제곱된 그래디언트의 이동 평균을 유지 (2차 모멘트)
    *   이를 통해 업데이트가 **방향적으로 일관성** 있고 **스케일에 적응적**이게 됩니다.

**(수식: 모멘텀과 RMSProp의 핵심 업데이트 공식을 나란히 보여줌)**

***

### **페이지 507**

**Adam의 편향 보정**

*   Adam에서, β₁ = 0.9, β₂ = 0.999
*   그리고 이동 평균 v_t, s_t는 0으로 초기화됩니다.
*   충분한 통계가 누적되기 전에, 초기 추정치(특히 t = 0에서)는 v₀ = 0과 s₀ = 0 때문에 **너무 작아집니다**.
*   결과적으로, 업데이트 단계는 **0 쪽으로 편향**될 것입니다.

**(수식: t=1일 때 모멘텀과 RMSProp의 이동 평균이 초기 그래디언트보다 훨씬 작아지는 것을 보여줌)**

***

### **페이지 508**

**Adam의 편향 보정**

*   Adam에서, β₁ = 0.9, β₂ = 0.999
*   그리고 이동 평균 v_t, s_t는 0으로 초기화됩니다.
*   충분한 통계가 누적되기 전에, 초기 추정치(특히 t = 0에서)는 **너무 작아집니다**.
*   이 편향을 보정하기 위해, Adam은 **정규화**를 사용합니다. 이 조정은 이동 평균의 기댓값이 편향되지 않도록 재조정하며, 특히 훈련의 초기 몇 단계 동안 그렇습니다.

**(수식: 편향 보정 공식을 추가하여, 초기 단계에서 이동 평균이 어떻게 보정되는지를 보여줌)**

***

### **페이지 509**

**Adam의 최종 업데이트 공식**

*   **편향 보정된 모멘트**를 계산한 후, Adam은 다음과 같이 파라미터를 업데이트합니다:
    *   v̂_t+1: 편향 보정된 **1차 모멘트** (모멘텀과 유사한 항)
    *   ŝ_t+1: 편향 보정된 **2차 모멘트** (RMS와 유사한 항)
    *   ε: 작은 상수 (예: 10⁻⁸) (수치적 안정성을 위해)
*   이 방정식은 **방향성 평활화**(모멘텀으로부터)와 **적응형 스케일링**(RMSProp으로부터)을 결합합니다.
*   Adam은 **스텝 크기**와 **업데이트 방향**을 동적으로 조정하여, 실제 적용에서 빠르고 안정적인 수렴을 이끌어냅니다.

**(수식: Adam 옵티마이저의 전체 업데이트 과정을 보여주는 공식들)**

***

### **페이지 510**

**Adam의 최종 업데이트 공식**

*   **편향 보정된 모멘트**를 계산한 후, Adam은 다음과 같이 파라미터를 업데이트합니다.

**(코드: Adam 옵티마이저의 전체 과정을 구현한 Python 유사 코드)**

***

### **페이지 511**

**시각화**

*   SGD vs SGD+모멘텀 vs RMSProp vs ADAM

**(이미지: 네 가지 옵티마이저(SGD, SGD+Momentum, RMSProp, Adam)의 최적화 경로를 동일한 손실 표면 위에서 비교하는 애니메이션의 한 장면)**

***

### **페이지 512**

**학습률 (η)과 그 동작**

*   **학습률**이라고도 불리는 스텝 크기 η는 파라미터가 음의 그래디언트 방향으로 얼마나 멀리 이동할지를 제어합니다.
*   그 값은 수렴 동작에 결정적인 영향을 미칩니다:
    *   **너무 크면 (↑ η)**: 스텝이 최솟값을 지나쳐 발산하거나 진동합니다.
    *   **너무 작으면 (↓ η)**: 수렴이 매우 느려집니다.
    *   **적절한 η**: 안정적이고 효율적인 수렴.

**(이미지: 학습률의 크기에 따른 손실 곡선의 변화를 보여주는 두 개의 그래프. 너무 크거나 작은 학습률의 문제점과 적절한 학습률의 중요성을 시각적으로 설명)**

***

### **페이지 513**

**학습률 (η) 선택하기**

*   실제로, 단 하나의 "올바른" 학습률은 없습니다.
*   거의 모든 η 값이 — 모델, 데이터셋, 옵티마이저 구성에 따라 — 작동할 수 있습니다.
*   따라서, 좋은 학습률을 찾는 것은 **항상 실험을 요구합니다**.

**(이미지: 다양한 학습률 설정(매우 높음, 낮음, 높음, 좋음)에 따른 손실 곡선을 보여주며, 어떤 것이 최선인지 묻고 "실제로는 이 모든 것이 좋은 학습률일 수 있다"고 답함)**

***

### **페이지 514**

**학습률 스케줄링: 감쇠 전략**

*   현대적인 훈련에서, 학습률(η)은 고정되지 않습니다 — 수렴을 개선하기 위해 **스케줄**을 사용하여 동적으로 조정됩니다.
*   가장 간단한 접근법은 **학습률 감쇠**이며, 특정 훈련 이정표에서 η에 상수 인자(예: 0.1)를 곱합니다.
*   **스텝 감쇠 (Step Decay)**

**(이미지: 특정 에포크(30, 60, 90)에서 학습률을 단계적으로 감소시키는 스텝 감쇠 전략과 그에 따른 훈련 손실 곡선을 보여주는 그래프)**

***

### **페이지 515**

**학습률 스케줄링: 코사인 어닐링**

*   또 다른 널리 사용되는 스케줄링 방법은 **코사인 어닐링**이며, 급격한 감소 대신 **코사인 곡선**을 따라 학습률을 부드럽게 감소시킵니다.
*   학습률은 높게 시작하여 점차 최솟값으로 감소하며, 초기에는 큰 탐색 단계를, 나중에는 미세 조정을 가능하게 합니다.
*   종종 **웜 리스타트(SGDR)**와 결합됩니다: η가 η_min에 도달하면, 새로운 훈련 주기를 위해 η_max로 재설정됩니다.

**(이미지 및 수식: 코사인 곡선을 따라 감소하는 학습률을 보여주는 그래프와 코사인 어닐링의 수식)**

***

### **페이지 516**

**학습률 스케줄링: 코사인 어닐링**

*   또 다른 널리 사용되는 스케줄링 방법은 **코사인 어닐링**이며, 급격한 감소 대신 **코사인 곡선**을 따라 학습률을 부드럽게 감소시킵니다.
*   학습률은 높게 시작하여 점차 최솟값으로 감소하며, 초기에는 큰 탐색 단계를, 나중에는 미세 조정을 가능하게 합니다.
*   종종 **웜 리스타트(SGDR)**와 결합됩니다: η가 η_min에 도달하면, 새로운 훈련 주기를 위해 η_max로 재설정됩니다.

**(이미지: 코사인 어닐링 스케줄에 따른 훈련 손실 곡선을 보여주는 그래프)**

***

### **페이지 517**

**학습률 스케줄링: 선형 감쇠**

*   또 다른 일반적인 스케줄은 **선형 감쇠**이며, 학습률이 훈련 동안 초기 값에서 더 작은 목표 값(종종 0)으로 **선형적으로 감소**합니다.
*   이 간단한 스케줄은 진동이나 급격한 점프 없이 부드럽고 예측 가능한 감소를 제공합니다.
*   코사인 어닐링보다 간단하며, 종종 트랜스포머 기반 훈련(예: BERT)에서 사용됩니다.
*   때때로 η가 작게 시작하여 감쇠하기 전에 선형적으로 증가하는 **웜업 단계**와 결합됩니다.

**(이미지 및 수식: 선형적으로 감소하는 학습률을 보여주는 그래프와 관련 수식)**

***

### **페이지 518**

**학습률 스케줄링: 역 제곱근 감쇠**

*   **역 제곱근 감쇠** 스케줄은 1/√t 곡선을 따라 훈련이 진행됨에 따라 학습률을 부드럽게 감소시킵니다.
*   스텝 또는 선형 감쇠와 달리, 감소는 시간이 지남에 따라 **점차 느려집니다**.
*   훈련 초기에는 η가 빠르게 감소하여 빠른 안정화를 가능하게 합니다.
*   나중에는 η가 더 느리게 감쇠하여 모델이 너무 일찍 멈추는 것을 방지합니다.

**(이미지 및 수식: 역 제곱근 곡선을 따라 감소하는 학습률을 보여주는 그래프와 관련 수식)**

***

### **페이지 519**

**선형 웜업과 현대 훈련에서의 사용**

*   **선형 웜업**이라는 방법이 있으며, 학습률이 작은 초기 값에서 목표 학습률까지 **선형적으로 점차 증가**합니다.
*   이는 초기 단계에서 훈련을 안정화시키는 데 도움이 되며, 모델 파라미터가 아직 초기화되지 않고 불안정할 때 갑작스러운 발산을 방지합니다.
*   현대 딥러닝에서, 선형 웜업은 종종 다양한 감쇠 스케줄과 **결합**됩니다.
*   대부분의 현대 아키텍처(예: 트랜스포머, BERT, 비전 트랜스포머)는 **"선형 웜업 + 감쇠"**를 표준 설정으로 채택합니다.

**(이미지: 초기에 선형적으로 증가했다가 감소하는 학습률 스케줄을 보여주는 그래프)**

***

### **페이지 520**

**참고 자료**

*   Stanford CS231N | 2025년 봄 | 강의 3: 정규화 및 최적화
    *   https://www.youtube.com/watch?v=dyNGd06MWn4&list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16&index=3
*   Stanford CS231N | 2025년 봄 | 강의 4: 신경망과 역전파
    *   https://www.youtube.com/watch?v=25zD5qJHYsk&list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16&index=4

***

### **페이지 521**

**컴퓨터 비전의 기초**
**2025년 2학기**
**- 엄광현 –**
**인공지능학과**

***

### **페이지 522**

**강의 7:**
**역전파 및 정규화**
*   왜 역전파인가
*   역전파란 무엇인가
*   왜 정규화가 필요한가
*   최적화를 정규화하는 방법

***

### **페이지 523**

**복습: 데이터셋 전체에 대한 총 손실**

*   이것은 네트워크가 단일 이미지가 아닌 **전체 훈련 세트**에서 얼마나 잘 수행하는지를 나타냅니다.
*   우리의 목표는 파라미터 W를 조정하여 이 **총 손실을 최소화**하는 것입니다.

**(이미지 및 수식: 총 손실을 최소화하는 것이 목표임을 다시 한번 강조하는 다이어그램)**

***

### **페이지 524**

**복습: 모델 훈련에 경사 하강법 적용하기**

*   우리의 궁극적인 목표는 총 손실을 최소화하는 파라미터 집합 W*를 찾는 것입니다.
*   L_total(W)를 최소화하기 위해, 우리는 W에 대한 **도함수(그래디언트)**를 계산합니다.
*   그런 다음 **가장 가파른 하강** 방향으로 W를 **업데이트**합니다.

**(수식: 모델 훈련에 경사 하강법을 적용하는 전체 과정을 수식으로 표현)**

***

### **페이지 525**

**신경망에서 그래디언트를 어떻게 계산하는가?**

*   신경망에서, 모든 파라미터(가중치와 편향)에 대한 손실의 그래디언트를 실제로 어떻게 계산할 수 있을까요?

**(수식: 이전 페이지의 수식에서 그래디언트 계산 부분에 "어떻게 계산하는가?"라는 질문을 추가)**

***

### **페이지 526**

**순전파에서 그래디언트 계산까지**

*   지금까지, 우리는 **입력 이미지**를 가져와 **신경망**을 통과시켜 출력에서 **손실**을 계산했습니다.

**(이미지 및 수식: 입력 이미지가 신경망을 통과하여 최종적으로 교차 엔트로피 손실 값이 계산되는 전체 순전파 과정을 보여주는 다이어그램)**

***

### **페이지 527**

**손실을 줄이기 위해 네트워크 조정하기**

*   이제 우리는 네트워크의 출력으로부터 **손실 값**을 계산했습니다.
*   우리의 목표는 명확합니다:
    *   이 손실을 줄이거나 (또는 동등하게, 정확한 클래스 점수 — 예: "고양이" 점수 — 를 높입니다).

**(이미지: 계산된 손실 값을 줄여야 함을 화살표로 표시)**

***

### **페이지 528**

**손실을 줄이기 위해 네트워크 조정하기**

*   이를 달성하기 위해, 우리는 네트워크의 내부 가지 — 즉, 각 레이어의 **가중치와 편향** — 를 **조정**해야 합니다.
*   그래서 네트워크의 예측이 정답에 더 가까워지도록 합니다.

**(이미지: 손실을 줄이기 위해 네트워크의 가중치를 조정해야 함을 나타내는 "가중치 조정" 화살표를 추가)**

***

### **페이지 529**

**여러 레이어에 걸친 많은 가중치**

*   신경망의 각 레이어는 **많은 가지**를 포함하며, 각 가지에는 한 뉴런이 다른 뉴런에 얼마나 강하게 영향을 미치는지를 결정하는 **가중치**가 있습니다.
*   각 가중치는 정보가 네트워크를 통해 어떻게 흐르는지를 제어합니다.

**(이미지: 네트워크의 모든 연결에 가중치(W¹₁₁, W¹₁₂, W²₁₁, 등)를 표시)**

***

### **페이지 530**

**핵심 질문 — 각 가중치는 손실에 어떻게 영향을 미치는가?**

*   각 개별 가중치를 변경하는 것이 전체 손실에 어떻게 영향을 미칠까요?
*   네트워크의 모든 가중치는 — 직접적으로든 간접적으로든 — 최종 출력과 그에 따른 **손실 값**에 기여합니다.

**(이미지: 특정 가중치(W²₁₁)가 최종 손실에 어떻게 영향을 미치는지 질문)**

***

### **페이지 531**

**핵심 질문 — 각 가중치는 손실에 어떻게 영향을 미치는가?**

*   각 개별 가중치를 변경하는 것이 전체 손실에 어떻게 영향을 미칠까요?
*   네트워크의 모든 가중치는 — 직접적으로든 간접적으로든 — 최종 출력과 그에 따른 **손실 값**에 기여합니다.

**(이미지: 특정 가중치가 손실에 영향을 미치는 경로를 화살표로 표시)**

***

### **페이지 532**

**핵심 질문 — 각 가중치는 손실에 어떻게 영향을 미치는가?**

*   각 개별 가중치를 변경하는 것이 전체 손실에 어떻게 영향을 미칠까요?
*   네트워크의 모든 가중치는 — 직접적으로든 간접적으로든 — 최종 출력과 그에 따른 **손실 값**에 기여합니다.

**(이미지: 특정 가중치를 약간 변경(증가 또는 감소)했을 때 손실이 증가할지 감소할지 묻는 그림)**

***

### **페이지 533**

**핵심 질문 — 각 가중치는 손실에 어떻게 영향을 미치는가?**

*   각 개별 가중치를 변경하는 것이 전체 손실에 어떻게 영향을 미칠까요?
*   네트워크의 모든 가중치는 — 직접적으로든 간접적으로든 — 최종 출력과 그에 따른 **손실 값**에 기여합니다.

**(이미지: 이전 계층의 가중치를 변경하는 것이 손실에 어떻게 영향을 미치는지 묻는 그림)**

***

### **페이지 534**

**핵심 질문 — 각 가중치는 손실에 어떻게 영향을 미치는가?**

*   가중치를 올바르게 업데이트하려면, **각 가중치**에 대한 **손실의 그래디언트**를 계산해야 합니다. 즉,
*   네트워크의 모든 파라미터 Wᵏᵢⱼ에 대해.

**(수식: 손실을 특정 가중치로 편미분하는 것을 수치 미분으로 근사하는 공식을 보여줌)**

***

### **페이지 535**

**핵심 질문 — 각 가중치는 손실에 어떻게 영향을 미치는가?**

*   가중치를 올바르게 업데이트하려면, **각 가중치**에 대한 **손실의 그래디언트**를 계산해야 합니다. 즉,
*   네트워크의 모든 파라미터 Wᵏᵢⱼ에 대해.

**(이미지: 네트워크의 모든 가중치에 대해 손실의 편미분을 계산해야 함을 보여줌)**

***

### **페이지 536**

**핵심 질문 — 각 가중치는 손실에 어떻게 영향을 미치는가?**

*   가중치를 올바르게 업데이트하려면, **각 가중치**에 대한 **손실의 그래디언트**를 계산해야 합니다.
*   네트워크의 모든 파라미터 Wᵏᵢⱼ에 대해.
*   그런 다음 **음의 그래디언트 방향**으로 업데이트하여 다음 번에 손실을 더 작게 만들 수 있습니다.

**(이미지 및 수식: 그래디언트를 계산한 후 경사 하강법으로 가중치를 업데이트하는 과정을 보여줌)**

***

### **페이지 537**

**간단한 2계층 네트워크의 수동 그래디언트 계산**

*   기본 수학 연산으로 구성된 2계층 네트워크와 같은 매우 작은 신경망의 경우 — 우리는 직접 도함수를 명시적으로 계산하고 가중치를 업데이트할 수 있습니다.
    *   순전파 계산
    *   필요한 편도함수를 손으로 계산
    *   해당 그래디언트를 사용하여 가중치 업데이트
*   간단한 장난감 모델에서는 그래디언트를 수동으로 유도하고 프로그래밍할 수 있습니다.

**(코드: 2계층 신경망의 훈련을 구현한 약 20줄의 Python 코드 예시)**

***

### **페이지 538**

**이 그래디언트를 손으로 계산할 수 있나요?**

*   우리의 손실 함수가 간단하고 깔끔한 표현이 아니라, 많은 하위 함수들의 **깊게 얽힌 비선형 조합**이라고 상상해 보세요.
*   이것은 단지 작은 예시일 뿐입니다 — 실제 신경망은 수천 또는 수백만 개의 얽힌 연산으로 **훨씬 더 복잡**합니다.
*   이제, 모든 파라미터 Wᵏᵢⱼ에 대해 그래디언트를 찾아야 한다고 가정해 봅시다.
*   이것을 **손으로** 계산할 수 있을까요? 연쇄 법칙을 층마다 적용해야 할 것입니다.
    *   모든 편도함수와 중간 값을 추적하는 것은 — 네트워크가 깊어질수록 인간에게는 **불가능한 작업**입니다.

**(수식: 매우 복잡한 손실 함수의 예시)**

***

### **페이지 539**

**역전파**

*   따라서, 우리는 이 모든 도함수를 계산할 **자동적이고 효율적인 방법**이 필요합니다.
    *   이것이 바로 **역전파**(또는 **자동 미분**)가 하는 일입니다.
*   **역전파 알고리즘**은 1970년대에 처음 공식화되었지만, David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams의 유명한 1986년 논문 이후 널리 알려지게 되었습니다.
    *   "오류를 역전파함으로써 표현 학습하기" (Nature, 1986).

**(이미지: 역전파의 개념을 보여주는 신경망 다이어그램)**

***

### **페이지 540**

**역전파 — 가장 간단한 예시**

*   역전파가 어떻게 작동하는지 이해하기 위해 가장 간단한 예시부터 시작해 봅시다.
*   함수가 있습니다: f(x, y, z) = (x + y)z
*   이것을 **계산 그래프**로 나타낼 수 있습니다.

**(이미지: f(x, y, z) = (x + y)z 함수를 덧셈과 곱셈 노드로 구성된 계산 그래프로 표현)**

***

### **페이지 541**

**역전파 — 가장 간단한 예시**

*   실제 숫자를 대입해 봅시다:
    *   x = -2, y = 5, z = -4

**(이미지: 이전 계산 그래프에 입력 값들을 대입)**

***

### **페이지 542**

**역전파 — 가장 간단한 예시**

*   실제 숫자를 대입해 봅시다:
    *   x = -2, y = 5, z = -4

**(이미지: 덧셈 노드의 출력이 3임을 계산하여 표시)**

***

### **페이지 543**

**역전파 — 가장 간단한 예시**

*   실제 숫자를 대입해 봅시다:
    *   x = -2, y = 5, z = -4

**(이미지: 최종 출력 f가 -12임을 계산하여 표시)**

***

### **페이지 544**

**역전파 — 가장 간단한 예시**

*   이제 우리는 최종 출력 f가 각 입력에 따라 어떻게 변하는지 찾고 싶습니다.
*   즉, ∂f/∂x, ∂f/∂y, ∂f/∂z

**(이미지: 각 입력에 대한 편미분을 구해야 함을 물음표로 표시)**

***

### **페이지 545**

**역전파 — 가장 간단한 예시**

*   가장 쉬운 부분부터 시작해 봅시다. 우리는 ∂f/∂z를 계산하고 싶습니다.
*   여기서, z에 대해 미분할 때 x + y는 상수로 취급됩니다.
*   따라서 우리는 다음을 얻습니다: ∂f/∂z = q = x + y

**(이미지: ∂f/∂z를 계산하는 과정을 보여줌)**

***

### **페이지 546**

**역전파 — 가장 간단한 예시**

*   가장 쉬운 부분부터 시작해 봅시다. 우리는 ∂f/∂z를 계산하고 싶습니다.
*   여기서, z에 대해 미분할 때 x + y는 상수로 취급됩니다.
*   따라서 우리는 다음을 얻습니다: ∂f/∂z = q = x + y = (-2 + 5) = 3

**(이미지: ∂f/∂z의 값이 3임을 계산하여 표시)**

***

### **페이지 547**

**역전파 — 가장 간단한 예시**

*   가장 쉬운 부분부터 시작해 봅시다. 우리는 ∂f/∂z를 계산하고 싶습니다.
*   여기서, z에 대해 미분할 때 x + y는 상수로 취급됩니다.
*   따라서 우리는 다음을 얻습니다.
*   z를 +1 증가시키면 f는 +3 증가합니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 548**

**y에 대한 그래디언트**

*   이제 출력 f가 y에 대해 어떻게 변하는지 찾아봅시다. 이전 경우와 달리, y는 하위 함수 — 덧셈 (x + y) — 안에 있습니다.
*   따라서 우리는 **연쇄 법칙**을 적용할 것입니다.

**(이미지: ∂f/∂y를 구해야 함을 물음표로 표시)**

***

### **페이지 549**

**y에 대한 그래디언트**

*   연쇄 법칙 적용:
    *   ∂f/∂y = (∂f/∂q) ∙ (∂q/∂y)

**(이미지 및 수식: 연쇄 법칙을 적용하는 과정을 보여줌)**

***

### **페이지 500**

**y에 대한 그래디언트**

*   연쇄 법칙 적용

**(이미지 및 수식: 연쇄 법칙의 각 항(∂f/∂q, ∂q/∂y)을 구해야 함을 보여줌)**

***

### **페이지 501**

**y에 대한 그래디언트**

*   연쇄 법칙 적용
    *   ∂f/∂q = z
    *   ∂q/∂y = 1

**(이미지 및 수식: 연쇄 법칙의 각 항을 계산한 결과를 보여줌)**

***

### **페이지 502**

**y에 대한 그래디언트**

*   연쇄 법칙 적용
    *   ∂f/∂q = z = -4
    *   ∂q/∂y = 1

**(이미지 및 수식: 계산 그래프의 중간 노드에서 그래디언트(∂f/∂q)를 계산하여 표시)**

***

### **페이지 503**

**y에 대한 그래디언트**

*   연쇄 법칙 적용
    *   ∂f/∂y = (∂f/∂q) ∙ (∂q/∂y) = z × 1 = -4

**(이미지 및 수식: 최종적으로 ∂f/∂y를 계산한 결과를 보여줌)**

***

### **페이지 504**

**y에 대한 그래디언트**

*   연쇄 법칙 적용

**(이미지: 계산된 그래디언트 값(∂f/∂y = -4)을 그래프에 표시)**

***

### **페이지 505**

**y에 대한 그래디언트**

*   z가 음수일 때, y를 증가시키면 f는 **감소**합니다.
*   다시 말해, f는 z의 값에 의해 스케일링된, y에 **선형적으로 민감**합니다.
*   이 단계는 그래디언트가 **어떻게 뒤로 흐르는지** 보여줍니다 — f의 y에 대한 도함수는 f의 중간 노드 q에 대한 그래디언트에 의존합니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 506**

**x에 대한 그래디언트**

*   마지막으로, 출력 f가 x에 대해 어떻게 변하는지 계산해 봅시다.
    *   ∂f/∂x = (∂f/∂q) ∙ (∂q/∂x) = z × 1 = -4

**(이미지 및 수식: x에 대한 그래디언트를 계산하는 과정을 보여줌)**

***

### **페이지 507**

**x에 대한 그래디언트**

*   마지막으로, 출력 f가 x에 대해 어떻게 변하는지 계산해 봅시다.

**(이미지: 계산된 그래디언트 값(∂f/∂x = -4)을 그래프에 표시)**

***

### **페이지 508**

**계산 노드를 통해 흐르는 그래디언트**

*   예시에서 보았듯이, 역전파의 가장 중요한 부분은 **각 계산 노드**에서 일어나는 일입니다.
*   각 노드는 덧셈, 곱셈, 활성화 등과 같은 **작은 지역 연산**을 나타냅니다.

**(이미지: 계산 그래프의 각 연산 노드(덧셈, 곱셈)를 강조)**

***

### **페이지 509**

**계산 노드를 통해 흐르는 그래디언트**

*   계산 그래프의 각 노드는 자신에게 직접 연결된 것 — **자신의 입력과 출력** — 에만 신경 씁니다.
*   전체 네트워크 구조를 알 필요가 **없습니다**.

**(이미지: 곱셈 노드와 그 직접적인 입력/출력에만 초점을 맞춘 그림)**

***

### **페이지 510**

**계산 노드를 통해 흐르는 그래디언트**

*   각 노드는 **출력으로부터 그래디언트**를 받습니다 — 즉, 노드의 출력이 변할 때 손실이 얼마나 변하는지.
*   이를 통해, 노드는 **각 입력**에 대해 손실이 어떻게 변하는지를 계산할 수 있습니다.

**(이미지: 덧셈 노드가 출력으로부터 그래디언트(∂f/∂q)를 받아, 이를 사용하여 입력에 대한 그래디언트(∂f/∂x, ∂f/∂y)를 계산하는 과정을 보여줌)**

***

### **페이지 511**

**상류, 지역, 하류 그래디언트**

*   그래디언트의 흐름을 더 명확하게 설명하기 위해, 우리는 그것들이 어디서 오고 어디로 가는지에 따라 세 가지 유형으로 나눌 수 있습니다.
*   **1. 상류 그래디언트 (Upstream Gradient)**
    *   이것은 다음 레이어("출력" 측)에서 노드로 **흘러 들어오는 그래디언트**입니다.

**(이미지: 덧셈 노드로 들어오는 그래디언트(∂f/∂q)를 "상류 그래디언트"로 표시)**

***

### **페이지 512**

**상류, 지역, 하류 그래디언트**

*   **2. 지역 그래디언트 (Local Gradient)**
    *   이것은 노드 자체의 내부 연산(덧셈, 곱셈, 활성화 등)에 기반하여 **노드 자체에서 생성된 그래디언트**입니다.
    *   노드의 **자체 출력**이 **입력**에 대해 어떻게 변하는지를 설명합니다.

**(이미지: 덧셈 노드의 입력에 대한 편미분(∂q/∂x, ∂q/∂y)을 "지역 그래디언트"로 표시)**

***

### **페이지 513**

**상류, 지역, 하류 그래디언트**

*   **3. 하류 그래디언트 (Downstream Gradient)**
    *   이것은 노드의 **입력으로 전달되는 그래디언트**입니다 — 상류 및 지역 그래디언트를 결합한 결과입니다.
    *   네트워크를 통해 **아래로** 흐르며, 결국 가중치와 입력에 도달합니다.

**(이미지: 최종적으로 계산된 입력에 대한 그래디언트(∂f/∂x, ∂f/∂y)를 "하류 그래디언트"로 표시)**

***

### **페이지 514**

**상류, 지역, 하류 그래디언트**

*   1. 상류 그래디언트
*   2. 지역 그래디언트
*   3. 하류 그래디언트

**(이미지: 입력 x, y와 출력 z를 가진 일반적인 함수 노드 f를 보여주는 그림)**

***

### **페이지 515**

**계산 노드를 통해 흐르는 그래디언트**

*   1. 상류 그래디언트
*   2. 지역 그래디언트
*   3. 하류 그래디언트

**(이미지: 이전 그림에 상류 그래디언트(∂L/∂z)와 지역 그래디언트(∂z/∂x, ∂z/∂y)를 표시)**

***

### **페이지 516**

**계산 노드를 통해 흐르는 그래디언트**

*   1. 상류 그래디언트
*   2. 지역 그래디언트
*   3. 하류 그래디언트

**(이미지: 이전 그림에 하류 그래디언트(∂L/∂x, ∂L/∂y)가 상류와 지역 그래디언트의 곱으로 계산됨을 연쇄 법칙으로 보여줌)**

***

### **페이지 517**

**간단한 신경망에서의 역전파**

*   이제 그래디언트가 개별 노드를 통해 어떻게 흐르는지 이해했으므로, 이것이 실제 **신경망**에 어떻게 적용되는지 봅시다.

**(이미지: 2계층 신경망의 순전파 및 손실 계산 과정을 보여주는 다이어그램)**

***

### **페이지 518**

**간단한 신경망에서의 역전파**

*   이제 그래디언트가 개별 노드를 통해 어떻게 흐르는지 이해했으므로, 이것이 실제 **신경망**에 어떻게 적용되는지 봅시다.

**(이미지: 이전 다이어그램에서 특정 가중치(W¹)가 손실에 어떻게 영향을 미치는지 알아내야 함을 물음표로 표시)**

***

### **페이지 519**

**간단한 신경망에서의 역전파**

*   이제 그래디언트가 개별 노드를 통해 어떻게 흐르는지 이해했으므로, 이것이 실제 **신경망**에 어떻게 적용되는지 봅시다.

**(이미지: 신경망의 첫 번째 뉴런에 대한 선형 결합 계산 과정을 시각적으로 보여줌)**

***

### **페이지 520**

**간단한 신경망에서의 역전파**

*   예제를 확장하여 선형 레이어 뒤에 **시그모이드 활성화**를 추가해 봅시다.

**(이미지: 이전 그림에 시그모이드 활성화 함수를 추가하여 최종 출력(f)을 계산하는 과정을 보여줌)**

***

### **페이지 521**

**계산 그래프 — 선형 레이어 + 시그모이드 활성화**

*   예제를 확장하여 선형 레이어 뒤에 **시그모이드 활성화**를 추가해 봅시다.

**(이미지: 선형 레이어와 시그모이드 활성화 함수를 포함하는 전체 계산 그래프를 보여줌)**

***

### **페이지 522**

**계산 그래프 — 선형 레이어 + 시그모이드 활성화**

*   예제를 확장하여 선형 레이어 뒤에 **시그모이드 활성화**를 추가해 봅시다.

**(이미지: 시그모이드 함수를 더 작은 연산 노드(곱셈, 지수, 덧셈, 나눗셈)로 분해하여 계산 그래프를 더 상세하게 보여줌)**

***

### **페이지 523**

**계산 그래프 — 선형 레이어 + 시그모이드 활성화**

*   핵심 **역전파 흐름**에 집중하기 위해 이전 예제를 단순화해 봅시다.

**(이미지: 2개의 입력과 2개의 가중치를 가진 더 단순화된 계산 그래프)**

***

### **페이지 524**

**순전파 — 출력 계산하기**

*   신경망 훈련의 **첫 번째 단계**는 **순전파**입니다. 여기서 우리는 주어진 **입력**과 **초기 가중치**에서 시작하여, 최종 **출력**과 **손실**까지 모든 중간 값을 계산합니다.

**(이미지: 이전의 단순화된 그래프에 초기 입력 및 가중치 값을 대입)**

***

### **페이지 525**

**순전파 — 출력 계산하기**

*   신경망 훈련의 **첫 번째 단계**는 **순전파**입니다. 여기서 우리는 주어진 **입력**과 **초기 가중치**에서 시작하여, 최종 **출력**과 **손실**까지 모든 중간 값을 계산합니다.

**(이미지: 순전파 계산을 완료하여 모든 중간 값과 최종 출력 값을 그래프에 표시)**

***

### **페이지 526**

**역전파 시작 — 최종 노드에서의 그래디언트**

*   **역전파**를 시작할 때, 우리는 **마지막 노드** — 전체 계산의 출력, 보통 손실 f 또는 L — 에서 시작합니다.
*   네트워크의 맨 끝에서, ∂f/∂f = 1

**(이미지: 계산 그래프의 최종 출력 노드에서 그래디언트가 1로 시작됨을 표시)**

***

### **페이지 527**

**역전파 예시 — 1/x 노드를 통과하기**

*   이제 계산 그래프에서 한 단계 이전으로 이동해 봅시다.

**(이미지: 1/x 노드에 대한 역전파 과정을 보여줌. 상류 그래디언트(1.00)와 지역 그래디언트(-1/1.37²)를 곱하여 하류 그래디언트(-0.53)를 계산)**

***

### **페이지 528**

**네트워크를 통한 그래디언트 전파**

*   현재 노드(1/x 예시처럼)에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: +1 노드에 대한 역전파 과정을 보여줌. 상류 그래디언트(-0.53)와 지역 그래디언트(1)를 곱하여 하류 그래디언트(-0.53)를 계산)**

***

### **페이지 529**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: eˣ 노드에 대한 역전파 과정을 보여줌. 상류 그래디언트(-0.53)와 지역 그래디언트(e⁻¹)를 곱하여 하류 그래디언트(-0.20)를 계산)**

***

### **페이지 530**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: ×(-1) 노드에 대한 역전파 과정을 보여줌. 상류 그래디언트(-0.20)와 지역 그래디언트(-1)를 곱하여 하류 그래디언트(0.20)를 계산)**

***

### **페이지 531**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: 덧셈 노드(z = ... + b)에 대한 역전파 과정을 보여줌. 덧셈 게이트는 상류 그래디언트를 그대로 분배하므로, b에 대한 하류 그래디언트는 0.20이 됨)**

***

### **페이지 532**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: 덧셈 노드의 다른 입력(4.00)에 대한 하류 그래디언트도 0.20이 됨을 보여줌)**

***

### **페이지 533**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: 또 다른 덧셈 노드(4.00 = -2.00 + 6.00)에 대한 역전파 과정을 보여줌. 상류 그래디언트(0.20)가 두 입력으로 분배됨)**

***

### **페이지 534**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: 곱셈 노드(-2.00 = 2.00 * -1.00)에 대한 역전파 과정을 보여줌. 상류 그래디언트(0.20)와 지역 그래디언트(다른 입력 값)를 곱하여 하류 그래디언트를 계산)**

***

### **페이지 535**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: x₁에 대한 하류 그래디언트(0.40)를 계산하는 과정을 보여줌)**

***

### **페이지 536**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: W₁에 대한 하류 그래디언트(-0.20)를 계산하는 과정을 보여줌)**

***

### **페이지 537**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: W₂에 대한 하류 그래디언트(-0.40)를 계산하는 과정을 보여줌)**

***

### **페이지 538**

**네트워크를 통한 그래디언트 전파**

*   현재 노드에 대한 그래디언트를 계산한 후, 우리는 계산 그래프를 통해 **뒤로** 계속 이동하며 — **노드별로** — 각 연결을 따라 그래디언트를 계산하고 전파합니다.

**(이미지: x₂에 대한 하류 그래디언트(-0.60)를 계산하는 과정을 보여줌)**

***

### **페이지 539**

**역전파의 끝 — 모든 파라미터에 대한 그래디언트**

*   이 시점에서, 우리는 손실 노드에서부터 모든 입력과 파라미터까지 네트워크를 통해 **모든 길을 거슬러** 그래디언트를 따라왔습니다.
*   이제, 모든 가중치와 편향은 최종 출력 f에 **어떻게 영향을 미치는지** 알려주는 고유한 그래디언트 값을 갖게 됩니다.

**(이미지: 모든 파라미터(W₁, W₂, b)에 대한 그래디언트가 계산되었음을 강조)**

***

### **페이지 540**

**역전파 후 가중치 업데이트**

*   역전파를 완료하고 모든 파라미터(가중치 w)에 대한 그래디언트를 얻으면, **경사 하강법**을 사용하여 가중치를 업데이트할 수 있습니다.
*   단순화를 위해 **학습률 = 1**이라고 가정하면, 결과는 다음과 같습니다.

**(이미지: 계산된 그래디언트를 사용하여 가중치와 편향을 업데이트한 결과를 보여줌)**

***

### **페이지 541**

**업데이트 후 — 중간 값들이 무효화됨**

*   계산된 그래디언트를 사용하여 가중치를 업데이트하면, 이전 순전파 및 역전파 과정에서 저장된 모든 **중간 값**(예: 활성화, 캐시된 값, 편도함수)은 **더 이상 유효하지 않습니다**.
*   이 값들은 **이전** 가중치를 기반으로 계산되었습니다.
    가중치가 변경되면 네트워크의 내부 계산도 변경됩니다.

**(이미지: 가중치 업데이트 후 중간 값들이 더 이상 유효하지 않음을 물음표로 표시)**

***

### **페이지 542**

**그래디언트 초기화 및 훈련 반복**

*   모든 가중치를 계산된 그래디언트를 사용하여 업데이트한 후, 해당 그래디언트는 더 이상 유효하지 않습니다.
*   그것들은 **이전** 가중치 값에 대해 특별히 계산된 것이었습니다.

**(이미지: 업데이트된 가중치를 보여주는 그림)**

***

### **페이지 543**

**그래디언트 초기화 및 훈련 반복**

*   그래디언트는 **초기화/리셋**됩니다.
*   업데이트된 가중치로 **새로운 순전파**를 수행합니다.
*   그런 다음 **역전파를 다시 실행**하여 새로운 그래디언트를 계산합니다.
*   이 **순전파 → 역전파 → 업데이트** 루프는 수렴할 때까지 계속됩니다.

**(이미지: 업데이트된 가중치로 새로운 순전파가 시작되는 과정을 보여줌)**

***

### **페이지 544**

**그래디언트 초기화 및 훈련 반복**

*   그래디언트는 **초기화/리셋**됩니다.
*   업데이트된 가중치로 **새로운 순전파**를 수행합니다.
*   그런 다음 **역전파를 다시 실행**하여 새로운 그래디언트를 계산합니다.
*   이 **순전파 → 역전파 → 업데이트** 루프는 수렴할 때까지 계속됩니다.

**(이미지: 순전파 후 다시 역전파가 시작되는 과정을 보여줌)**

***

### **페이지 545**

**그래디언트 해석 — 부호와 크기**

*   만약 w₁이 약간 증가하면, 최종 출력 f는 **감소**합니다 (음수, 약한 효과).
*   w₂를 증가시키면 f는 **감소**합니다 (음수, 더 강한 효과).
*   편향 b를 약간 증가시키면 f는 **증가**합니다 (양수, 약한 영향).

**(이미지: 각 파라미터에 대해 계산된 그래디언트 값을 보여줌)**

***

### **페이지 546**

**시그모이드 함수의 도함수와 그 간결한 표현**

*   참고로, **시그모이드 함수의 도함수**와 그것이 계산 그래프에서 어떻게 깔끔하게 표현될 수 있는지 살펴봅시다.
    *   df/dx = f(x)(1 - f(x))

**(수식 및 이미지: 시그모이드 함수의 도함수가 출력값 자체로 표현될 수 있음을 보여주는 수식과 다이어그램)**

***

### **페이지 547**

**시그모이드 함수의 도함수와 그 간결한 표현**

*   참고로, **시그모이드 함수의 도함수**와 그것이 계산 그래프에서 어떻게 깔끔하게 표현될 수 있는지 살펴봅시다.
*   이 결과는 **시그모이드의 그래디언트**가 **자신의 출력 값**에만 의존한다는 것을 의미합니다.
*   즉, 우리가 이미 y = f(x)를 알고 있다면, 지수 항을 다시 사용하지 않고 y로부터 직접 도함수를 계산할 수 있습니다.

**(이미지: 시그모이드 게이트의 역전파가 출력값(0.73)을 사용하여 간단하게 계산되는 과정을 보여줌)**

***

### **페이지 548**

**그래디언트 흐름의 패턴**

*   신경망에서는 대부분의 연산이 몇 가지 간단한 **계산 게이트**로 분해될 수 있기 때문에 동일한 **그래디언트 흐름 패턴**이 반복적으로 나타납니다.
*   각 게이트는 그래디언트를 뒤로 전파하는 특징적인 방식을 가지고 있습니다.

**덧셈 게이트: 그래디언트 분배기**

**(이미지: 덧셈 게이트가 상류 그래디언트를 입력으로 그대로 분배하는 것을 보여주는 그림)**

***

### **페이지 549**

**그래디언트 흐름의 패턴**

*   신경망에서는 대부분의 연산이 몇 가지 간단한 **계산 게이트**로 분해될 수 있기 때문에 동일한 **그래디언트 흐름 패턴**이 반복적으로 나타납니다.
*   각 게이트는 그래디언트를 뒤로 전파하는 특징적인 방식을 가지고 있습니다.

**덧셈 게이트: 그래디언트 분배기**
**곱셈 게이트: "교환 곱셈기"**

**(이미지: 덧셈 게이트와 곱셈 게이트의 그래디언트 흐름 패턴을 보여주는 그림)**

***

### **페이지 550**

**그래디언트 흐름의 패턴**

*   신경망에서는 대부분의 연산이 몇 가지 간단한 **계산 게이트**로 분해될 수 있기 때문에 동일한 **그래디언트 흐름 패턴**이 반복적으로 나타납니다.
*   각 게이트는 그래디언트를 뒤로 전파하는 특징적인 방식을 가지고 있습니다.

**덧셈 게이트: 그래디언트 분배기**
**곱셈 게이트: "교환 곱셈기"**
**복사 게이트: 그래디언트 합산기**
**최대값 게이트: 그래디언트 라우터**

**(이미지: 덧셈, 곱셈, 복사, 최대값 게이트의 그래디언트 흐름 패턴을 보여주는 그림)**

***

### **페이지 551**

**구현**

*   **순전파**

**(코드 및 이미지: 간단한 신경망의 순전파 과정을 보여주는 코드와, 계산 그래프에 순전파 계산 결과를 표시한 그림)**

***

### **페이지 552**

**구현**

*   **순전파**
*   **역전파 (그래디언트 계산)**

**(코드 및 이미지: 순전파와 역전파 과정을 보여주는 코드와, 계산 그래프에 역전파 계산 결과를 표시한 그림)**

***

### **페이지 553**

**구현**

*   **순전파**
*   **역전파 (그래디언트 계산)**

**(코드 및 이미지: 시그모이드 게이트에 해당하는 순전파와 역전파 코드 부분을 강조)**

***

### **페이지 554**

**구현**

*   **순전파**
*   **역전파 (그래디언트 계산)**

**(코드 및 이미지: 덧셈 게이트에 해당하는 순전파와 역전파 코드 부분을 강조)**

***

### **페이지 555**

**구현**

*   **순전파**
*   **역전파 (그래디언트 계산)**

**(코드 및 이미지: 또 다른 덧셈 게이트에 해당하는 순전파와 역전파 코드 부분을 강조)**

***

### **페이지 556**

**구현**

*   **순전파**
*   **역전파 (그래디언트 계산)**

**(코드 및 이미지: 곱셈 게이트에 해당하는 순전파와 역전파 코드 부분을 강조)**

***

### **페이지 557**

**구현**

*   **순전파**
*   **역전파 (그래디언트 계산)**

**(코드 및 이미지: 또 다른 곱셈 게이트에 해당하는 순전파와 역전파 코드 부분을 강조)**

***

### **페이지 558**

**구현**

*   **업데이트 단계 (경사 하강법)**
*   **역전파 (그래디언트 계산)**

**(코드 및 이미지: 계산된 그래디언트를 사용하여 가중치를 업데이트하는 코드와 과정을 강조)**

***

### **페이지 559**

**계산 그래프에서 순전파 및 역전파의 모듈식 구현**

*   이러한 계산 그래프에서, 순전파와 역전파의 구현 및 공식화에 따라, 우리는 각 노드(게이트 또는 함수)를 코드에서 API나 객체와 같은 독립적인 구성 요소로 모듈화할 수 있습니다.
*   예를 들어, **곱셈 게이트**의 경우 다음과 같이 구현할 수 있습니다.

**(코드 및 이미지: 곱셈 게이트를 순전파와 역전파 메서드를 가진 클래스로 구현한 예시)**

***

### **페이지 560**

**실제 PyTorch에서의 구현**

*   실제 PyTorch에서는 순전파와 역전파가 이러한 모듈식 방식으로 구현됩니다.
*   각 연산(예: 곱셈)은 `torch.autograd.Function`의 서브클래스로 정의되며, `forward()` 메서드는 계산을 수행하고 `backward()` 메서드는 그래디언트가 어떻게 전파되는지를 정의합니다.

**(이미지: PyTorch에서 곱셈 연산을 위한 모듈화된 구현 코드 예시)**

***

### **페이지 561**

**PyTorch에서의 구현**

*   실제 구현은 공식 PyTorch GitHub 저장소에서 찾을 수 있습니다.
*   하지만, 소스 코드는 여러 모듈에 분산되어 있고 성능을 위해 고도로 최적화되어 있어, 계산 흐름을 읽거나 따라가기가 간단하지 않습니다.

**(이미지: PyTorch의 GitHub 저장소에서 시그모이드 함수 커널의 C++ 구현 코드를 보여주는 스크린샷)**

***

### **페이지 562**

**이제 우리는 신경망의 모든 그래디언트를 계산할 수 있다**

*   이 시점에서, 우리는 **역전파**를 사용하여 신경망 전체의 **모든 그래디언트**를 계산할 수 있습니다.
*   **연쇄 법칙**을 층별로 체계적으로 적용함으로써, 우리는 **모든 파라미터**(가중치와 편향)에 대한 손실의 그래디언트를 얻을 수 있습니다.

**(이미지: 2계층 신경망의 전체 구조와 손실 계산 과정을 다시 보여줌)**

***

### **페이지 563**

**이제 우리는 신경망의 모든 그래디언트를 계산할 수 있다**

*   이 시점에서, 우리는 **역전파**를 사용하여 신경망 전체의 **모든 그래디언트**를 계산할 수 있습니다.
*   **연쇄 법칙**을 층별로 체계적으로 적용함으로써, 우리는 **모든 파라미터**(가중치와 편향)에 대한 손실의 그래디언트를 얻을 수 있습니다.

**(이미지: 첫 번째 은닉 뉴런(z₁)에 연결된 가중치들(W¹₁₁, W¹₁₂, ...)을 보여줌)**

***

### **페이지 564**

**이제 우리는 신경망의 모든 그래디언트를 계산할 수 있다**

*   이 시점에서, 우리는 **역전파**를 사용하여 신경망 전체의 **모든 그래디언트**를 계산할 수 있습니다.
*   **연쇄 법칙**을 층별로 체계적으로 적용함으로써, 우리는 **모든 파라미터**(가중치와 편향)에 대한 손실의 그래디언트를 얻을 수 있습니다.

**(이미지: 첫 번째 은닉 뉴런으로 들어오는 상류 그래디언트(∂L/∂z₁)를 표시)**

***

### **페이지 565**

**이제 우리는 신경망의 모든 그래디언트를 계산할 수 있다**

*   이 시점에서, 우리는 **역전파**를 사용하여 신경망 전체의 **모든 그래디언트**를 계산할 수 있습니다.
*   **연쇄 법칙**을 층별로 체계적으로 적용함으로써, 우리는 **모든 파라미터**(가중치와 편향)에 대한 손실의 그래디언트를 얻을 수 있습니다.

**(이미지: 상류 그래디언트를 사용하여 가중치 W¹₁₁에 대한 하류 그래디언트(∂L/∂W¹₁₁)를 계산해야 함을 보여줌)**

***

### **페이지 566**

**이제 우리는 신경망의 모든 그래디언트를 계산할 수 있다**

*   이 시점에서, 우리는 **역전파**를 사용하여 신경망 전체의 **모든 그래디언트**를 계산할 수 있습니다.
*   **연쇄 법칙**을 층별로 체계적으로 적용함으로써, 우리는 **모든 파라미터**(가중치와 편향)에 대한 손실의 그래디언트를 얻을 수 있습니다.

**(이미지 및 수식: 연쇄 법칙(하류 = 상류 × 지역)을 사용하여 가중치에 대한 그래디언트를 계산하는 과정을 보여줌)**

***

### **페이지 567**

**이제 우리는 신경망의 모든 그래디언트를 계산할 수 있다**

*   이 시점에서, 우리는 **역전파**를 사용하여 신경망 전체의 **모든 그래디언트**를 계산할 수 있습니다.
*   **연쇄 법칙**을 층별로 체계적으로 적용함으로써, 우리는 **모든 파라미터**(가중치와 편향)에 대한 손실의 그래디언트를 얻을 수 있습니다.

**(수식: 첫 번째 은닉 뉴런의 출력(z₁)이 입력과 가중치의 함수임을 보여주는 수식)**

***

### **페이지 568**

**이제 우리는 신경망의 모든 그래디언트를 계산할 수 있다**

*   이 시점에서, 우리는 **역전파**를 사용하여 신경망 전체의 **모든 그래디언트**를 계산할 수 있습니다.
*   **연쇄 법칙**을 층별로 체계적으로 적용함으로써, 우리는 **모든 파라미터**(가중치와 편향)에 대한 손실의 그래디언트를 얻을 수 있습니다.

**(수식: 첫 번째 은닉 뉴런에 연결된 모든 가중치에 대한 그래디언트 계산 공식을 보여줌)**

***

### **페이지 569**

**이제 우리는 신경망의 모든 그래디언트를 계산할 수 있다**

*   이 시점에서, 우리는 **역전파**를 사용하여 신경망 전체의 **모든 그래디언트**를 계산할 수 있습니다.
*   **연쇄 법칙**을 층별로 체계적으로 적용함으로써, 우리는 **모든 파라미터**(가중치와 편향)에 대한 손실의 그래디언트를 얻을 수 있습니다.

**(이미지: 계산된 그래디언트들이 D개의 요소를 가짐을 표시)**

***

### **페이지 570**

**단일 출력 노드에 대한 벡터화된 그래디언트**

*   우리는 그것들을 단일 **벡터 형태**로 그룹화할 수 있습니다.
*   여기서, W₁¹은 첫 번째 레이어의 **첫 번째 출력 뉴런**으로 들어가는 **전체 가중치 집합**을 나타냅니다.
*   이 벡터화된 표기법은 표현을 간결하게 만들고 행렬 연산을 사용하여 효율적인 계산을 가능하게 합니다.

**(수식: 스칼라 그래디언트 계산을 벡터화된 형태로 변환하는 과정을 보여줌)**

***

### **페이지 571**

**단일 출력 노드에 대한 벡터화된 그래디언트**

*   우리는 그것들을 단일 **벡터 형태**로 그룹화할 수 있습니다.
*   여기서, W₁¹은 첫 번째 레이어의 **첫 번째 출력 뉴런**으로 들어가는 **전체 가중치 집합**을 나타냅니다.
*   이 벡터화된 표기법은 표현을 간결하게 만들고 행렬 연산을 사용하여 효율적인 계산을 가능하게 합니다.

**(이미지 및 수식: 두 번째 은닉 뉴런에 대한 그래디언트 계산 과정을 보여줌)**

***

### **페이지 572**

**모든 출력 노드로 그래디언트 확장하기**

*   우리는 그래디언트 표현을 레이어의 **모든 출력 노드**를 포함하도록 확장할 수 있습니다.
*   모든 출력 노드를 함께 쌓음으로써, 그래디언트는 **행렬**을 형성합니다.

**(이미지 및 수식: 모든 은닉 뉴런에 대한 그래디언트 계산을 쌓아서 행렬 형태로 만드는 과정을 보여줌)**

***

### **페이지 573**

**모든 출력 노드로 그래디언트 확장하기**

*   우리는 그래디언트 표현을 레이어의 **모든 출력 노드**를 포함하도록 확장할 수 있습니다.
*   모든 출력 노드를 함께 쌓음으로써, 그래디언트는 **행렬**을 형성합니다.

**(이미지: 이전 수식에서 각 그래디언트 그룹(하류, 상류, 지역)을 시각적으로 구분)**

***

### **페이지 574**

**모든 출력 노드로 그래디언트 확장하기**

*   우리는 그래디언트 표현을 레이어의 **모든 출력 노드**를 포함하도록 확장할 수 있습니다.
*   모든 출력 노드를 함께 쌓음으로써, 그래디언트는 **행렬**을 형성합니다.

**(수식: 최종적으로 가중치 행렬 W¹에 대한 그래디언트(∂L/∂W¹)가 상류 그래디언트(∂L/∂z)와 지역 그래디언트(∂z/∂W¹)의 곱으로 표현됨을 행렬 차원과 함께 보여줌)**

***

### **페이지 575**

**역전파의 지역 자코비안**

*   역전파에서, ∂z/∂W¹ 항은 **지역 자코비안**이라고 불립니다. 이것은 중간 변수 z가 파라미터 행렬 W¹에 대해 어떻게 변하는지를 나타냅니다.
*   **자코비안**은 벡터 값 함수의 모든 1차 편도함수의 행렬입니다.

**(수식 및 이미지: 지역 자코비안의 개념을 설명하고, 일반적인 자코비안 행렬의 정의를 보여줌)**

***

### **페이지 576**

**완전 연결 레이어의 지역 자코비안**

*   완전 연결 레이어 z = W¹x에 대해, ∂zᵢ/∂W¹ᵢⱼ = xⱼ 입니다.
*   따라서, W¹에 대한 지역 자코비안은 **행이 반복되는 구조**를 가집니다.
*   이를 통해 우리는 그래디언트를 **외적(outer-product)**으로 단순화할 수 있습니다.

**(수식: 완전 연결 레이어의 지역 자코비안이 입력 벡터 x의 외적으로 단순화되는 과정을 보여줌)**

***

### **페이지 577**

**모든 출력 노드로 그래디언트 확장하기**

*   우리는 그래디언트 표현을 레이어의 **모든 출력 노드**를 포함하도록 확장할 수 있습니다.
*   모든 출력 노드를 함께 쌓음으로써, 그래디언트는 **행렬**을 형성합니다.

**(이미지 및 수식: 최종적으로 가중치 행렬 W¹에 대한 그래디언트가 상류 그래디언트 벡터와 입력 벡터 x의 외적으로 계산됨을 보여주는 다이어그램과 수식)**

***

### **페이지 578**

**2계층 신경망에서 그래디언트 유도 완료하기**

*   먼저, 두 번째 (출력) 레이어에 대한 그래디언트를 계산합니다.
*   그런 다음, 첫 번째 (은닉) 레이어를 통해 역전파합니다.
*   ∂L/∂W²와 ∂L/∂W¹이 모두 유도되면, 네트워크의 모든 가중치에 대한 그래디언트가 계산된 것입니다.

**(이미지 및 수식: 2계층 신경망의 역전파 과정을 보여주는 다이어그램. 각 레이어의 가중치에 대한 그래디언트 계산 공식을 보여줌)**

***

### **페이지 579**

**2계층 신경망에서 그래디언트 유도 완료하기**

*   먼저, 두 번째 (출력) 레이어에 대한 그래디언트를 계산합니다.
*   그런 다음, 첫 번째 (은닉) 레이어를 통해 역전파합니다.
*   ∂L/∂W²와 ∂L/∂W¹이 모두 유도되면, 네트워크의 모든 가중치에 대한 그래디언트가 계산된 것입니다.
*   이 시점에서, **그래디언트 계산은 완료**됩니다 — 모든 파라미터는 경사 하강법을 사용하여 업데이트될 수 있습니다.

**(수식: 계산된 그래디언트를 사용하여 각 가중치 행렬을 업데이트하는 공식을 보여줌)**

***

### **페이지 580**

**미니배치 경사 하강법**

*   **단일 샘플 훈련에서 배치 훈련으로**
    *   지금까지 우리는 **배치 크기 = 1**인 경우, 즉 한 번에 **단 하나의 훈련 예제**만 가져와 파라미터를 업데이트하는 경우를 고려했습니다.
*   하지만, 실제로는 단일 샘플을 사용하여 모델을 업데이트하지 않습니다.
    *   대신, N개의 샘플로 구성된 **미니배치**를 사용하고 배치에 대한 그래디언트를 계산합니다.

**(이미지 및 수식: 배치 크기가 1일 때의 훈련 과정을 보여주는 다이어그램과 수식)**

***

### **페이지 581**

**미니배치 경사 하강법**

*   **단일 샘플 훈련에서 배치 훈련으로**
    *   지금까지 우리는 **배치 크기 = 1**인 경우를 고려했습니다.
*   하지만, 실제로는 단일 샘플을 사용하여 모델을 업데이트하지 않습니다.
    *   대신, N개의 샘플로 구성된 **미니배치**를 사용하고 배치에 대한 그래디언트를 계산합니다.

**(이미지 및 수식: 배치 크기가 N일 때의 훈련 과정을 보여주는 다이어그램과 수식. 행렬 차원이 N을 포함하도록 변경됨)**

***

### **페이지 582**

**중간 노드 z의 그래디언트**

*   참고로, **중간 노드 z**에 대한 그래디언트 계산은 약간 다릅니다.
    *   각 **가중치**는 **하나의 출력 노드**에만 영향을 미칩니다. 반면, 입력 노드 zⱼ는 **모든 출력 노드**에 기여합니다.
    *   따라서, 그 그래디언트는 모든 출력에서 오는 편도함수를 **누적(합산)**해야 합니다.
*   이는 zⱼ가 모든 출력에 영향을 분산시킨다는 사실을 반영하며, 따라서 역전파 동안 **모든 경로**로부터의 그래디언트가 합산되어야 합니다.

**(이미지: 은닉 노드 z₁이 모든 출력 노드에 영향을 미치므로, z₁의 그래디언트를 계산하려면 모든 경로를 고려해야 함을 보여주는 그림)**

***

### **페이지 583**

**중간 노드 z의 그래디언트**

*   참고로, **중간 노드 z**에 대한 그래디언트 계산은 약간 다릅니다.
    *   각 **가중치**는 **하나의 출력 노드**에만 영향을 미칩니다. 반면, 입력 노드 zⱼ는 **모든 출력 노드**에 기여합니다.
    *   따라서, 그 그래디언트는 모든 출력에서 오는 편도함수를 **누적(합산)**해야 합니다.
*   이는 zⱼ가 모든 출력에 영향을 분산시킨다는 사실을 반영하며, 따라서 역전파 동안 **모든 경로**로부터의 그래디언트가 합산되어야 합니다.

**(수식: z₁에 대한 그래디언트가 모든 출력(ŷ₁, ŷ₂, ..., ŷ_c)으로부터의 그래디언트 기여도를 합산하여 계산됨을 연쇄 법칙으로 보여줌)**

***

### **페이지 584**

**중간 노드 z의 그래디언트**

*   참고로, **중간 노드 z**에 대한 그래디언트 계산은 약간 다릅니다.
    *   각 **가중치**는 **하나의 출력 노드**에만 영향을 미칩니다. 반면, 입력 노드 zⱼ는 **모든 출력 노드**에 기여합니다.
    *   따라서, 그 그래디언트는 모든 출력에서 오는 편도함수를 **누적(합산)**해야 합니다.
*   이는 zⱼ가 모든 출력에 영향을 분산시킨다는 사실을 반영하며, 따라서 역전파 동안 **모든 경로**로부터의 그래디언트가 합산되어야 합니다.

**(수식: 이전 수식을 벡터와 행렬의 곱 형태로 간결하게 표현)**

***

### **페이지 585**

**중간 노드 z의 그래디언트**

*   참고로, **중간 노드 z**에 대한 그래디언트 계산은 약간 다릅니다.
    *   각 **가중치**는 **하나의 출력 노드**에만 영향을 미칩니다. 반면, 입력 노드 zⱼ는 **모든 출력 노드**에 기여합니다.
    *   따라서, 그 그래디언트는 모든 출력에서 오는 편도함수를 **누적(합산)**해야 합니다.
*   이는 zⱼ가 모든 출력에 영향을 분산시킨다는 사실을 반영하며, 따라서 역전파 동안 **모든 경로**로부터의 그래디언트가 합산되어야 합니다.

**(이미지 및 수식: 은닉 노드 z₂에 대한 그래디언트 계산 과정을 보여줌)**

***

### **페이지 586**

**중간 노드 z의 그래디언트**

*   참고로, **중간 노드 z**에 대한 그래디언트 계산은 약간 다릅니다.
    *   각 **가중치**는 **하나의 출력 노드**에만 영향을 미칩니다. 반면, 입력 노드 zⱼ는 **모든 출력 노드**에 기여합니다.
    *   따라서, 그 그래디언트는 모든 출력에서 오는 편도함수를 **누적(합산)**해야 합니다.
*   이는 zⱼ가 모든 출력에 영향을 분산시킨다는 사실을 반영하며, 따라서 역전파 동안 **모든 경로**로부터의 그래디언트가 합산되어야 합니다.

**(수식: 모든 은닉 노드(z₁부터 z_H까지)에 대한 그래디언트 계산 공식을 보여줌)**

***

### **페이지 587**

**중간 노드 z의 그래디언트**

*   참고로, **중간 노드 z**에 대한 그래디언트 계산은 약간 다릅니다.
    *   각 **가중치**는 **하나의 출력 노드**에만 영향을 미칩니다. 반면, 입력 노드 zⱼ는 **모든 출력 노드**에 기여합니다.
    *   따라서, 그 그래디언트는 모든 출력에서 오는 편도함수를 **누적(합산)**해야 합니다.
*   이는 zⱼ가 모든 출력에 영향을 분산시킨다는 사실을 반영하며, 따라서 역전파 동안 **모든 경로**로부터의 그래디언트가 합산되어야 합니다.

**(이미지: 이전 수식에서 각 그래디언트 그룹을 시각적으로 구분)**

***

### **페이지 588**

**중간 노드 z의 그래디언트**

*   참고로, **중간 노드 z**에 대한 그래디언트 계산은 약간 다릅니다.
    *   각 **가중치**는 **하나의 출력 노드**에만 영향을 미칩니다. 반면, 입력 노드 zⱼ는 **모든 출력 노드**에 기여합니다.
    *   따라서, 그 그래디언트는 모든 출력에서 오는 편도함수를 **누적(합산)**해야 합니다.
*   이는 zⱼ가 모든 출력에 영향을 분산시킨다는 사실을 반영하며, 따라서 역전파 동안 **모든 경로**로부터의 그래디언트가 합산되어야 합니다.

**(수식: 전체 그래디언트 계산을 행렬과 벡터의 곱으로 표현)**

***

### **페이지 589**

**중간 노드 z의 그래디언트**

*   참고로, **중간 노드 z**에 대한 그래디언트 계산은 약간 다릅니다.
    *   각 **가중치**는 **하나의 출력 노드**에만 영향을 미칩니다. 반면, 입력 노드 zⱼ는 **모든 출력 노드**에 기여합니다.
    *   따라서, 그 그래디언트는 모든 출력에서 오는 편도함수를 **누적(합산)**해야 합니다.
*   이는 zⱼ가 모든 출력에 영향을 분산시킨다는 사실을 반영하며, 따라서 역전파 동안 **모든 경로**로부터의 그래디언트가 합산되어야 합니다.

**(이미지: 이전 수식에서 지역 자코비안 부분을 강조)**

***

### **페이지 590**

**완전 연결 레이어의 지역 자코비안**

*   선형 레이어의 역전파를 고려해 봅시다: ŷ = W²z
*   지역 자코비안은 **가중치 행렬**로 축소됩니다: ∂ŷᵢ/∂zⱼ = W²

**(수식: 완전 연결 레이어의 지역 자코비안이 가중치 행렬 W²와 동일함을 보여주는 유도 과정)**

***

### **페이지 591**

**중간 노드 z의 그래디언트**

*   선형 레이어의 역전파를 고려해 봅시다: ŷ = W²z
*   지역 자코비안은 **가중치 행렬**로 축소됩니다: ∂ŷᵢ/∂zⱼ = W²

**(수식: 최종적으로 z에 대한 그래디언트가 가중치 행렬의 전치와 ŷ에 대한 그래디언트의 곱으로 계산됨을 보여줌)**

***

### **페이지 592**

**중간 노드 z의 그래디언트**

*   선형 레이어의 역전파를 고려해 봅시다: ŷ = W²z
*   지역 자코비안은 **가중치 행렬**로 축소됩니다: ∂ŷᵢ/∂zⱼ = W²

**(수식: 이전 수식을 배치 크기 N을 고려한 행렬 차원으로 확장)**

***

### **페이지 593**

**참고: 비선형 활성화(예: ReLU)에서의 그래디언트 흐름**

*   y=max(0,x)로 정의되는 ReLU와 같은 활성화 함수의 경우, 그래디언트는 입력 값에 따라 다르게 동작합니다.
*   **입력이 0보다 클 때:**
    *   상류 그래디언트는 변경되지 않고 그대로 통과됩니다.
*   **입력이 0보다 작거나 같을 때:**
    *   이 경로를 통해 그래디언트가 뒤로 흐르지 않습니다.
*   따라서, ReLU는 **"그래디언트 게이트"** 역할을 합니다.
    *   비활성 뉴런에 대한 그래디언트를 차단하고, 활성 뉴런(x > 0)을 통해서만 흐르도록 허용합니다.

**(이미지: ReLU 함수의 순전파와 역전파 과정을 보여주는 다이어그램)**

***

### **페이지 594**

**요약: 역전파**

*   **역전파**는 **연쇄 법칙**을 계산 그래프를 따라 재귀적으로 적용하여 모든 **입력, 파라미터, 중간 노드**에 대한 그래디언트를 계산합니다.
*   실제 구현에서는 그래프 구조가 보존됩니다 — 각 노드는 두 개의 API를 제공합니다:
    *   **순전파**: 연산의 출력을 계산하고 그래디언트 계산에 필요한 **중간 결과를 저장**합니다.
    *   **역전파**: 최종 **손실 함수**에서 시작하여 **연쇄 법칙을 역순으로 적용**하여 입력에 대한 그래디언트를 계산합니다.
*   **모든 파라미터**에 대한 그래디언트가 얻어지면,
    이를 사용하여 **모델 파라미터를 업데이트**할 수 있습니다 (예: 경사 하강법을 통해).
*   요컨대, 역전파는 계산 그래프를 통해 **그래디언트를 뒤로 체계적으로 전파**하여 신경망에서 효율적인 학습을 가능하게 하는 것입니다.

***

### **페이지 595**

**정규화**

**(이미지: 데이터 과학을 상징하는 그래픽)**

***

### **페이지 596**

**복습: 데이터셋 전체에 대한 총 손실**

*   이것은 네트워크가 단일 이미지가 아닌 **전체 훈련 세트**에서 얼마나 잘 수행하는지를 나타냅니다.
*   우리의 목표는 파라미터 W를 조정하여 이 **총 손실을 최소화**하는 것입니다.

**(이미지 및 수식: 총 손실을 최소화하는 것이 목표임을 다시 한번 강조하는 다이어그램)**

***

### **페이지 597**

**손실이 0에 도달할 때까지 훈련해야 하는가?**

*   언뜻 보기에는, 신경망이 100% 정확도를 달성하거나 **훈련 손실이 0**이 될 때까지 계속 훈련하는 것이 이상적으로 보일 수 있습니다.
*   하지만, 이것이 반드시 **좋은 방법은 아닙니다**.

**(이미지 및 수식: 이전 페이지의 다이어그램을 반복하여 보여줌)**

***

### **페이지 598**

**간단한 장난감 예시: 5개의 데이터 포인트에 대한 과적합**

*   2D 플롯에 **단 5개의 훈련 지점**만 있다고 가정해 봅시다 — 각 점은 입력 x와 그 목표 출력 y를 나타냅니다.

**(이미지: 2D 공간에 5개의 데이터 포인트가 흩어져 있는 그래프)**

***

### **페이지 599**

**간단한 장난감 예시: 5개의 데이터 포인트에 대한 과적합**

*   **사례 1 (f₁): 과도하게 복잡한 모델 (과적합)**
    *   5개의 모든 점을 **정확하게** 통과하는 **고차 다항식**(예: 10차)을 적합시킵니다.
    *   **훈련 손실 = 0**이지만, 곡선은 점들 사이에서 심하게 진동합니다.
*   **사례 2 (f₂): 단순한 모델 (좋은 일반화)**
    *   **직선**(예: 선형 회귀)을 적합시킵니다.
    *   선이 모든 점을 정확하게 통과하지는 않지만, **전체적인 추세**를 포착합니다.

**(이미지: 5개의 데이터 포인트에 대해 과적합된 복잡한 곡선(f₁)과 잘 일반화된 단순한 직선(f₂)을 비교하는 그래프)**

***

### **페이지 600**

**간단한 장난감 예시: 5개의 데이터 포인트에 대한 과적합**

*   **사례 1 (f₁): 과도하게 복잡한 모델 (과적합)**
    *   → 훈련 데이터에는 완벽하지만, 새로운 데이터에 대해서는 **일반화 성능이 나쁨**.
*   **사례 2 (f₂): 단순한 모델 (좋은 일반화)**
    *   → 훈련 데이터에 약간의 오차가 있지만, 보지 못한 데이터에 대해서는 **좋은 일반화 성능**.

**(이미지: 새로운 데이터 포인트(흰색 원)가 주어졌을 때, f₁은 오차가 크고 f₂는 오차가 작음을 보여주는 그림)**

***

### **페이지 601**

**일반화와 과적합**

*   **과적합 위험**
    *   모델이 훈련 데이터를 **너무 완벽하게** 맞추면, 새로운 데이터에 일반화되지 않는 노이즈와 특정 패턴을 **암기**하기 시작합니다.
    *   결과적으로, **훈련 손실은 감소**하지만, **검증 손실과 정확도는 악화**됩니다.
*   **훈련의 목표**
    *   진정한 목표는 완벽한 피팅이 아니라 **일반화**입니다.
    *   우리는 훈련 세트뿐만 아니라 **보지 못한 (검증/테스트) 데이터**에서도 잘 수행하는 모델을 원합니다.

**(이미지: 미적합(Underfitted), 좋은 적합(Good Fit/Robust), 과적합(Overfitted) 상태를 보여주는 세 개의 그래프 비교)**

***

### **페이지 602**

**정규화: 과적합 방지하기**

*   과적합을 방지하기 위한 한 가지 효과적인 전략은 **훈련 과정을 정규화**하는 것입니다.
*   우리의 현재 손실 함수는 모델의 예측이 훈련 예제와 얼마나 잘 맞는지를 측정하는 **데이터 손실**만 포함합니다.
*   이는 모델이 데이터를 **가능한 한 가깝게** 맞추도록 유도하며, 잠재적으로 과적합의 대가를 치르게 됩니다.

**(수식: 데이터 손실만 포함된 총 손실 함수)**

***

### **페이지 603**

**정규화: 과적합 방지하기**

*   **정규화된 손실 함수:**
    *   과도하게 복잡한 모델에 불이익을 주는 **정규화 항**을 포함하도록 목적 함수를 수정할 수 있습니다:
        *   R(W): 모델 복잡도를 측정하는 정규화 함수
        *   λ: 정규화의 강도를 제어하는 하이퍼파라미터
*   **정규화 항**은 모델이 파라미터 크기 면에서 단순하거나 작게 유지되도록 유도합니다.

**(수식: 데이터 손실과 정규화 항을 모두 포함하는 총 손실 함수)**

***

### **페이지 604**

**정규화의 종류: L2와 L1**

*   **L2 정규화 (릿지 회귀)**
    *   손실 함수에 **가중치의 제곱의 합**을 더합니다.
    *   가중치가 **작지만 0이 아니도록** 유도합니다.
*   **L1 정규화 (라쏘 회귀)**
    *   가중치의 **절댓값의 합**을 더합니다.
    *   **희소성(sparsity)**을 유도합니다 — 일부 가중치를 정확히 **0**으로 만듭니다.

**(수식: L1 및 L2 정규화 항의 공식과, 이를 포함한 전체 손실 함수)**

***

### **페이지 605**

**L1 vs L2 정규화의 기하학적 관점**

*   정규화는 가중치 벡터 W = (w₁, w₂)에 **제약 조건**을 부과하는 것으로 기하학적으로 이해할 수 있습니다.
*   우리는 가중치가 원점으로부터 특정 **거리** 내에 있도록 제한합니다.

**(이미지: L1 거리(마름모)와 L2 거리(원)의 제약 조건을 기하학적으로 비교하는 그림)**

***

### **페이지 606**

**L1 vs L2 정규화의 기하학적 관점**

*   정규화는 가중치 벡터 W = (w₁, w₂)에 **제약 조건**을 부과하는 것으로 기하학적으로 이해할 수 있습니다.
*   우리는 가중치가 원점으로부터 특정 **거리** 내에 있도록 제한합니다.

*   → **작고 고르게 분포된** 가중치를 장려합니다.
*   → **희소성**(일부 가중치는 정확히 0)을 장려합니다.

**(이미지: L2 제약(원)과 L1 제약(마름모) 하에서 손실 함수의 등고선이 최적점을 찾아가는 과정을 비교. L1은 축에 닿기 쉬워 희소성을 유도함을 보여줌)**

***

### **페이지 607**

**L1 vs L2 정규화의 기하학적 관점**

*   정규화는 가중치 벡터 W = (w₁, w₂)에 **제약 조건**을 부과하는 것으로 기하학적으로 이해할 수 있습니다.
*   우리는 가중치가 원점으로부터 특정 **거리** 내에 있도록 제한합니다.

**(이미지: L1과 L2 제약 영역과, 다양한 손실 함수의 등고선이 만나는 모습을 보여주는 그림)**

***

### **페이지 608**

**L1 vs L2 정규화 선호도**

*   **L2는 w₂ (분산된 가중치)를 선호합니다**
    *   가중치가 퍼져 있는 것을 선호합니다.
    *   차원 전반에 걸쳐 가중치를 부드럽게 만듭니다.
*   **L1은 w₁ (하나의 0이 아닌 가중치)을 선호합니다**
    *   가중치의 희소성을 장려합니다 (많은 0).
    *   가중치를 정확히 0으로 만듭니다.

**(이미지 및 수식: 동일한 결과를 내는 두 가중치 벡터 w₁=과 w₂=[0.25,0.25,0.25,0.25]에 대해 L1과 L2 정규화가 각각 어느 것을 선호할지 묻는 예시)**

***

### **페이지 609**

**개념도**

*   정규화가 포함된 손실은 다음과 같이 시각화될 수 있습니다:
    *   입력 xᵢ가 모델에 들어가 → 점수 f(xᵢ, W)를 생성
    *   점수 → 소프트맥스 → 교차 엔트로피 → **데이터 손실**
    *   가중치 W는 **정규화 항**에도 입력됨
    *   최종 손실 = 데이터 손실 + 정규화 손실

**(이미지: 데이터 손실과 정규화 손실이 결합되어 최종 손실 L을 구성하는 과정을 보여주는 다이어그램)**

***

### **페이지 610**

**정규화를 사용한 역전파**

*   전체 손실은 두 항의 합이므로, **역전파**는 **두 가지 모두**에 대한 그래디언트를 계산해야 합니다.
    *   **데이터 손실**로부터의 그래디언트는 네트워크를 통해 뒤로 흐릅니다 (파란색 경로).
    *   **정규화 항**으로부터의 그래디언트는 W로 직접 흐릅니다 (녹색 경로).
    *   이 그래디언트들은 가중치를 업데이트하기 위해 **함께 더해집니다**.

**(이미지: 이전 다이어그램에 데이터 손실과 정규화 손실 각각에 대한 역전파 경로를 화살표로 표시)**

***

### **페이지 611**

**정규화 강도(λ)의 효과**

*   정규화 강도 λ는 우리가 큰 가중치에 얼마나 강하게 불이익을 줄지를 제어합니다:
*   **큰 λ** → 강한 정규화
    *   모델 가중치가 더 많이 줄어듭니다.
    *   결정 경계가 더 **단순해지거나 부드러워집니다**.
    *   과적합이 감소합니다.
    *   하지만 너무 크면 **미적합(underfitting)의 위험**이 증가합니다.

**(이미지: λ 값이 0.001, 0.01, 0.1로 커짐에 따라 결정 경계가 점점 더 단순해지는 것을 보여주는 그림)**

***

### **페이지 612**

**요약: 일반화 vs. 과적합**

*   **과적합 위험**
    *   모델이 훈련 데이터를 **너무 완벽하게** 맞추면, 노이즈를 **암기**하기 시작합니다.
    *   훈련 손실은 계속 감소하지만, **검증 손실은 증가하고 테스트 정확도는 떨어집니다**.
*   **훈련의 목표**
    *   진정한 목표는 0의 훈련 손실이 아니라 **일반화**입니다.
    *   우리는 훈련 샘플뿐만 아니라 **보지 못한 데이터**에서도 잘 수행하는 모델을 원합니다.
*   **조기 종료 (Early Stopping)**
    *   일반적인 전략은 **검증 손실**을 모니터링하는 것입니다.
    *   훈련 손실이 계속 감소하더라도 검증 성능이 향상되지 않으면 멈춥니다.
*   **정규화 및 일반화 기법**
    *   드롭아웃, 가중치 감쇠(L2 정규화), 데이터 증강, 조기 종료는 모두 과적합을 방지하는 데 도움이 됩니다.
    *   이러한 기법들은 피팅과 일반화 사이의 균형을 유지합니다.
*   손실이 0이 될 때까지 훈련하는 것은 종종 개선이 아닌 **과적합**으로 이어집니다.
    대신, 최적의 중단 지점은 모델이 **최고의 검증 성능**을 달성할 때 — 새로운 데이터에 가장 잘 일반화될 때입니다.

***

### **페이지 613**

**조기 종료**

*   **조기 종료**
    *   일반적인 전략은 **검증 손실**을 모니터링하는 것입니다.
    *   훈련 손실이 계속 감소하더라도 검증 성능이 향상되지 않으면 멈춥니다.

**(이미지: 훈련 오차는 계속 감소하지만 검증 오차는 특정 지점(조기 종료 지점)에서 다시 증가하기 시작하는 것을 보여주는 그래프)**

***

### **페이지 614**

**최적의 모델 복잡도 선택하기**

*   최적의 모델 복잡도를 찾기 위해, 우리는 다른 설정 하에서 성능을 평가합니다:
    *   정규화 강도 (λ)
    *   모델 아키텍처 용량 (레이어 수, 뉴런 수, 너비/깊이).
*   우리는 이러한 구성에 걸쳐 **훈련 오차**와 **검증 오차**를 비교하여 미적합과 과적합 사이의 최상의 절충점을 식별합니다.
    *   최적 지점 = **최소 검증 오차**

**(이미지: 모델 복잡도에 따라 훈련 오차와 검증 오차가 어떻게 변하는지를 보여주는 그래프. 최적의 모델 복잡도는 검증 오차가 가장 낮은 지점임)**

***

### **페이지 615**

**참고 자료**

*   Stanford CS231N | 2025년 봄 | 강의 3: 정규화 및 최적화
    *   https://www.youtube.com/watch?v=dyNGd06MWn4&list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16&index=3
*   Stanford CS231N | 2025년 봄 | 강의 4: 신경망과 역전파
    *   https://www.youtube.com/watch?v=25zD5qJHYsk&list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16&index=4

***

### **페이지 616**

**컴퓨터 비전의 기초**
**2025년 2학기**
**- 엄광현 –**
**인공지능학과**

***

### **페이지 617**

**강의 8:**
**합성곱 신경망 (CNN)**
*   왜 합성곱 신경망인가
*   합성곱 신경망이란 무엇인가

***

### **페이지 618**

**요약: 지금까지 배운 것**

*   우리는 신경망을 처음부터 훈련시키기 위한 완전한 기초를 구축했습니다.
    지금까지 우리는 다음을 다루었습니다:
*   **신경망 기초**
    *   뉴런, 레이어, 활성화, 순전파 계산
*   **손실 함수 및 최적화**
    *   모델이 손실을 최소화하여 학습하는 방법
*   **그래디언트 계산**
    *   그래디언트를 효율적으로 계산하기 위한 역전파
*   **신경망 훈련**
    *   그래디언트 기반 최적화를 사용하여 파라미터 업데이트
*   **정규화**
    *   과적합을 방지하고 일반화를 개선하는 기법

***

### **페이지 619**

**요약: 지금까지 배운 것**

*   이제 여러분은 **이미지 분류**를 위한 신경망을 구축하는 전체 파이프라인을 이해하고 있으며, 다음을 할 수 있습니다:
    *   자신만의 네트워크 구조 설계하기
    *   순전파 및 역전파 구현하기
    *   경사 하강법으로 훈련하기
    *   정규화를 통해 과적합 피하기
*   **이제 여러분은 자신만의 신경망을 처음부터 구축할 수 있습니다!**
    *   여러분은 신경망을 처음부터 구현하는 데 필요한 모든 핵심 구성 요소를 배웠습니다.

**(이미지 및 수식: 신경망 훈련의 전체 파이프라인을 요약하는 그림과 수식)**

***

### **페이지 620**

**완전 연결 네트워크가 항상 좋은 선택일까?**

*   지금까지 우리는 신경망에서 **완전 연결(밀집) 레이어**만 다루었습니다.
*   하지만 - 완전 연결(FC) 레이어만 사용하는 것이 항상 최선의 접근 방식일까요?

**(이미지: 모든 레이어가 완전 연결 레이어로 구성된 신경망 다이어그램)**

***

### **페이지 621**

**문제 #1: 파라미터의 수**

*   먼저 **파라미터 수** 문제를 살펴봅시다. 이미지 작업에서 입력 크기는 일반적으로 매우 큽니다. 예를 들어, 일반적인 이미지 크기를 고려해 봅시다:
    *   **512 × 512 × 3** (RGB 이미지) = **786,432**개의 입력 값
    *   완전 연결 레이어에서, 모든 뉴런은 **모든 입력**에 연결됩니다.
*   **파라미터 수 예시 (완전 연결 레이어)**
    *   첫 번째 은닉 레이어가 **1,000개**의 뉴런을 가지고 있다고 가정해 봅시다.
    *   그러면 첫 번째 레이어의 파라미터 수는:
        *   786,432 × 1,000 = 786,432,000 (**7억 8640만 개의 파라미터**)
        *   단지 **첫 번째 레이어**에 대해서만.

**(이미지: 512x512x3 이미지가 786,432 차원 벡터로 변환되어 신경망에 입력되는 과정을 보여주는 그림)**

***

### **페이지 622**

**문제 #1: 파라미터의 수**

*   **파라미터 수 예시 (완전 연결 레이어)**
    *   첫 번째 은닉 레이어가 **1,000개**의 뉴런을 가지고 있다고 가정해 봅시다.
    *   그러면 첫 번째 레이어의 파라미터 수는:
        *   786,432 × 1,000 = 786,432,000 (**7억 8640만 개의 파라미터**) 단지 **첫 번째 레이어**에 대해서만.
*   **메모리 요구 사항**
    *   **float32 (가중치당 4바이트)**를 가정:
        *   786,432,000 × 4 = 3,145,728,000 바이트 ≈ **3.1GB**
*   이 가중치들은 훈련 설정에 따라 **RAM (CPU) 및/또는 GPU 메모리**에 저장되어야 합니다.
    *   훈련 중에는 그래디언트, 옵티마이저 상태(예: Adam은 ×2 추가)를 위한 메모리도 필요합니다.
        *   3.1GB × 4 = **12.4GB**
        *   하나의 완전 연결 레이어 ≈ **12.4GB의 메모리**

**(이미지: 이전 페이지의 그림을 반복하여 보여줌)**

***

### **페이지 623**

**문제 #1: 파라미터의 수**

*   **완전 연결 네트워크에서 파라미터는 어떻게 확장되는가?**
    *   **뉴런 수가 증가할 때 (너비 ↑)**
        *   첫 번째 레이어: **선형 증가**
        *   은닉-은닉 레이어: **이차 증가**
        *   만약 뉴런 수를 10배 늘리면:
            *   12.4GB × 100 = 1240GB ≈ **1.2TB**
    *   **더 많은 은닉 레이어를 추가할 때 (깊이 ↑)**
        *   각 새로운 은닉 레이어는 **H²개의 파라미터**를 추가합니다.
        *   파라미터는 깊이에 따라 선형적으로 증가합니다.

**(이미지: 이전 페이지의 그림을 반복하여 보여줌)**

***

### **페이지 624**

**문제 #2: FC 레이어는 공간 구조를 무시한다**

*   **완전 연결(FC, 밀집) 레이어**는 이미지를 **1D 벡터로 펼칩니다**.
    *   이미지 내의 **공간적 및 지역적 관계**를 잃어버립니다.
*   **왜 이것이 나쁜가?**
    *   **실제 이미지에서:**
        *   눈은 서로 **가까이** 있습니다.
        *   입은 코 **아래에** 있습니다.
        *   객체는 **지역적 패턴**을 가집니다.
    *   **하지만 펼친 후에는:**
        *   이미지에서 **이웃이었던 픽셀**들이 벡터에서는 **멀리 떨어지게** 됩니다.
        *   네트워크는 더 이상 두 개의 인접한 픽셀이 엣지, 곡선, 눈 등을 형성한다는 것을 알지 못합니다.

**(이미지: 고양이 이미지를 1D 벡터로 펼치는 과정을 보여주며, 이 과정에서 공간 정보가 손실됨을 물음표로 표현)**

***

### **페이지 625**

**문제 #2: FC 레이어는 공간 구조를 무시한다**

*   **완전 연결(FC, 밀집) 레이어**는 이미지를 **1D 벡터로 펼칩니다**.
    *   이미지 내의 **공간적 및 지역적 관계**를 잃어버립니다.
*   **왜 이것이 나쁜가?**
    *   **실제 이미지에서:**
        *   눈은 서로 **가까이** 있습니다.
        *   입은 코 **아래에** 있습니다.
        *   객체는 **지역적 패턴**을 가집니다.
    *   **하지만 펼친 후에는:**
        *   이미지에서 **이웃이었던 픽셀**들이 벡터에서는 **멀리 떨어지게** 됩니다.
        *   네트워크는 더 이상 두 개의 인접한 픽셀이 엣지, 곡선, 눈 등을 형성한다는 것을 알지 못합니다.

*   **FC 레이어에게, 그것들은 단지 임의의 위치에 있는 임의의 숫자일 뿐입니다.**

**(이미지: 이전 그림에 설명을 추가)**

***

### **페이지 626**

**문제 #2: FC 레이어는 공간 구조를 무시한다**

*   **핵심 메시지**
    *   완전 연결 레이어는 **이미지 구조를 이해하지 못합니다**.
        *   **이웃**에 대한 감각이 없음
        *   **지역적 패턴**이 없음
    *   이것이 완전 연결 네트워크가 이미지에 어려움을 겪는 이유입니다.
*   우리는 **공간 구조를 존중**하고 **지역적 패턴을 포착**하는 레이어가 필요합니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 627**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변(translation-invariant)이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**(이미지: 동일한 고양이 이미지가 두 개의 다른 위치에 있는 것을 보여줌)**

***

### **페이지 628**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**(이미지: 왼쪽 위의 고양이 일부에 가중치 W₁이 적용되는 것을 보여줌. 오른쪽으로 이동하면 이 가중치는 더 이상 해당 패턴에 적용되지 않음)**

***

### **페이지 629**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**(이미지: 이미지의 다른 위치에 있는 동일한 패턴을 인식하기 위해 다른 가중치(W₂, W₃, ...)가 필요함을 보여줌)**

***

### **페이지 630**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 631**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 632**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**(이미지: 왼쪽의 고양이는 "이것은 고양이다!"라고 인식하지만, 위치가 바뀐 오른쪽 고양이는 인식하지 못하는 문제를 보여줌)**

***

### **페이지 633**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 634**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**(이미지: 위치가 바뀐 고양이 이미지에 대해, 이전에 학습된 가중치(W₁)가 적용될 위치가 바뀌어 올바른 패턴을 찾지 못하고, 결과적으로 무엇인지 알 수 없게 됨을 물음표로 표현)**

***

### **페이지 635**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**모델은 그것이 고양이가 아니라고 생각합니다.**

**(이미지: 위치가 바뀐 고양이를 인식하지 못하는 문제를 보여줌)**

***

### **페이지 636**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**모델은 그것이 고양이가 아니라고 생각합니다.**

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 637**

**문제 #3: 이동 불변이 아님**

*   완전 연결 레이어는 **이동 불변이 아닙니다**.
*   만약 객체가 이미지에서 약간 움직이면, 완전 연결 네트워크는 그것을 **완전히 다른 패턴**으로 봅니다.
*   고양이가 이미지의 **왼쪽 위**에 나타난다고 상상해 봅시다.
*   나중에, **같은 고양이**가 **오른쪽 아래**에 나타납니다.
    *   **인간에게:** 여전히 고양이.
    *   **완전 연결 네트워크에게:**
        *   픽셀 위치가 변경됨 → 모든 입력 인덱스가 이동함

**모델은 그것이 고양이가 아니라고 생각합니다.**

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 638**

**문제 #3: 이동 불변이 아님**

*   **왜 이것이 문제인가**
*   **실제 객체**는 거의 한 고정된 지점에 머물지 않습니다:
    *   자동차는 거리 장면 어디에나 나타날 수 있습니다.
    *   종양은 CT 슬라이스 어디에나 나타날 수 있습니다.
    *   손으로 쓴 숫자는 MNIST에서 이동할 수 있습니다.
    *   얼굴은 셀카에서 약간 중심에서 벗어날 수 있습니다.
*   그럼에도 불구하고, 모델은 항상 그것들을 인식해야 합니다.

*   완전 연결 네트워크는 **내장된 이동 불변성**이 부족합니다.
*   입력의 작은 이동조차도 **완전히 다른 활성화**로 이어집니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 639**

**요약**

*   **파라미터 폭발**
    *   엄청난 수의 가중치 → 메모리 및 훈련이 비현실적 (예: 512×512 이미지의 한 레이어에만 >12GB)
*   **공간 구조 무시**
    *   펼치기는 지역적 픽셀 관계를 파괴함 (예: 눈-눈-코 공간 패턴 손실)
*   **이동 불변이 아님**
    *   다른 위치에 있는 동일한 객체 → 네트워크는 다른 패턴으로 취급 ("더 이상 고양이가 아님")
*   **계산적으로 비효율적**
    *   너무 많은 파라미터 → 느리고, 메모리를 많이 사용하며, 데이터 비효율적인 훈련
*   **완전 연결 네트워크는 이미지 데이터에 확장되지 않습니다.**
    그들은 공간적 패턴을 활용하지 못하고 막대한 계산 및 메모리를 요구합니다.

***

### **페이지 640**

**대신 우리에게 필요한 것**

*   지역적 연결성
*   가중치 공유
*   이동 불변성
*   효율적인 파라미터 사용
*   → **합성곱 신경망 (CNNs)**
*   **"FC 레이어는 *픽셀*을 보고, CNN은 *패턴*을 봅니다."**

**(이미지: FC 레이어의 문제점과 CNN(단일 필터 W₃를 이미지 전체에 적용)의 해결책을 시각적으로 비교)**

***

### **페이지 641**

**대신 우리에게 필요한 것**

*   지역적 연결성
*   가중치 공유
*   이동 불변성
*   효율적인 파라미터 사용
*   → **합성곱 신경망 (CNNs)**
*   **"FC 레이어는 *픽셀*을 보고, CNN은 *패턴*을 봅니다."**

**(이미지: FC 레이어가 위치가 바뀐 고양이를 인식하지 못하는 문제(X)와 CNN이 동일한 필터로 인식하는 성공 사례를 비교)**

***

### **페이지 642**

**대신 우리에게 필요한 것**

*   지역적 연결성
*   가중치 공유
*   이동 불변성
*   효율적인 파라미터 사용
*   → **합성곱 신경망 (CNNs)**
*   **"FC 레이어는 *픽셀*을 보고, CNN은 *패턴*을 봅니다."**

**(이미지: CNN이 동일한 가중치(W₃)를 이미지의 다른 위치에 적용하여 동일한 객체를 인식하는 '가중치 공유' 개념을 보여줌)**

***

### **페이지 643**

**대신 우리에게 필요한 것**

*   지역적 연결성
*   가중치 공유
*   이동 불변성
*   효율적인 파라미터 사용
*   → **합성곱 신경망 (CNNs)**

**(이미지: '이동 불변성'을 보여주는 그림. 동일한 가중치(W)가 이미지의 여러 위치로 이동하며 '고양이' 패턴을 성공적으로 탐지함)**

***

### **페이지 644**

**합성곱이란 무엇인가?**

*   **합성곱**은 다음을 의미합니다:
    *   **동일한 가중치 집합(필터)**을 사용하여 다른 공간적 위치에 걸쳐 적용하고, **지역적 창** 내의 값들을 곱하고 합산하는 것.
*   이 필터는 이미지를 **슬라이딩**하며 모든 위치에서 응답을 계산합니다.
*   **각 위치에서:**
    *   출력(x, y) = Σ (가중치 × 지역 픽셀)
*   그런 다음 창을 한 단계 이동하고 반복 → **슬라이딩 윈도우 연산**
*   모든 곳에서 동일한 가중치 사용 → **가중치 공유 + 지역 패턴 탐지**
*   합성곱 필터는 전체 이미지를 스캔하는 **패턴 탐지기**입니다.

**(이미지: 동일한 가중치(W)를 가진 필터가 이미지의 여러 위치를 스캔하는 모습을 보여줌)**

***

### **페이지 645**

**복습: 이미지를 위한 완전 연결 레이어**

*   이미지에 완전 연결(FC) 레이어를 사용하려면:
    *   이미지를 **펼칩니다(Flatten)**.
    *   동일한 크기의 가중치 벡터로 **곱합니다**.
    *   결과를 **합산**하고 편향을 더합니다.
*   이는 해당 뉴런에 대한 **하나의 출력 값(점수)**을 생성합니다.

**(이미지: 4x4 이미지를 16개 요소의 1D 벡터로 펼치고, 16개 요소의 가중치와 곱하고 합산하여 하나의 숫자(점수)를 출력하는 과정을 보여줌)**

***

### **페이지 646**

**FC 가중치는 이미지로 볼 수 있다**

*   펼치는 대신: 가중치 벡터를 이미지와 동일한 모양인 **4×4 행렬**로 재구성할 수 있습니다.
*   이것은 펼치지 않고 2D 구조를 유지한 채로, 여전히 **동일한 곱셈-합산 연산**입니다.

**(이미지: 1D 가중치 벡터를 2D 필터(가중치 행렬)로 재구성하여 2D 이미지와 직접 연산하는 과정을 보여줌)**

***

### **페이지 647**

**전체 이미지 합성곱으로서의 FC**

*   가중치가 입력 이미지와 **동일한 공간적 크기**를 가질 때, 완전 연결 연산은 전체 이미지를 덮는 **합성곱 필터**를 적용하는 것과 동일합니다.

**(이미지: FC 연산이 전체 이미지 크기의 필터를 사용한 단일 합성곱(*) 연산과 동일함을 보여줌)**

***

### **페이지 648**

**다중 출력: FC vs 합성곱**

*   **완전 연결(FC) 레이어의 경우:**
    *   **여러 출력 뉴런**을 원한다면, 간단히 **여러 가중치 벡터**를 사용합니다 → 출력 뉴런당 하나의 가중치 벡터.

**(이미지: 여러 개의 가중치 벡터를 사용하여 여러 개의 출력 점수를 생성하는 FC 레이어와, 단일 필터로 단일 점수를 생성하는 합성곱을 비교)**

***

### **페이지 649**

**다중 출력: FC vs 합성곱**

*   **완전 연결(FC) 레이어의 경우:**
    *   **여러 출력 뉴런**을 원한다면, 간단히 **여러 가중치 벡터**를 사용합니다 → 출력 뉴런당 하나의 가중치 벡터.

**(이미지: 여러 개의 필터를 사용하여 여러 개의 출력 점수를 생성하는 합성곱을 보여줌. 이는 FC에서 여러 가중치 벡터를 사용하는 것과 유사함)**

***

### **페이지 650**

**전체 이미지 합성곱이 실용적이지 않은 이유**

*   우리는 이미지와 **동일한 크기**의 필터를 사용하는 것을 보았습니다.
    *   **엄청난 계산 및 메모리**를 요구합니다.
    *   객체가 **정확히 동일한 위치**에 있을 때만 작동합니다.
    *   **이동-변이**: 고양이를 움직이면 → 모델 실패.

**같은 고양이, 다른 위치 ⇒ "고양이가 아님."**

**(이미지: 전체 이미지 크기의 필터가 이미지와 합성곱 연산을 수행하여 하나의 점수를 출력하는 과정을 보여줌)**

***

### **페이지 651**

**전체 이미지 합성곱이 실용적이지 않은 이유**

*   우리는 이미지와 **동일한 크기**의 필터를 사용하는 것을 보았습니다.
    *   **엄청난 계산 및 메모리**를 요구합니다.
    *   객체가 **정확히 동일한 위치**에 있을 때만 작동합니다.
    *   **이동-변이**: 고양이를 움직이면 → 모델 실패.

**같은 고양이, 다른 위치 ⇒ "고양이가 아님."**

**(이미지: 필터가 이미지의 특정 부분(고양이 얼굴)과 일치할 때만 높은 점수를 출력함을 보여줌)**

***

### **페이지 652**

**전체 이미지 합성곱이 실용적이지 않은 이유**

*   우리는 이미지와 **동일한 크기**의 필터를 사용하는 것을 보았습니다.
    *   **엄청난 계산 및 메모리**를 요구합니다.
    *   객체가 **정확히 동일한 위치**에 있을 때만 작동합니다.
    *   **이동-변이**: 고양이를 움직이면 → 모델 실패.

**같은 고양이, 다른 위치 ⇒ "고양이가 아님."**

**(이미지: 고양이 이미지가 왼쪽으로 이동한 모습을 보여줌)**

***

### **페이지 653**

**전체 이미지 합성곱이 실용적이지 않은 이유**

*   우리는 이미지와 **동일한 크기**의 필터를 사용하는 것을 보았습니다.
    *   **엄청난 계산 및 메모리**를 요구합니다.
    *   객체가 **정확히 동일한 위치**에 있을 때만 작동합니다.
    *   **이동-변이**: 고양이를 움직이면 → 모델 실패.

**같은 고양이, 다른 위치 ⇒ "고양이가 아님."**

**(이미지: 이동된 고양이 이미지에 필터를 적용하면 패턴이 일치하지 않아 "고양이가 아님"이라는 낮은 점수를 출력함을 보여줌)**

***

### **페이지 654**

**전체 이미지 합성곱이 실용적이지 않은 이유**

*   우리는 이미지와 **동일한 크기**의 필터를 사용하는 것을 보았습니다.
    *   **엄청난 계산 및 메모리**를 요구합니다.
    *   객체가 **정확히 동일한 위치**에 있을 때만 작동합니다.
    *   **이동-변이**: 고양이를 움직이면 → 모델 실패.

**같은 고양이, 다른 위치 ⇒ "고양이가 아님."**

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 655**

**대신 작은 필터를 사용하면 어떨까?**

*   이미지만큼 큰 필터를 사용하는 대신…
    *   **작은 필터**(예: 5×5, 11×11, 25×25)를 사용해 봅시다.
    *   훨씬 적은 파라미터.

**(이미지: 큰 이미지에 작은 5x5 필터를 적용하는 합성곱 연산을 보여줌)**

***

### **페이지 656**

**대신 작은 필터를 사용하면 어떨까?**

*   이미지만큼 큰 필터를 사용하는 대신…
    *   **작은 필터**(예: 5×5, 11×11, 25×25)를 사용해 봅시다.
    *   훨씬 적은 파라미터.
    *   **지역적 패턴**(고양이 얼굴)을 탐지할 수 있습니다.

**(이미지: 작은 필터가 이미지의 고양이 얼굴 부분과 일치할 때 높은 점수("고양이! (얼굴)")를 출력하는 것을 보여줌)**

***

### **페이지 657**

**대신 작은 필터를 사용하면 어떨까?**

*   이미지만큼 큰 필터를 사용하는 대신…
    *   **작은 필터**를 사용해 봅시다.
    *   훨씬 적은 파라미터.
    *   **지역적 패턴**(고양이 얼굴)을 탐지할 수 있습니다.
*   그리고 이 필터를 이미지 전체에 **슬라이드**할 수 있습니다.
    *   어떤 위치에서든 **동일한 패턴**을 탐지합니다.

**작은 커널 ⇒ 이미지 스캔**

**(이미지: 필터가 이미지 위를 이동(Translation)하는 모습을 보여줌)**

***

### **페이지 658**

**대신 작은 필터를 사용하면 어떨까?**

*   이미지만큼 큰 필터를 사용하는 대신…
    *   **작은 필터**를 사용해 봅시다.
    *   훨씬 적은 파라미터.
    *   **지역적 패턴**(고양이 얼굴)을 탐지할 수 있습니다.
*   그리고 이 필터를 이미지 전체에 **슬라이드**할 수 있습니다.
    *   어떤 위치에서든 **동일한 패턴**을 탐지합니다.

**작은 커널 ⇒ 이미지 스캔**

**(이미지: 이동된 위치에서도 동일한 필터(가중치)가 고양이 얼굴 패턴을 탐지하여 "역시, 고양이! (얼굴)"라는 높은 점수를 출력함을 보여줌)**

***

### **페이지 659**

**대신 작은 필터를 사용하면 어떨까?**

*   이미지만큼 큰 필터를 사용하는 대신…
    *   **작은 필터**를 사용해 봅시다.
    *   훨씬 적은 파라미터.
    *   **지역적 패턴**(고양이 얼굴)을 탐지할 수 있습니다.
*   그리고 이 필터를 이미지 전체에 **슬라이드**할 수 있습니다.
    *   어떤 위치에서든 **동일한 패턴**을 탐지합니다.

**작은 커널 ⇒ 이미지 스캔**

**(이미지: 필터가 이미지 위를 계속 슬라이딩하는 모습을 보여줌)**

***

### **페이지 660**

**출력이 단지 하나의 숫자인가?**

*   작은 필터(커널)를 이미지에 적용할 때, 출력은 **단일 점수가 아닙니다**.
*   각 필터는 공간적 위치당 하나의 값을 생성하지만, 우리는 필터를 **이미지 전체에 슬라이드**하며 매번 하나의 값을 계산합니다.

**(이미지: 13x13 이미지에 5x5 필터를 적용한 결과가 하나의 숫자인지 묻는 그림)**

***

### **페이지 661**

**결과: 2D 출력 특징 맵**

*   하나의 숫자 대신, 우리는 **2D 배열(특징 맵)**을 얻습니다.
*   **예시**
    *   입력 이미지 크기: **13 × 13**
    *   필터 크기: **5 × 5**
    *   스트라이드: **1**
    *   패딩 없음
*   출력 크기 = 13 - 5 + 1 = 9
    → **출력 특징 맵: 9 × 9**

**(이미지: 13x13 이미지에 5x5 필터를 적용하여 9x9 크기의 출력 특징 맵(2D 배열)을 생성하는 과정을 보여줌)**

***

### **페이지 662**

**결과: 2D 출력 특징 맵**

**(이미지: 이전 그림에서 필터가 첫 번째 위치에서 연산을 수행하여 출력 특징 맵의 첫 번째 값을 생성하는 과정을 보여줌)**

***

### **페이지 663**

**결과: 2D 출력 특징 맵**

**(이미지: 필터가 이미지의 여러 위치를 슬라이딩하며 출력 특징 맵의 여러 값을 채워나가는 과정을 보여줌)**

***

### **페이지 664**

**합성곱 쌓기**

*   작은 필터를 사용한 합성곱은 단일 점수를 생성하지 않고, 대신 **특징 맵**을 얻습니다.
*   이것은 강력합니다 — 왜냐하면 다음을 의미하기 때문입니다:
    *   이 특징 맵에 **또 다른 합성곱**을 수행할 수 있습니다.
    *   **더 높은 수준**의 그리고 더 **추상적인 특징**을 추출할 수 있습니다.
    *   이 과정을 **층별로 반복**할 수 있습니다.

**(이미지: 9x9 특징 맵에 다시 5x5 필터를 적용하여 5x5 크기의 두 번째 출력 특징 맵을 생성하는 과정을 보여줌)**

***

### **페이지 665**

**합성곱 쌓기**

**(이미지: 이전 그림에서 두 번째 합성곱 연산이 수행되는 과정을 보여줌)**

***

### **페이지 666**

**결국, 하나의 점수를 얻을 수 있다**

*   합성곱 레이어를 쌓으면서, 특징 맵의 공간적 크기는 점차 줄어듭니다.
    *   (예: 13×13 → 9×9 → … → 5×5).
*   특징 맵이 충분히 작아지면(예: 5×5), 전체 특징 맵을 덮는 필터를 적용할 수 있습니다.
    *   이는 **단일 출력 값** — **점수** — 를 생성합니다.
*   이 점수는 클래스나 작업에 대한 최종 신뢰도 또는 예측을 나타냅니다.

**(이미지: 5x5 특징 맵에 5x5 필터를 적용하여 최종적으로 하나의 점수를 얻는 과정을 보여줌)**

***

### **페이지 667**

**결국, 하나의 점수를 얻을 수 있다**

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 668**

**결국, 하나의 점수를 얻을 수 있다**

**(이미지: 최종 점수가 "고양이? 개?"와 같은 분류 결과를 나타냄을 보여줌)**

***

### **페이지 669**

**결국, 하나의 점수를 얻을 수 있다**

*   **초기 레이어**: 공간적 패턴 포착
*   **최종 합성곱 (또는 완전 연결 레이어)** → 1개의 출력 점수

**(이미지: 최종 5x5 특징 맵을 펼치고(Flatten) 곱하고 합산하는 완전 연결 레이어를 통해 하나의 점수를 얻는 과정을 보여줌)**

***

### **페이지 670**

**결국, 하나의 점수를 얻을 수 있다**

**(이미지: 이전 그림의 완전 연결 레이어 과정을 1D 벡터 연산으로 시각화하여 보여줌)**

***

### **페이지 671**

**요약**

*   지금까지, 우리는 완전한 파이프라인을 구축했습니다:
    *   **입력 이미지**로 시작합니다.
    *   **5×5 합성곱 필터**를 층별로 쌓아 적용합니다.
    *   특징 맵의 공간적 크기는 **점차 줄어듭니다**.
    *   끝에서, **최종 합성곱**(또는 완전 연결 레이어)을 적용합니다.
    *   **하나의 출력 점수**를 생성합니다.
*   다시 말해, 합성곱 레이어를 쌓음으로써, 우리는 이미지로부터 점점 더 추상적인 특징을 추출하고, **초기에 펼치지 않고도** 궁극적으로 **단일 예측 값**에 도달할 수 있습니다.

**(이미지: 입력 이미지 → 5x5 합성곱 → 특징 맵 1 → 5x5 합성곱 → 특징 맵 2 → 최종 합성곱(FC) → 점수로 이어지는 전체 CNN 파이프라인을 요약)**

***

### **페이지 672**

**잠깐 — 뭔가 빠지지 않았나요?**

*   지금까지 우리는 여러 합성곱 레이어를 쌓았습니다. 하지만 무언가 이상한 점이 있습니다:
    *   합성곱은 단지 **선형 가중 합**입니다.
    *   선형 연산만 쌓는 것은 여전히 **선형**입니다.
    *   따라서, 수학적으로, **여러 선형 합성곱은 하나의 큰 등가 합성곱으로 축소**됩니다.
*   그래서 만약 우리가 **비선형성 없이** conv 레이어만 쌓는다면,
    *   **실질적인 이득이 없습니다** — 그것은 모두 하나의 합성곱으로 대체될 수 있습니다.

**(이미지: 이전 파이프라인의 모든 단계가 "선형"임을 강조)**

***

### **페이지 673**

**잠깐 — 뭔가 빠지지 않았나요?**

*   지금까지 우리는 여러 합성곱 레이어를 쌓았습니다. 하지만 무언가 이상한 점이 있습니다:
    *   합성곱은 단지 **선형 가중 합**입니다.
    *   선형 연산만 쌓는 것은 여전히 **선형**입니다.
    *   따라서, 수학적으로, **여러 선형 합성곱은 하나의 큰 등가 합성곱으로 축소**됩니다.
*   그래서 만약 우리가 **비선형성 없이** conv 레이어만 쌓는다면,
    *   **실질적인 이득이 없습니다** — 그것은 모두 하나의 합성곱으로 대체될 수 있습니다.

**(이미지: 여러 개의 작은 선형 합성곱을 쌓는 것이 하나의 큰 선형 합성곱을 사용하는 것과 동일하며, 파라미터 수 측면에서 이점이 없음을 보여줌)**

***

### **페이지 674**

**비선형성은 필수적이다**

*   우리는 이미 신경망이 복잡한 패턴을 모델링하기 위해 **비선형 활성화 함수**(예: ReLU)가 필요하다는 것을 배웠습니다.
*   완전 연결 레이어가 활성화를 사용하는 것처럼, **합성곱 레이어도 똑같이 해야 합니다**.
*   각 합성곱 이후, 우리는 모든 공간적 위치에 **비선형성**을 적용합니다.

**(이미지: 선형 합성곱의 결과(출력 특징 맵)가 활성화 함수(비선형)를 통과하여 활성화된 출력 특징 맵을 생성하는 과정을 보여줌)**

***

### **페이지 675**

**참고: 합성곱 레이어의 편향**

*   다이어그램에서 종종 생략되는 구성 요소가 하나 더 있습니다:
*   완전 연결 레이어와 마찬가지로, 각 합성곱에도 **편향 항**이 있습니다.
    *   먼저, 합성곱 필터를 사용하여 **선형 가중 합**을 수행합니다.
    *   그런 다음, **편향을 더합니다**.
*   **합성곱 레이어의 편향:**
    *   **필터(채널)당 하나의 값**입니다.
    *   출력 특징 맵의 **모든 공간적 위치**에 더해집니다.
    *   FC 레이어와 유사하게 **상수 오프셋**처럼 작동합니다.

**(이미지: 합성곱 연산 후, 출력 특징 맵의 모든 픽셀에 동일한 편향 값(스칼라)이 더해지는 과정을 보여줌)**

***

### **페이지 676**

**참고: 합성곱 레이어의 편향**

*   다이어그램에서 종종 생략되는 구성 요소가 하나 더 있습니다:
*   완전 연결 레이어와 마찬가지로, 각 합성곱에도 **편향 항**이 있습니다.
    *   먼저, 합성곱 필터를 사용하여 **선형 가중 합**을 수행합니다.
    *   그런 다음, **편향을 더합니다**.
*   **합성곱 레이어의 편향:**
    *   **필터(채널)당 하나의 값**입니다.
    *   출력 특징 맵의 **모든 공간적 위치**에 더해집니다.
    *   FC 레이어와 유사하게 **상수 오프셋**처럼 작동합니다.

**우리는 다이어그램에서 편향을 생략할 것입니다.**

**(이미지: 전체 합성곱 레이어 연산을 "선형 가중 합 + 편향 → 활성화"로 요약)**

***

### **페이지 677**

**비선형성은 필수적이다**

*   우리는 이미 신경망이 복잡한 패턴을 모델링하기 위해 **비선형 활성화 함수**(예: ReLU)가 필요하다는 것을 배웠습니다.
*   완전 연결 레이어가 활성화를 사용하는 것처럼, **합성곱 레이어도 똑같이 해야 합니다**.
*   각 합성곱 이후, 우리는 모든 공간적 위치에 **비선형성**을 적용합니다.

**(이미지: 각 합성곱(Conv) 단계 이후에 비선형 활성화(ReLU)가 적용되는 전체 CNN 파이프라인을 보여줌. 마지막 레이어는 선형임)**

***

### **페이지 678**

**단 하나의 점수만 필요한가?**

*   하지만 잠깐 — 단지 **하나의 점수**만 있으면 충분할까요?
*   실제로는, 우리는 "고양이" 또는 "고양이 아님"뿐만 아니라 **여러 클래스**를 구별하고 싶습니다.
*   예를 들어, 우리 모델은 **고양이, 개, 배, 자동차** 등을 구별해야 합니다.

**(이미지: 이전 파이프라인의 최종 출력이 단 하나의 점수라는 점에 의문을 제기)**

***

### **페이지 679**

**다중 점수 → 다중 필터**

*   **다중 점수**를 얻기 위해, 우리는 간단히 **다중 합성곱 필터**를 사용할 수 있습니다.
*   각 필터는 이미지에서 **다른 패턴이나 특징**을 탐지하도록 학습합니다.
*   각 필터의 출력은 **별도의 점수** — 각 클래스에 대한 하나 — 에 해당합니다.
*   따라서, 여러 필터를 쌓음으로써, 우리는 **다중 클래스 분류**를 수행할 수 있습니다.

**(이미지: 최종 레이어에서 여러 개의 필터를 사용하여 각 클래스(고양이, 개, 배)에 대한 별도의 점수를 출력하는 과정을 보여줌)**

***

### **페이지 680**

**다중 점수 → 다중 필터**

*   **다중 점수**를 얻기 위해, 우리는 간단히 **다중 합성곱 필터**를 사용할 수 있습니다.
*   각 필터는 이미지에서 **다른 패턴이나 특징**을 탐지하도록 학습합니다.
*   각 필터의 출력은 **별도의 점수** — 각 클래스에 대한 하나 — 에 해당합니다.
*   따라서, 여러 필터를 쌓음으로써, 우리는 **다중 클래스 분류**를 수행할 수 있습니다.

**(이미지: 이전 그림에서 최종 레이어의 필터들이 쌓여(Stack filters) 있음을 강조)**

***

### **페이지 681**

**다중 점수 → 다중 필터**

*   여러 필터를 가질 수 있는 것은 **최종 레이어**뿐만이 아닙니다.

**(이미지: 이전 레이어들도 단일 필터만 사용하는지에 대한 의문을 제기)**

***

### **페이지 682**

**다중 점수 → 다중 필터**

*   여러 필터를 가질 수 있는 것은 **최종 레이어**뿐만이 아닙니다.
*   특징 맵을 추출할 때, 각 합성곱 필터는 이미지의 **다른 측면**을 포착하도록 학습합니다.
    *   엣지, 질감, 색상, 모양 등.

**(이미지: 수직/수평 엣지 필터, 라플라시안 필터, 평균 필터, 가우시안 필터 등 다양한 종류의 필터를 보여주며, 초기 레이어에서도 여러 필터를 사용할 수 있음을 암시)**

***

### **페이지 683**

**끝에서만이 아닌 — 모든 곳에 다중 필터**

*   사실, 우리는 네트워크 **전반에 걸쳐 다중 필터**를 사용할 수 있습니다 (그리고 사용해야 합니다).
*   특징 맵을 추출할 때, 각 합성곱 필터는 이미지의 **다른 측면**을 포착하도록 학습합니다.
    *   엣지, 질감, 색상, 모양 등.
*   고전적인 이미지 필터링에서 x 및 y 방향에 대해 다른 엣지 필터를 사용하는 것처럼, CNN은 **다른 유형의 특징을 탐지하기 위해 다른 필터**를 사용합니다.
*   이러한 다양한 필터를 쌓음으로써, 우리는 모델이 이미지에서 **무엇이 어디에** 있는지 이해하는 데 도움이 되는 풍부한 특징 맵을 구축합니다.

**(이미지: 각 합성곱 레이어에서 여러 개의 필터(3D 블록)를 사용하여 여러 채널의 출력 특징 맵을 생성하는 과정을 보여줌)**

***

### **페이지 684**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.

**(이미지: R, G, B 채널 이미지가 합쳐져 컬러 이미지를 만드는 과정과, 디지털 컬러 이미지가 3개의 색상 행렬(red, green, blue)로 구성됨을 보여주는 그림)**

***

### **페이지 685**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.

**(이미지: 컬러 고양이 이미지가 Red, Green, Blue 3개의 채널로 분리되는 것을 보여줌)**

***

### **페이지 686**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.
*   만약 입력이 3개의 채널을 가지고 있다면, 필터는 5×5일 수 있을까요? **맞나요?**

**(이미지: 3채널 입력 이미지에 2D(5x5) 필터를 적용하는 것이 가능한지 묻는 그림)**

***

### **페이지 687**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.
*   만약 입력이 3개의 채널을 가지고 있다면, 필터는 5×5일 수 있을까요?
*   **아니오**, 합성곱 필터도 **동일한 수의 채널**을 가져야 합니다.
*   따라서 각 필터는 단지 2D 패치가 아니라 — **3D 블록**(너비 × 높이 × 채널)입니다.

**(이미지: 3채널 입력에 적용되는 필터도 3채널(3x5x5)을 가져야 함을 보여주는 그림)**

***

### **페이지 688**

**필터도 채널을 가진다**

*   **아니오**, 합성곱 필터도 **동일한 수의 채널**을 가져야 합니다.
*   따라서 각 필터는 단지 2D 패치가 아니라 — **3D 블록**(너비 × 높이 × 채널)입니다.
*   필터의 각 "슬라이스"는 해당 이미지 채널과 상호작용하며, 그 결과들은 **하나의 특징 맵**을 생성하기 위해 합산됩니다.

**(이미지: 3채널 필터가 3채널 이미지와 합성곱 연산을 수행한 결과가 무엇인지 묻는 그림)**

***

### **페이지 689**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.
*   만약 입력이 3개의 채널을 가지고 있다면, 필터는 5×5일 수 있을까요?
*   **아니오**, 합성곱 필터도 **동일한 수의 채널**을 가져야 합니다.
*   따라서 각 필터는 단지 2D 패치가 아니라 — **3D 블록**(너비 × 높이 × 채널)입니다.

**(이미지: 3x5x5 필터가 3x13x13 이미지와 연산하여 1x9x9 크기의 단일 채널 출력 특징 맵을 생성함을 보여줌)**

***

### **페이지 690**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.
*   만약 입력이 3개의 채널을 가지고 있다면, 필터는 5×5일 수 있을까요?
*   **아니오**, 합성곱 필터도 **동일한 수의 채널**을 가져야 합니다.
*   따라서 각 필터는 단지 2D 패치가 아니라 — **3D 블록**(너비 × 높이 × 채널)입니다.

**(이미지: 입력 이미지와 필터를 모두 3D 블록으로 시각화하여 합성곱 연산을 보여줌)**

***

### **페이지 691**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.
*   만약 입력이 3개의 채널을 가지고 있다면, 필터는 5×5일 수 있을까요?
*   **아니오**, 합성곱 필터도 **동일한 수의 채널**을 가져야 합니다.
*   각 필터는 단지 2D 패치가 아니라 — **3D 블록**입니다.
*   우리는 또한 **여러 개의 이러한 필터**를 가질 수 있으며,
    이러한 각 필터는 **자체적인 출력 채널**을 생성합니다.

**(이미지: 4개의 3x5x5 필터를 사용하여 4채널의 출력 특징 맵(4x9x9)을 생성하는 과정을 보여줌)**

***

### **페이지 692**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.
*   만약 입력이 3개의 채널을 가지고 있다면, 필터는 5×5일 수 있을까요?
*   **아니오**, 합성곱 필터도 **동일한 수의 채널**을 가져야 합니다.
*   각 필터는 단지 2D 패치가 아니라 — **3D 블록**입니다.
*   우리는 또한 **여러 개의 이러한 필터**를 가질 수 있으며,
    이러한 각 필터는 **자체적인 출력 채널**을 생성합니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 693**

**필터도 채널을 가진다**

*   기억하세요 — 이미지는 보통 **3개의 채널**(R, G, B)을 가집니다.
*   만약 입력이 3개의 채널을 가지고 있다면, 필터는 5×5일 수 있을까요?
*   **아니오**, 합성곱 필터도 **동일한 수의 채널**을 가져야 합니다.
*   각 필터는 단지 2D 패치가 아니라 — **3D 블록**입니다.
*   우리는 또한 **여러 개의 이러한 필터**를 가질 수 있으며,
    이러한 각 필터는 **자체적인 출력 채널**을 생성합니다.

**(이미지: 4채널의 출력 특징 맵이 하나의 3D 블록으로 합쳐지는 것을 보여줌)**

***

### **페이지 694**

**여러 이미지가 들어올 때 — 배치 차원**

*   실제로, 우리는 한 번에 단 하나의 이미지만 처리하는 경우는 거의 없습니다.
*   효율성과 안정성을 위해 네트워크에 이미지 **배치**를 공급합니다.
*   **배치 차원(B)**은 동일하게 유지됩니다 — 우리는 모든 이미지를 병렬로 처리합니다.
*   **채널의 수**는 우리가 사용하는 필터(C_out)의 수에 따라 변경됩니다.

**(이미지: N개의 이미지로 구성된 입력 배치(N x 3 x 13x13)가 4개의 필터와 연산하여 N개의 출력 특징 맵 배치(N x 4 x 9x9)를 생성하는 과정을 보여줌)**

***

### **페이지 695**

**여러 이미지가 들어올 때 — 배치 차원**

*   실제로, 우리는 한 번에 단 하나의 이미지만 처리하는 경우는 거의 없습니다.
*   효율성과 안정성을 위해 네트워크에 이미지 **배치**를 공급합니다.
*   **배치 차원(B)**은 동일하게 유지됩니다 — 우리는 모든 이미지를 병렬로 처리합니다.
*   **채널의 수**는 우리가 사용하는 필터(C_out)의 수에 따라 변경됩니다.

**(이미지: 단일 3x32x32 이미지가 6개의 3x5x5 필터를 가진 합성곱 레이어를 통과하여 6개의 1x28x28 활성화 맵을 생성하는 과정을 보여주는 다이어그램)**

***

### **페이지 696**

**여러 이미지가 들어올 때 — 배치 차원**

*   실제로, 우리는 한 번에 단 하나의 이미지만 처리하는 경우는 거의 없습니다.
*   효율성과 안정성을 위해 네트워크에 이미지 **배치**를 공급합니다.
*   **배치 차원(B)**은 동일하게 유지됩니다 — 우리는 모든 이미지를 병렬로 처리합니다.
*   **채널의 수**는 우리가 사용하는 필터(C_out)의 수에 따라 변경됩니다.

**(이미지: N개의 이미지 배치(N x C_in x H x W)가 합성곱 레이어를 통과하여 N개의 출력 배치(N x C_out x H' x W')를 생성하는 과정을 보여주는 다이어그램)**

***

### **페이지 697**

**개요 — 합성곱에서 CNN까지**

*   **합성곱 연산(Conv)**과 **비선형 활성화(ReLU)**를 결합할 때, 우리는 그 단위를 **합성곱 레이어**라고 부릅니다.
*   이러한 합성곱 레이어들의 스택(종종 풀링 및 정규화와 혼합됨)이 우리가 **합성곱 신경망(CNN)**이라고 부르는 것을 형성합니다.

**(이미지: 입력 이미지 → 합성곱 레이어 1 → 특징 맵 1 → 합성곱 레이어 2 → 특징 맵 2 → 최종 합성곱/FC 레이어 → 클래스 점수로 이어지는 전체 CNN 아키텍처를 보여주는 다이어그램)**

***

### **페이지 698**

**지금 필터 가중치는 무엇인가?**

*   이 시점에서, 당신은 궁금할 것입니다 —
    **이 필터 가중치들의 실제 값은 무엇일까?**

**(이미지: 이전 다이어그램의 첫 번째 필터에 물음표를 표시하여 가중치 값에 대한 질문을 제기)**

***

### **페이지 699**

**지금 필터 가중치는 무엇인가?**

*   엣지 탐지기, 스무딩 필터, 또는 샤프닝 커널과 같이 우리가 이전에 사용했던 고전적인 필터들과 달리, 합성곱 신경망의 필터들은 **미리 정의되지 않습니다**.
*   전통적인 이미지 처리에서는 필터 가중치가 **수동으로 설계**되었습니다 (예: 소벨, 가우시안, 라플라시안).

**(이미지: 수동으로 설계된 수직/수평 엣지 필터의 예시를 보여줌)**

***

### **페이지 700**

**지금 필터 가중치는 무엇인가?**

*   하지만 CNN에서는, 이 가중치들은 **무작위 값**으로 시작하여 훈련을 통해 데이터로부터 **자동으로 학습**됩니다.

**(이미지: 훈련 데이터셋(고양이, 개 이미지)이 CNN 학습에 사용됨을 보여줌)**

***

### **페이지 701**

**CNN은 신경망과 똑같이 학습한다**

*   **합성곱 신경망(CNN)**은, 그 핵심에 있어서,
    여전히 **신경망**입니다 — 단지 완전 연결 레이어 대신 합성곱 레이어를 가질 뿐입니다.
*   CNN은 우리가 이전에 배운 **동일한 순전파 및 역전파 과정**을 사용하여 훈련될 수 있습니다.

**(이미지: CNN의 순전파(Forward) 과정을 보여주는 다이어그램)**

***

### **페이지 702**

**CNN은 신경망과 똑같이 학습한다**

*   하지만 CNN에서는, 이 가중치들은 **무작위 값**으로 시작하여 훈련을 통해 데이터로부터 **자동으로 학습**됩니다.
*   CNN은 우리가 이전에 배운 **동일한 순전파 및 역전파 과정**을 사용하여 훈련될 수 있습니다.

**(이미지: CNN의 순전파 및 역전파(Backward) 과정을 보여주는 다이어그램과 관련 손실 및 업데이트 수식)**

***

### **페이지 703**

**합성곱 필터는 실제로 무엇을 배우는가?**

*   CNN의 각 합성곱 필터는 미리 정의된 공식이 아닌 데이터로부터 **특정 패턴**을 학습합니다. 우리는 이 학습된 필터들을 **시각화**하여 그들이 무엇에 집중하는지 볼 수 있습니다.
    *   일부 필터는 **방향성 엣지 탐지기**(예: 수평, 수직, 대각선)가 됩니다.
    *   다른 필터들은 **색상-대비 패턴**(예: 빨강-초록 또는 파랑-노랑 대비)을 포착합니다.
    *   일부는 **질감 패턴** — 줄무늬, 코너, 또는 간단한 그래디언트 — 에 반응합니다.

**(이미지: AlexNet의 첫 번째 합성곱 레이어에서 학습된 64개의 3x11x11 필터들을 시각화한 결과)**

***

### **페이지 704**

**복습: 선형 분류기가 "보는" 것**

*   **시각화**
    *   **비행기**: 전체적으로 파란색 영역 → 모델이 **하늘 배경 패턴**을 학습함
    *   **자동차**: 중앙의 둥근 보라색 모양 → **자동차 몸체 모양**에 집중함
    *   **개구리**: 녹색과 노란색의 혼합 → 개구리의 **질감과 색상 패턴**을 포착함
    *   **배**: 수평의 파란색 영역 → **수면 배경 패턴**을 학습함

*   **흐릿한 가중치**
    *   모델은 데이터로부터 **평균적인 패턴**만 학습하기 때문에, 대부분의 **미세한 세부 사항은 사라지고**, 흐릿한 **원형(prototypical) 패턴**만 남습니다.

**(이미지: CIFAR-10 데이터셋의 각 클래스에 대해 학습된 선형 분류기의 가중치를 시각화한 결과)**

***

### **페이지 705**

**합성곱 필터는 실제로 무엇을 배우는가?**

*   네트워크의 더 깊은 곳으로 갈수록, 필터는 **더 복잡하고 추상적인 패턴**을 학습합니다.
    *   **더 깊은 레이어**: 눈, 얼굴, 글자, 또는 객체 모양과 같은 **고수준 구조**
*   하지만, 더 깊은 레이어가 무엇을 배우는지 시각화하는 것은 **간단하지 않습니다**.
    *   필터의 직접적인 검사는 의미가 없어집니다 — 더 이상 간단한 픽셀 패턴에 해당하지 않습니다.
    *   대신, 우리는 특정 뉴런이나 특징 맵을 최대로 활성화하는 입력 이미지를 찾는 **활성화 최대화**와 같은 방법을 사용합니다.
    *   이러한 시각화 결과는 깊은 레이어가 낮은 수준의 엣지가 아닌 **의미적으로 의미 있는 구조**를 포착한다는 것을 보여줍니다.

**(이미지: ImageNet으로 훈련된 모델의 6번째 합성곱 레이어에서 학습된 패턴을 활성화 최대화 기법으로 시각화한 결과. 눈, 글자, 복잡한 질감 등이 보임)**

***

### **페이지 706**

**합성곱 필터는 실제로 무엇을 배우는가?**

*   네트워크의 더 깊은 곳으로 갈수록, 필터는 **더 복잡하고 추상적인 패턴**을 학습합니다.
    *   **초기 레이어**: 간단한 엣지와 색상 대비
    *   **중간 레이어**: 질감과 객체의 일부
    *   **더 깊은 레이어**: 눈, 얼굴, 글자, 또는 객체 모양과 같은 **고수준 구조**

**(이미지: CNN의 계층적 특징 학습을 보여주는 그림. 초기 레이어(Low-Level)에서는 간단한 엣지, 중간 레이어(Mid-Level)에서는 눈/코와 같은 부분, 최종 레이어(High-Level)에서는 얼굴 전체와 같은 복잡한 패턴을 학습함)**

***

### **페이지 707**

**왜 합성곱 후에 특징 맵이 줄어드는가?**

*   각 합성곱 레이어 이후, 특징 맵의 **공간적 크기**는 종종 **더 작아집니다**.
    *   이는 합성곱이 커널이 입력 이미지 내부에 완전히 맞는 영역에만 적용되기 때문에 발생합니다.
    *   패딩을 사용하지 않으면, 출력은 모든 레이어에서 줄어듭니다.
*   이것은 처음에는 **이상하게** 보입니다 — 만약 특징 맵이 계속 줄어든다면, 맵이 사라지기 전에 **몇 개의 레이어**(예: 2개 또는 3개)만 쌓을 수 있을 것입니다!

**(이미지: 합성곱 레이어를 거치면서 특징 맵의 크기가 점차 줄어드는 CNN 파이프라인)**

***

### **페이지 708**

**왜 합성곱 후에 특징 맵이 줄어드는가?**

*   각 합성곱 레이어 이후, 특징 맵의 **공간적 크기**는 종종 **더 작아집니다**.
    *   이는 합성곱이 커널이 입력 이미지 내부에 완전히 맞는 영역에만 적용되기 때문에 발생합니다.
    *   패딩을 사용하지 않으면, 출력은 모든 레이어에서 줄어듭니다.
*   이것은 처음에는 **이상하게** 보입니다 — 만약 특징 맵이 계속 줄어든다면, **몇 개의 레이어**(예: 2개 또는 3개)만 쌓을 수 있을 것입니다!

**(이미지 및 수식: 7x7 입력에 3x3 필터를 적용하면 5x5 출력이 나오는 예시를 통해 특징 맵이 줄어드는 문제를 설명)**

***

### **페이지 709**

**왜 합성곱 후에 특징 맵이 줄어드는가?**

*   따라서, 실제로는 **패딩**(크기 보존)과 **스트라이드 제어**(다운샘플링 관리)와 같은 기술을 사용하여, 더 깊은 아키텍처를 구축할 수 있도록 합니다.

**(이미지 및 수식: 패딩을 추가하여 출력 크기를 조절하는 방법을 설명. 해결책: 필터를 슬라이딩하기 전에 입력 주위에 패딩을 추가)**

***

### **페이지 710**

**하나의 출력 픽셀에 몇 개의 입력 픽셀이 기여하는가?**

*   각 **출력 픽셀**에 대해, 네트워크는 입력의 **지역적 영역**으로부터 정보를 결합하여 그 값을 계산합니다.

**(이미지: 출력 픽셀 하나에 해당하는 입력 영역이 어디인지 묻는 그림)**

***

### **페이지 711**

**하나의 출력 픽셀에 몇 개의 입력 픽셀이 기여하는가?**

*   각 **출력 픽셀**에 대해, 네트워크는 입력의 **지역적 영역**으로부터 정보를 결합하여 그 값을 계산합니다.
*   만약 합성곱 커널의 크기가 **k × k**라면,
    **k × k**개의 입력 픽셀이 **하나의 출력 픽셀**을 생성하는 데 관여합니다.
*   이 영역을 해당 출력 픽셀의 **수용장(receptive field)**이라고 합니다.

**(이미지: 출력 픽셀 하나에 해당하는 입력의 k x k 수용장을 보여주는 그림)**

***

### **페이지 712**

**하나의 출력 픽셀에 몇 개의 입력 픽셀이 기여하는가?**

*   각 **출력 픽셀**에 대해, 네트워크는 입력의 **지역적 영역**으로부터 정보를 결합하여 그 값을 계산합니다.
*   만약 합성곱 커널의 크기가 **k × k**라면, **k × k**개의 입력 픽셀이 **하나의 출력 픽셀**을 생성하는 데 관여합니다.
*   더 깊이 들어갈수록, 수용장은 **확장**됩니다 — 각 더 깊은 유닛은 **원본 이미지의 더 넓은 부분**에 의존합니다.

**(이미지: 합성곱 레이어를 여러 개 쌓을수록 최종 출력 픽셀 하나에 대한 원본 입력 이미지의 수용장이 점점 더 넓어지는 과정을 보여줌)**

***

### **페이지 713**

**하나의 출력 픽셀에 몇 개의 입력 픽셀이 기여하는가?**

*   각 **출력 픽셀**에 대해, 네트워크는 입력의 **지역적 영역**으로부터 정보를 결합하여 그 값을 계산합니다.
*   만약 합성곱 커널의 크기가 **k × k**라면, **k × k**개의 입력 픽셀이 **하나의 출력 픽셀**을 생성하는 데 관여합니다.
*   더 깊이 들어갈수록, 수용장은 **확장**됩니다 — 각 더 깊은 유닛은 **원본 이미지의 더 넓은 부분**에 의존합니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 714**

**하나의 출력 픽셀에 몇 개의 입력 픽셀이 기여하는가?**

*   각 **출력 픽셀**에 대해, 네트워크는 입력의 **지역적 영역**으로부터 정보를 결합하여 그 값을 계산합니다.
*   만약 합성곱 커널의 크기가 **k × k**라면, **k × k**개의 입력 픽셀이 **하나의 출력 픽셀**을 생성하는 데 관여합니다.
*   더 깊이 들어갈수록, 수용장은 **확장**됩니다 — 각 더 깊은 유닛은 **원본 이미지의 더 넓은 부분**에 의존합니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 715**

**문제: 수용장이 너무 느리게 증가한다**

*   이론적으로, 각 합성곱 레이어는 수용장을 레이어당 (K - 1) 픽셀만큼만 증가시킵니다.
    *   **예시**: K = 3일 때, 각 레이어는 단지 2픽셀만 추가합니다.
    *   10개 레이어 후 → 수용장 ≈ 21 × 21
    *   하지만 입력 이미지는 512×512 또는 심지어 1024×1024일 수 있습니다!
*   그래서... 단일 출력 뉴런이 언제쯤 **전체 이미지**를 "볼" 수 있을까요?
    *   → **수백 개의 레이어**가 필요할 것입니다 — 명백히 비효율적입니다.
*   이것이 CNN이 수용장을 더 빠르게 확장하기 위해 **풀링, 스트라이드 합성곱, 또는 확장된 합성곱**을 도입하는 이유입니다.

**(이미지: 512x512 크기의 큰 입력 이미지에 비해 수용장이 매우 느리게 증가하는 문제를 보여줌)**

***

### **페이지 716**

**통찰: 네트워크 내부에 다운샘플링이 필요하다**

*   이전 논의로부터, 우리는 합성곱만 쌓으면 **수용장이 너무 느리게 증가**한다는 것을 깨닫습니다.
*   따라서, 네트워크는 특징 맵을 **다운샘플링**해야 합니다 — **의미적 커버리지**를 증가시키면서 공간적 크기를 줄이는 것입니다.
*   이를 통해 각 더 깊은 유닛이 입력 이미지의 **더 넓은 부분**을 "보고" 더 높은 수준의, 더 추상적인 특징을 효율적으로 포착할 수 있습니다.

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 717**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 7x7 입력에 3x3 필터를 스트라이드 2로 적용하여 3x3 출력을 생성하는 과정을 보여줌)**

***

### **페이지 718**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 필터가 2픽셀씩 이동하는 모습을 보여줌)**

***

### **페이지 719**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 필터가 2픽셀씩 이동하는 모습을 보여줌)**

***

### **페이지 720**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 721**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(수식: 입력(W), 필터(K), 패딩(P), 스트라이드(S)를 사용하여 출력 크기를 계산하는 일반 공식)**

***

### **페이지 722**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 스트라이드=2일 때 수용장이 어떻게 확장되는지 보여주는 그림)**

***

### **페이지 723**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 스트라이드=2일 때 수용장이 7x7로 확장됨을 보여줌)**

***

### **페이지 724**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 725**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 이전 그림을 반복하여 보여줌)**

***

### **페이지 726**

**스트라이드 합성곱**

*   **스트라이드 합성곱**은 필터를 한 번에 **한 픽셀 이상** 이동시키면서 합성곱을 수행합니다.
*   다시 말해, 합성곱은 1 대신 "스트라이드" 픽셀만큼 이동합니다.
    *   **예시**: 스트라이드 = 2 → 필터가 **2픽셀마다** 슬라이드합니다.
*   이는 효과적으로 **출력 크기를 줄입니다** (다운샘플링).

**(이미지: 스트라이드=2인 합성곱을 한 번 더 적용하면 수용장이 15x15로 확장됨을 보여줌)**

***

### **페이지 727**

**합성곱 예제**

*   입력 볼륨: 3 × 32 × 32
*   필터: 10개 필터, 각각 5 × 5
*   스트라이드: 1
*   패딩: 2

**(이미지: 합성곱 연산을 나타내는 다이어그램)**

***

### **페이지 728**

**합성곱 예제**

*   입력 볼륨: 3 × 32 × 32
*   필터: 10개 필터, 각각 5 × 5
*   스트라이드: 1
*   패딩: 2
*   출력 볼륨 크기: 10 x 32 x 32
    *   32 = (32 + 2*2 - 5)/1 + 1

**(이미지 및 수식: 출력 크기 계산 과정을 보여줌)**

***

### **페이지 729**

**합성곱 예제**

*   **학습 가능한 파라미터 수**
    *   각 필터: 3 × 5 × 5 = 75개의 가중치 + 1개의 편향
    *   → 10 × (75 + 1) = **760**개의 파라미터

**(이미지 및 수식: 파라미터 수 계산 과정을 보여줌)**

***

### **페이지 730**

**합성곱 예제**

*   **곱셈-덧셈 연산(MACs) 수**
    *   각 출력 값: 75 MACs
    *   총 출력: 10 × 32 × 32 = 10,240
    *   → 75 × 10,240 = **768,000** MACs

**(이미지 및 수식: 연산량(MACs) 계산 과정을 보여줌)**

***

### **페이지 731**

**합성곱 예제**

*   **완전 연결 레이어와 비교**
    *   펼쳐진 입력: 3×32×32 = 3072
    *   출력 유닛: 10
    *   → 파라미터: 3072 × 10 + 10 = **30,730**
    *   → MACs: 3072 × 10 = **30,720**

**(이미지 및 수식: 이전 페이지의 계산 결과와 완전 연결 레이어의 경우를 비교)**

***

### **페이지 732**

**합성곱 요약**

*   요약.

**(이미지: 합성곱 레이어의 입력, 하이퍼파라미터(커널 크기, 필터 수, 패딩, 스트라이드), 가중치 행렬, 출력 크기 등을 요약하고 일반적인 설정 값들을 보여주는 표)**

***

### **페이지 733**

**PyTorch 합성곱 레이어 정의**

*   학습 가능한 가중치: shape = (out_channels, in_channels, kH, kW)
*   학습 가능한 편향: shape = (out_channels,)

**(이미지: PyTorch의 `torch.nn.Conv2d` 클래스에 대한 공식 문서 스크린샷. "우리는 그룹이나 팽창에 대해서는 이야기하지 않았습니다..."라는 메모 포함)**

***

### **페이지 734**

**PyTorch 합성곱 레이어 정의**

*   학습 가능한 가중치: shape = (out_channels, in_channels, kH, kW)
*   학습 가능한 편향: shape = (out_channels,)

**(코드: PyTorch에서 `nn.Conv2d`를 사용하여 합성곱 레이어를 정의하는 예시 코드)**

***

### **페이지 735**

**다른 차원에 존재하는 합성곱**

*   합성곱은 **2D 이미지에만 국한되지 않습니다**.
    입력 데이터의 차원에 따라 달라집니다.

**(이미지: 2D 합성곱(이미지용)과 1D 합성곱(시퀀스 데이터용)을 비교하는 그림)**

***

### **페이지 736**

**다른 차원에 존재하는 합성곱**

*   합성곱은 **2D 이미지에만 국한되지 않습니다**.
    입력 데이터의 차원에 따라 달라집니다.

**(이미지: 2D 합성곱(이미지용)과 3D 합성곱(볼륨 데이터용)을 비교하는 그림)**

***

### **페이지 737**

**풀링: 다운샘플링의 또 다른 방법**

*   스트라이드 합성곱 외에도, CNN은 특징 맵을 다운샘플링하기 위해 **풀링 레이어**를 사용합니다.
    *   특징 맵의 **공간적 크기(H × W)를 줄입니다**.
    *   **채널의 수**는 동일하게 유지합니다.
*   **풀링 레이어의 장점 - 비모수적 연산**
    *   풀링은 **학습 가능한 파라미터가 없습니다** — 단순히 고정된 연산(최대값 또는 평균)을 적용합니다.
    *   → 이는 **간단하고, 빠르며, 메모리 효율적**이게 만듭니다.
    *   모델의 용량을 증가시키지 않으므로, 추가 가중치로 인한 **과적합의 위험을 줄입니다**.

**(이미지: 64x224x224 입력이 풀링을 통해 64x112x112로 다운샘플링되는 과정을 보여주는 다이어그램)**

***

### **페이지 738**

**풀링: 다운샘플링의 또 다른 방법**

*   **일반적인 유형**
    *   **최대 풀링**: 각 지역 영역에서 **최대값**을 취합니다.
    *   **평균 풀링**: 각 지역 영역에서 **평균값**을 취합니다.

**(이미지 및 코드: 2x2 최대 풀링이 어떻게 작동하는지 보여주는 예시와 PyTorch 코드)**

***

### **페이지 739**

**풀링: 다운샘플링의 또 다른 방법**

*   **평균 풀링 (쌍선형 다운샘플링)**
    *   **평균 풀링**은 다운샘플링 방법입니다.
    *   이미지는 작은 블록(예: 2×2, 3×3)으로 나뉩니다.
    *   각 블록은 **픽셀의 평균값**으로 대체됩니다.

**(이미지 및 코드: 평균 풀링이 어떻게 작동하는지 보여주는 예시와 PyTorch 코드)**

***

### **페이지 740**

**CNN의 이동 등변성**

*   **합성곱**과 **풀링** 레이어는 CNN을 **이동 등변(translation equivariant)**하게 만듭니다.
*   만약 입력이 이동하면, → **출력 특징 맵도 동일한 양만큼 이동**하지만, **활성화 패턴은 동일하게 유지**됩니다.
*   입력의 이동은 출력에서 상응하는 이동을 유발합니다 — 표현이 객체와 "함께 움직입니다".

**(이미지: 입력 이미지가 이동하면 출력 특징 맵도 동일하게 이동하는 것을 보여주는 다이어그램)**

***

### **페이지 741**

**CNN의 이동 등변성**

*   **합성곱**과 **풀링** 레이어는 CNN을 **이동 등변(translation equivariant)**하게 만듭니다.
*   만약 입력이 이동하면, → **출력 특징 맵도 동일한 양만큼 이동**하지만, **활성화 패턴은 동일하게 유지**됩니다.
*   입력의 이동은 출력에서 상응하는 이동을 유발합니다 — 표현이 객체와 "함께 움직입니다".

**직관: 이미지의 특징은 이미지 내 위치에 의존하지 않습니다.**
**CNN은 이미지 어디에서나 동일한 특징을 탐지할 수 있습니다.**

**(이미지: 이동 등변성의 수학적 표현(Conv(Translate(X)) = Translate(Conv(X)))과 그 의미를 설명하는 다이어그램)**

***

### **페이지 742**

**참고 자료**

*   Stanford CS231N | 2025년 봄 | 강의 5: CNN을 이용한 이미지 분류
    *   https://www.youtube.com/watch?v=f3g1zGdxptI&t=2882s

***

### **페이지 743**

**프로그래밍 과제 (OpenCV + MediaPipe)**

*   **1. 비디오 I/O 연습**
    *   **목표**: 비디오 스트림을 캡처, 표시, 저장하는 방법을 배웁니다.
    *   **과제**:
        *   자신의 장치(예: 노트북 웹캠, 데스크톱 웹캠 또는 스마트폰 카메라)를 사용하여 짧은 비디오 클립을 녹화합니다.
        *   녹화된 비디오를 OpenCV에서 로드하고 재생합니다.
        *   각 프레임에 간단한 이미지 연산을 적용합니다. 예:
            *   그레이스케일로 변환 (`cv2.cvtColor`) / 수평으로 뒤집기 (`cv2.flip`)
            *   적절한 코덱(예: XVID 또는 MP4V)으로 `cv2.VideoWriter`를 초기화합니다.
            *   처리된 비디오를 `.avi` 또는 `.mp4`로 저장합니다.
        *   라이브 웹캠(`cv2.VideoCapture(0)`) 피드를 **FPS 오버레이**와 함께 실시간으로 표시합니다.

*   **2. GUI 상호작용 연습**
    *   **목표**: 키보드, 트랙바, 마우스를 사용한 대화형 제어를 배웁니다.
    *   **과제**:
        *   **키보드 제어**: 키(q, s 등)를 눌러 다른 동작(예: 그레이스케일 토글, 스냅샷)을 트리거합니다.
        *   **트랙바 제어**: 트랙바로 임계값을 조정하여 **이진화 결과**를 동적으로 업데이트합니다.
        *   **마우스 상호작용**:
            *   왼쪽 클릭으로 점을 그리고, 오른쪽 클릭으로 원을 그립니다.
            *   왼쪽 마우스 버튼으로 드래그하여 **관심 영역(ROI)을 잘라냅니다**.

***

### **페이지 744**

**프로그래밍 과제 (OpenCV + MediaPipe)**

*   **3. 그리기 함수 연습**
    *   **목표**: OpenCV의 기본 그리기 및 주석 함수를 연습합니다.
    *   **과제**:
        *   빈 캔버스에 도형(사각형, 원, 선, 다각형)을 그립니다.
        *   다른 글꼴, 색상, 크기로 텍스트 주석(`cv2.putText`)을 추가합니다.
        *   간단한 **스케치패드**를 만듭니다: 마우스를 누르고 드래그하여 자유로운 선을 그립니다.

*   **4. MediaPipe 연습**
    *   **목표**: 실시간 랜드마크 탐지 및 시각화를 탐색합니다.
    *   **과제**:
        *   **인체 자세**: 웹캠을 사용하여 33개의 신체 랜드마크와 함께 스켈레톤을 표시합니다.
        *   **손 자세**: 양손을 탐지하고, 21개의 랜드마크를 그리며, 핀치 거리(엄지-검지)를 표시합니다.
        *   **얼굴 메쉬**: 468개의 랜드마크와 얼굴 윤곽을 실시간으로 표시합니다.

*   **제출 가이드라인**
    *   **제출물**: 코드, 보고서(pdf)

***

### **페이지 745**

**보고서 필수 섹션**

*   **구현 요약**
    *   구현한 내용에 대한 간략한 설명
    *   어떤 핵심 OpenCV 함수가 사용되었는지
    *   높은 수준의 워크플로우 (전체 코드가 아닌, 주요 로직만)

*   **실행 결과**
    *   프로그램 실행 **스크린샷** (예: 원본 vs 처리된 비디오 프레임, 주석이 달린 이미지, 랜드마크 오버레이)
    *   다른 경우를 보여주기 위해 필요한 경우 여러 예시 포함
    *   각 스크린샷 아래에 명확한 캡션

*   **토론**
    *   직면했던 어려움과 해결 방법
    *   한계 또는 가능한 개선 사항
    *   이 과제를 통해 배운 점

***

### **페이지 746**

**프로그래밍 과제 04**

*   **1. 목표**

**(이미지: 입력 이미지 → 키포인트 탐지 → 기술자(영역) 추출로 이어지는 특징 기반 컴퓨터 비전 파이프라인을 보여주는 그림)**

***

### **페이지 747**

**프로그래밍 과제 04**

*   **1. 목표**
    *   이 과제는 고전 컴퓨터 비전의 핵심 시각적 파이프라인에 중점을 둡니다:
        *   **(1) 특징 탐지 → (2) 특징 기술 → (3) 특징 매칭.**
    *   이 작업을 마치면 다음을 할 수 있어야 합니다:
        *   내장된 OpenCV 함수를 사용하여 이미지에서 키포인트를 탐지합니다.
        *   지역 기술자(SIFT, ORB 등)를 추출합니다.
        *   두 이미지 간의 특징을 매칭하고 결과를 시각화합니다.
        *   기술자 성능과 매칭 품질을 분석하고 비교합니다.

***

### **페이지 748**

**프로그래밍 과제 04**

*   **2. 데이터 및 설정**
    *   **제공된 데이터셋 (필수):**
        *   `Lab03/images.zip` 폴더의 모든 이미지를 사용합니다 (예: `box.png`, `box_in_scene.png` 등).
        *   이 이미지들에 대한 모든 결과를 생성하여 보고서에 포함합니다.
    *   **자신이 촬영한 사진 (필수):**
        *   동일한 객체를 **다른 시점**(각도, 거리 또는 회전)에서 **두 장의 사진**을 찍습니다.
        *   두 이미지 간에 매칭이 가능한지 확인합니다.
        *   이것이 추가 테스트 쌍이 될 것입니다.

***

### **페이지 749**

**3. 과제**

*   **파트 A. 특징 탐지** OpenCV의 내장 코너/키포인트 탐지 함수 사용:
    *   **이동된 창의 제곱 차이 합(SSD)**을 사용하여 간단한 코너 탐지기를 **처음부터 구현**합니다. 이 섹션은 OpenCV의 탐지기와의 직접적인 이해와 비교를 위한 것입니다.
*   **회색조 이미지의 각 픽셀 (i, j)에 대해:**
    *   **창 선택**
        *   (i, j)를 중심으로 하는 w × w 크기의 정사각형 창 W를 사용합니다 (예: w ∈ {3, 5, 7, 8, 9, ...})
    *   **방향 이동**
        *   집합 {(±1,0), (0,±1), (±1,±1)} (총 8개 방향)의 각 작은 이동 (u, v)에 대해 다음을 계산합니다:
    *   **코너 점수**
        *   (i, j)에서의 코너 점수를 8개 방향 SSD의 **최소값**으로 정의합니다.
    *   **임계값 처리**
        *   S(i, j)가 전역 또는 백분위수 임계값보다 높은 픽셀만 유지합니다 (또는 상위 K개 선택).
    *   **시각화**
        *   탐지된 점을 원본 이미지에 원으로 그립니다.
        *   점수 히트맵(선택 사항)과 오버레이 결과를 모두 저장합니다.
*   **보고할 내용:**
    *   탐지된 코너 (임계값 처리된 결과를 원본 이미지에 오버레이).
    *   코너 점수의 히트맵.
    *   창 크기 w(예: 5, 7, 9) / 임계값(절대값 또는 백분위수)과 같은 **최소 두 가지 다른 파라미터 설정**을 시도합니다.
    *   결과가 어떻게 변하는지 비교합니다.
    *   해리스/시-토마시와의 결과를 비교하는 문장을 포함합니다.

***

### **페이지 750**

**3. 과제**

*   **파트 A. 특징 탐지** OpenCV의 내장 코너/키포인트 탐지 함수 사용:
    *   `cv2.cornerHarris`를 사용하여 **해리스 코너 탐지**를 적용하거나, `cv2.goodFeaturesToTrack`을 사용하여 **시-토마시**를 적용합니다.
    *   탐지된 점들을 원본 이미지에 오버레이하여 시각화합니다.
    *   파라미터(블록 크기, 커널 크기, 임계값 등)를 조정하고 작은 표로 요약합니다.
    *   다른 파라미터가 탐지된 코너의 밀도나 분포에 어떻게 영향을 미치는지 간략하게 논의합니다.

**(코드: 해리스 코너 탐지를 수행하는 Python 코드 예시)**

***

### **페이지 751**

**3. 과제**

*   **파트 A. 특징 탐지** OpenCV의 내장 코너/키포인트 탐지 함수 사용:
    *   `cv2.cornerHarris`를 사용하여 **해리스 코너 탐지**를 적용하거나, `cv2.goodFeaturesToTrack`을 사용하여 **시-토마시**를 적용합니다.
    *   탐지된 점들을 원본 이미지에 오버레이하여 시각화합니다.
    *   파라미터(블록 크기, 커널 크기, 임계값 등)를 조정하고 작은 표로 요약합니다.
    *   다른 파라미터가 탐지된 코너의 밀도나 분포에 어떻게 영향을 미치는지 간략하게 논의합니다.

**(코드: 시-토마시 코너 탐지를 수행하는 Python 코드 예시)**

***

### **페이지 752**

**3. 과제**

*   **파트 B. 특징 기술자**
    *   **B-1. SIFT**
        *   `cv2.SIFT_create()`를 사용하여 키포인트와 128D 기술자를 추출합니다.
        *   **보고서:**
            *   탐지된 키포인트의 수.
            *   기술자 차원 (128).
            *   선택된 키포인트 주변의 2-3개 샘플 패치(예: 16×16 영역)와 간단한 설명.

**(코드 및 이미지: SIFT 특징을 탐지하고 시각화하는 Python 코드와 그 결과)**

***

### **페이지 753**

**3. 과제**

*   **파트 B. 특징 기술자**
    *   **B-2. ORB**
        *   `cv2.ORB_create()`를 사용하여 이진 기술자를 탐지하고 계산합니다.
        *   ORB 기술자는 이진(일반적으로 256비트)이며 **해밍 거리**를 사용한다는 점에 유의하세요.
        *   보고서에서 SIFT와 ORB를 비교합니다 (매치 수, 견고성, 시각적 명확성 또는 속도).

**(코드 및 이미지: ORB 특징을 탐지하고 시각화하는 Python 코드와 그 결과)**

***

### **페이지 754**

**3. 과제**

*   **파트 C. 특징 매칭**
    *   **C-1. 매칭 설정**
        *   OpenCV의 **Brute-Force Matcher** (`cv2.BFMatcher`)를 사용합니다 - 무차별 최근접 이웃 매칭.
            *   **SIFT** (부동소수점 기술자)의 경우: **L2 (유클리드) 거리**를 사용합니다.
            *   **ORB** (이진 기술자)의 경우: **해밍 거리**를 사용합니다.
        *   **KNN 매칭 (k=2)**을 사용하여 두 이미지 간의 기술자를 매칭합니다.
        *   또는 `cv2.Flann`을 사용합니다.
    *   **C-2. Lowe의 비율 테스트**
        *   **Lowe의 비율 테스트**를 구현하고 적용하여 매치를 필터링합니다.
        *   최소 두 개의 임계값(예: 0.7, 0.8)을 시도하고 수용된 매치 수가 어떻게 변하는지 비교합니다.
    *   **C-3. 시각화**
        *   **상위 N개**의 매치를 시각화합니다 (예: N=50).
        *   "상위"는 **가장 작은 거리 값**(더 나은 유사성)을 가진 매치를 의미합니다.
        *   `cv2.drawMatches()` 또는 유사한 시각화를 사용합니다.
        *   다음을 요약하는 간단한 표를 포함합니다:
            *   키포인트 수 (이미지 A / 이미지 B)
            *   원본 매치 수
            *   비율 필터링 후 매치 수

***

### **페이지 755**

**3. 과제**

*   **파트 C. 특징 매칭 - SIFT**

**(코드 및 이미지: SIFT 특징을 사용하여 두 이미지 간의 특징을 매칭하고 Lowe의 비율 테스트를 적용한 후 상위 N개의 매치를 시각화하는 Python 코드와 그 결과)**

***

### **페이지 756**

**3. 과제**

*   **파트 C. 특징 매칭 - ORB**

**(코드 및 이미지: ORB 특징을 사용하여 두 이미지 간의 특징을 매칭하고 Lowe의 비율 테스트를 적용한 후 상위 N개의 매치를 시각화하는 Python 코드와 그 결과)**

***

### **페이지 757**

**3. 과제**

*   **파트 D. 호모그래피를 통한 객체 인식**
    *   특징 매칭 실험을 확장하여, 매칭된 특징점을 사용하여 더 큰 장면 이미지 내에서 **객체 인식**을 수행합니다.
    *   이 부분은 특징 대응 관계를 사용하여 두 이미지 간의 **공간적 변환**(호모그래피)을 추정하고 장면에서 객체의 위치를 찾는 방법을 보여줍니다.

**(이미지: 특징 매칭과 호모그래피를 사용하여 큰 이미지에서 특정 객체(로봇)를 찾아 경계 상자로 표시한 결과)**

***

### **페이지 758**

**3. 과제**

*   **파트 D. 호모그래피를 통한 객체 인식**

**(코드: RANSAC을 사용하여 호모그래피를 찾고, 이를 이용해 장면 이미지에 객체의 경계를 그리는 Python 코드 예시)**

***

### **페이지 759**

**3. 과제**

*   데이터셋 및 추가 사진 쌍
*   사용된 방법 및 주요 파라미터
*   정량적 결과 표 (#키포인트, #매치 등)
*   제공된 모든 이미지 및 사용자 지정 이미지에 대한 시각화
*   2-3가지 통찰 (예: 어떤 방법이 더 안정적이었는지, 비율 임계값의 효과, 일반적인 실패 사례)

***

### **페이지 760**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 761**

**프로그래밍 과제 5 - 과제 목표**

*   전통적인 **특징 기반** 접근법과 현대적인 **비-특징 기반** 접근법을 모두 사용하여 **이미지 분류**를 구현합니다.
*   **최근접 이웃(NN)** 및 **선형 분류기** 모델의 성능을 비교합니다.
*   공정한 평가를 위해 데이터셋을 **훈련, 검증, 테스트** 세트로 분할합니다.
*   k-최근접 이웃에서 다른 **k 값**에 따라 분류 정확도가 어떻게 변하는지 분석합니다.
*   최근접 이웃 결과에서 **매칭된 이미지 쌍**을 시각화합니다.
*   **선형 분류기**를 훈련한 후, 학습된 패턴을 해석하기 위해 **가중치 행렬**을 시각화합니다.

***

### **페이지 762**

**데이터셋: CIFAR-10**

*   **데이터셋**: CIFAR-10 (Canadian Institute for Advanced Research)
*   **목적**: 일반 이미지 분류 (10개 객체 카테고리)
*   **클래스**: 비행기, 자동차, 새, 고양이, 사슴, 개, 개구리, 말, 배, 트럭
*   **이미지 크기**: 32 × 32 RGB 컬러 이미지
*   **총 샘플 수**: 60,000 (훈련용 50,000, 테스트용 10,000)

**(코드: TensorFlow/Keras를 사용하여 CIFAR-10 데이터셋을 로드하고 전처리하는 Python 코드 예시)**

***

### **페이지 763**

**원시 픽셀 k-최근접 이웃 분류기**

*   **원시 픽셀**(특징 없음)을 사용하고 **최근접 이웃** 검색으로 레이블을 예측하는 이미지 분류기를 구현합니다.
*   **규칙 및 제약 조건**
    *   분류기(거리, 이웃 검색, 투표)에는 **NumPy만** 사용합니다.
    *   데이터셋 = **CIFAR-10** (32×32×3).
    *   **훈련 / 검증 / 테스트**로 분할합니다.
    *   여러 **k 값**을 시도하고, **검증** 세트에서 가장 좋은 것을 선택한 다음, **테스트** 정확도를 보고합니다.
    *   일부 테스트 샘플에 대해 **k개의 가장 가까운 훈련 이미지**를 보여줍니다.

***

### **페이지 764**

**특징 추출 + 최근접 이웃 (k-NN)**

*   **아이디어**: 각 이미지를 단일의 전역 **HOG 기술자**(그레이스케일 → HOG 특징 벡터)로 변환한 다음, 해당 벡터에 대해 **k-최근접 이웃**을 실행합니다.
*   **왜 HOG인가?** 고전적인 CV 기준선보다 훨씬 강력한, 작은 조명 및 자세 변화에 강인한 엣지 방향 구조를 포착합니다.
*   `from skimage.feature import hog`
*   이미지에서 **전역 특징**을 추출합니다.
*   **HOG 특징** (전역 기술자; 키포인트 기반 아님)
*   **색상 히스토그램 특징** (채널별 RGB 히스토그램)
*   **융합된 특징** = HOG ⊕ ColorHist (연결)
*   세 가지 모두에 걸쳐 결과를 비교합니다.

**(코드: HOG 특징 추출기를 정의하는 Python 코드 예시)**

***

### **페이지 765**

**원시 픽셀 + 로지스틱 회귀 (scikit-learn 사용)**

*   **과제**
    *   원시 CIFAR-10 픽셀에 대해 **다항 로지스틱 회귀**를 구현하고 실험을 실행합니다.
*   **규칙**
    *   훈련을 위해 `from sklearn.linear_model import LogisticRegression`을 사용합니다.
    *   입력 = 원시 픽셀 (32×32×3 → 3072D).
    *   **훈련 / 검증 / 테스트**로 분할합니다.
    *   **시각화 필수**: 각 클래스 가중치를 **32×32×3** 템플릿으로 재구성하고 **10개 클래스 가중치**를 모두 표시합니다.
    *   **성능 보고 필수**: (하이퍼파라미터에 대한) **검증 정확도**와 **최종 테스트 정확도**를 보고합니다.

***

### **페이지 766**

**특징 추출 + 선형 분류기 (로지스틱 회귀)**

*   **과제**
    *   이미지에서 **전역 특징**을 추출한 다음, **다항 로지스틱 회귀** 분류기를 훈련시킵니다:
        *   **HOG 특징** (전역 기술자; 키포인트 기반 아님)
        *   **색상 히스토그램 특징** (채널별 RGB 히스토그램)
        *   **융합된 특징** = HOG ⊕ ColorHist (연결)
        *   세 가지 모두에 걸쳐 결과를 비교합니다.
*   **규칙**
    *   데이터셋: **CIFAR-10 (32×32 RGB)**
    *   분할: **훈련 / 검증 / 테스트**
    *   훈련: `sklearn.linear_model.LogisticRegression` (다항)
    *   스케일링이 권장됩니다 (예: `StandardScaler`).
    *   **보고서**: 검증 정확도(하이퍼파라미터용), 최종 테스트 정확도, 비교 표 (HOG vs ColorHist vs 융합).

***

### **페이지 767**

**제출 가이드라인 (단일 PDF 필수)**

*   **1. 실행 증거**
    *   스크린샷은 다음을 명확하게 보여야 합니다:
        *   **사용자 코드** (에디터 또는 노트북)
        *   **터미널 출력** (훈련 로그, 정확도 등)
        *   **결과 창** (예: imshow 출력, 플롯)
    *   각 실험 섹션에는 **코드가 보이는 스크린샷 + 출력 + 시각화**가 최소 하나 이상 포함되어야 합니다.
*   **2. k-최근접 이웃 (원시 픽셀)**
    *   여러 k 값(예: k = 1, 3, 5, 7, 9)에 대한 **검증/테스트 정확도**를 보여줍니다.
    *   정확도 vs. k를 비교하는 **표 또는 그래프**를 포함합니다.
    *   k가 증가하거나 감소함에 따라 정확도가 어떻게 변하는지 분석합니다.
    *   최소 하나의 테스트 이미지에 대해, 훈련 세트로부터의 **k개의 최근접 이웃**을 표시합니다 (이웃 이미지와 예측/실제 레이블 모두 표시).
*   **3. 원시 픽셀 + 로지스틱 회귀**
    *   `sklearn.linear_model import LogisticRegression`을 사용합니다.
    *   **검증 및 테스트 정확도**를 보고합니다.
    *   **학습된 가중치 행렬을 시각화**합니다:
        *   각 클래스 가중치를 **32×32×3**으로 재구성합니다.
        *   **10개 클래스 가중치 템플릿**을 모두 하나의 그림에 표시합니다.
        *   클래스 레이블(예: 비행기, 고양이, 개 등)을 추가합니다.
        *   각 클래스 가중치가 무엇을 포착하는 것처럼 보이는지(예: 색조, 질감 패턴)에 대한 **간단한 분석**을 제공합니다.

***

### **페이지 768**

**제출 가이드라인 (단일 PDF 필수)**

*   **4. 특징 추출 + 로지스틱 회귀**
    *   **세 가지 설정**을 비교합니다:
        *   HOG 특징
        *   색상 히스토그램 특징
        *   융합된 특징 = HOG + 색상 히스토그램
    *   각각에 대해 **검증 및 테스트 정확도**를 보고합니다.
    *   **비교 표 또는 막대 차트**를 포함합니다.
    *   어떤 특징 표현이 가장 좋은 성능을 보이며 그 이유(예: 엣지 vs. 색상 민감도, 융합의 보완적 특징)에 대한 **간단한 분석**을 작성합니다.
*   **5. 전체 토론**
    *   **특징 기반** 접근법과 **원시 픽셀 기반** 접근법을 비교합니다.
    *   **토론:**
        *   어느 것이 더 높은 정확도를 달성했으며 그 이유는 무엇인가
        *   표현 품질 vs. 분류기 유형의 역할
        *   시각화로부터의 통찰 (예: 가중치 템플릿, 이웃 예시)
*   **최종 제출물 (Assignment\_\<학번\>\_\<이름\>.zip)**
    *   모든 코드 파일 (Python 스크립트 또는 노트북)
    *   **단일 PDF 파일** 제출:
    *   PDF에는 **모든 스크린샷, 표, 그림, 분석**이 명확하게 레이블링되어 포함되어야 합니다.
    *   모든 스크린샷, 그림, 표, 분석은 보고서에서 명확하게 보여야 합니다.
    *   코드는 수정 없이 **실행 가능**해야 합니다.
    *   모든 결과는 **하나의 압축 파일(.zip)**로 제출해야 합니다.
    *   **파일 이름 규칙을 준수하지 않으면 감점 또는 거부**될 수 있습니다.

***

### **페이지 769**

**프로그래밍 과제 06: 신경망 구현**

*   **개요**
    *   이 과제에서는 기본 Python과 NumPy만을 사용하여 **간단한 신경망을 수동으로 구현**하고 훈련시킬 것입니다.
    *   목표는 순전파, 미분을 통한 그래디언트 계산, 경사 하강법을 사용한 파라미터 업데이트를 포함한 계산의 각 부분을 코딩함으로써 신경망이 내부적으로 어떻게 작동하는지 이해하는 것입니다.
    *   PyTorch, TensorFlow 또는 Keras와 같은 **딥러닝 프레임워크를 사용하는 것은 허용되지 않습니다**.
*   신경망은 장난감 데이터에 대한 간단한 분류 또는 회귀 작업을 위해 설계된 **2계층 아키텍처**(하나의 은닉 레이어)가 될 것입니다.

***

### **페이지 770**

**프로그래밍 과제 06: 신경망 구현**

*   **학습 목표**
    *   **2계층 신경망**(입력 → 은닉 → 출력)의 기본 구조를 이해합니다.
    *   **순전파 계산**과 **그래디언트 기반 파라미터 업데이트** 과정을 수동으로 구현합니다.
    *   **활성화 함수**와 **손실 함수**가 네트워크의 학습 행동에 어떻게 영향을 미치는지 배웁니다.
    *   훈련 과정을 시각화하고 반복적인 학습을 통해 모델이 개선되는지 확인합니다.
    *   외부 라이브러리에 의존하지 않고 신경망이 어떻게 학습하는지에 대한 더 깊은 직관을 개발합니다.

***

### **페이지 771**

**1단계: 2계층 신경망 구현하기 (순전파만)**

*   **목표**
    *   이 단계에서는 간단한 **2계층 신경망**을 처음부터 구현합니다.
    *   네트워크는 하나의 컬러 이미지를 입력으로 받아 각 클래스에 대한 예측 확률을 출력합니다.
    *   이 단계는 **순전파 계산**에만 초점을 맞춥니다 — 학습 및 가중치 업데이트는 나중에 추가될 것입니다.
*   **네트워크 구조**
    *   입력 (H × W × 3) → 은닉 레이어 (사용자 정의 노드 수) → 출력 레이어 (C 클래스)
    *   **입력 펼치기**
        *   입력 이미지(H×W×3)를 길이 H*W*3의 1-D 벡터로 변환합니다.
    *   **레이어 1 (은닉 레이어)**
        *   계산: Z1 = X * W1 + b1
        *   활성화 적용: H1 = ReLU(Z1) (ReLU(x) = max(0, x))
    *   **레이어 2 (출력 레이어)**
        *   계산: S = H1 * W2 + b2
        *   소프트맥스를 적용하여 클래스 확률 얻기: P = softmax(S)
    *   **출력**
        *   P는 각 클래스에 대한 예측 확률을 나타내는 길이 C의 벡터입니다.
        *   P의 모든 값의 합은 대략 **1.0**이어야 합니다.

***

### **페이지 772**

**1단계: 2계층 신경망 구현하기 (순전파만)**

*   **파라미터 초기화**
    *   모든 가중치와 편향을 작은 무작위 숫자로 초기화합니다.
    *   (예: `np.random.rand()` 또는 `np.random.randn()`을 0.01로 스케일링)
    *   Xavier나 He와 같은 특별한 초기화 방법을 사용할 필요는 없습니다.
*   **제한 사항**
    *   PyTorch, TensorFlow 또는 Keras를 사용하지 **마세요**.
    *   수학적 연산에는 **NumPy만** 허용됩니다.
*   **체크리스트**
    *   입력 → 올바르게 펼쳐짐
    *   은닉 및 출력 레이어가 올바르게 계산됨
    *   소프트맥스 확률의 합이 1
    *   코드가 단일 이미지 입력(배치 차원 없음)으로 실행됨
    *   딥러닝 라이브러리 사용 안 함

***

### **페이지 773**

**1단계: 2계층 신경망 구현하기 (순전파만)**

**(코드: NumPy만을 사용하여 2계층 신경망의 순전파를 구현하는 전체 Python 코드. 단일 샘플을 테스트하는 코드도 포함됨)**

***

### **페이지 774**

**2단계: 데이터셋 로드하기 (MNIST)**

*   **왜 MNIST인가?**
    *   손으로 쓴 숫자 (0 – 9)
    *   각 이미지는 **28 × 28 × 1 (회색조)**
    *   총 **10개 클래스**
    *   **60,000개**의 훈련 샘플과 **10,000개**의 테스트 샘플
    *   작고, 깨끗하며, 신경망을 처음부터 훈련하는 방법을 배우기에 완벽함
*   **데이터 형식**
    *   각 샘플은 다음으로 구성됩니다:
        *   **이미지**: (28, 28) ← 회색조 이미지
        *   **레이블**: int 0–9 ← 정확한 숫자

**(이미지: MNIST 데이터셋의 숫자 이미지 샘플들)**

***

### **페이지 775**

**2단계: 데이터셋 로드하기 (MNIST)**

*   **데이터셋 로드하기 (간소화됨)**

**(코드: TensorFlow/Keras를 사용하여 MNIST 데이터셋을 로드하고, 정규화한 후, 이전 단계에서 구현한 2계층 신경망에 하나의 샘플을 통과시켜 예측하는 Python 코드)**

***

### **페이지 776**

**3단계: 신경망 훈련하기 (손실 + 경사 하강법)**

*   이 단계에서는 2단계에서 로드한 MNIST 데이터셋을 사용하여 2계층 신경망을 훈련시킬 것입니다.
*   대신, **수학적 그래디언트 결과**가 제공되며, 여러분의 과제는 NumPy를 사용하여 이를 **직접 구현**하는 것입니다.
*   **지침**
    *   분류를 위해 **교차 엔트로피 손실**을 사용합니다.
    *   **경사 하강법**을 구현합니다:
        *   다음 그래디언트들은 이미 여러분을 위해 유도되었습니다 — 그냥 코드로 작성하세요.
*   **하나의 훈련 샘플 (x, y)에 대해:**
    *   (순전파 수식)
*   **주어진 그래디언트**
    *   (역전파 그래디언트 수식)

***

### **페이지 777**

**3단계: 신경망 훈련하기 (손실 + 경사 하강법)**

**(코드: NumPy만을 사용하여 2계층 신경망을 처음부터 훈련하는 전체 Python 코드. 순전파, 역전파(주어진 그래디언트 사용), 파라미터 업데이트를 포함)**

***

### **페이지 778**

**3단계: 신경망 훈련하기 (손실 + 경사 하강법)**

*   여기에 **미니배치 경사 하강법** 예시가 있습니다.
    3단계 코드의 훈련 루프를 대체하여 이것을 삽입하세요 (변수와 초기화는 변경되지 않음).

**(코드: 샘플별 그래디언트를 누적하여 미니배치 단위로 파라미터를 업데이트하는 미니배치 경사 하강법의 Python 코드)**

***

### **페이지 779**

**3단계: 신경망 훈련하기 (손실 + 경사 하강법)**

*   여기에 현재 미니배치 루프에 연결할 수 있는 **다른 옵티마이저**의 예시가 있습니다. 시도해보고 비교해 보세요!
*   **1) 모멘텀 SGD (고전적)**

**(코드: 모멘텀 SGD 업데이트 규칙을 구현한 Python 코드)**

***

### **페이지 780**

**3단계: 신경망 훈련하기 (손실 + 경사 하강법)**

*   여기에 현재 미니배치 루프에 연결할 수 있는 **다른 옵티마이저**의 예시가 있습니다. 시도해보고 비교해 보세요!
*   **2) RMSProp (중심화되지 않음)**

**(코드: RMSProp 업데이트 규칙을 구현한 Python 코드)**

***

### **페이지 781**

**3단계: 신경망 훈련하기 (손실 + 경사 하강법)**

*   여기에 현재 미니배치 루프에 연결할 수 있는 **다른 옵티마이저**의 예시가 있습니다. 시도해보고 비교해 보세요!
*   **3) Adam**

**(코드: Adam 옵티마이저 업데이트 규칙을 구현한 Python 코드)**

***

### **페이지 782**

**3단계: 신경망 훈련하기 (손실 + 경사 하강법)**

*   여기에 현재 미니배치 루프에 연결할 수 있는 **다른 옵티마이저**의 예시가 있습니다. 시도해보고 비교해 보세요!
*   **3) Adam**

**제안된 시작점**
*   **Adam**: lr=1e-3
*   **모멘텀**: lr=0.1, beta=0.9
*   **RMSProp**: lr=1e-3 ~ 1e-2, decay=0.99

이것들은 다른 옵티마이저의 예시입니다. **그것들을 시도해보고** 옵티마이저, 배치 크기, 학습률에 따른 훈련 곡선(손실/정확도)을 비교하세요.

***

### **페이지 783**

**4단계: 학습률 스케줄.**

상수 `lr`을 **스케줄 함수**로 바꾸고 **다른 스케줄을 시도**해 보세요. 설정에 따른 **손실 곡선**과 **정확도**를 비교하세요.

**해야 할 일**
*   훈련 루프는 동일하게 유지합니다.
*   각 미니배치 업데이트 시, `lr = lr_fn(global_step)` (또는 에포크 기반)을 계산합니다.
*   여러 스케줄을 시도합니다: **스텝 감쇠, 코사인, 선형 감쇠, 역-제곱근 감쇠, 선형 웜업 (그리고 조합)**.
*   `lr`, 손실, 정확도를 기록하고; 훈련 후 플롯합니다.

***

### **페이지 784**

**4단계: 학습률 스케줄.**

**루프에 연결하는 방법 (최소한의 변경)**

**(코드: 기존 훈련 루프에 학습률 스케줄러를 통합하는 방법을 보여주는 Python 유사 코드)**

***

### **페이지 785**

**4단계: 학습률 스케줄.**

**스케줄 코드 스니펫 (삽입용)**
**1) 스텝 감쇠 (에포크 기반 "드롭")**

**(코드: 스텝 감쇠 학습률 스케줄러를 구현하는 Python 함수)**

***

### **페이지 786**

**4단계: 학습률 스케줄.**

**스케줄 코드 스니펫 (삽입용)**
**2) 코사인 감쇠 (재시작 없음)**

**(코드: 코사인 감쇠 학습률 스케줄러를 구현하는 Python 함수)**

***

### **페이지 787**

**4단계: 학습률 스케줄.**

**스케줄 코드 스니펫 (삽입용)**
**3) 선형 감쇠**

**(코드: 선형 감쇠 학습률 스케줄러를 구현하는 Python 함수)**

***

### **페이지 788**

**4단계: 학습률 스케줄.**

**스케줄 코드 스니펫 (삽입용)**
**4) 역-제곱근 감쇠 (트랜스포머 스타일)**

**(코드: 역-제곱근 감쇠 학습률 스케줄러를 구현하는 Python 함수)**

***

### **페이지 789**

**4단계: 학습률 스케줄.**

**스케줄 코드 스니펫 (삽입용)**
**5) 선형 웜업 (어떤 스케줄이든 감싸기)**

**(코드: 기존 스케줄 함수에 선형 웜업을 추가하는 래퍼 함수를 구현하는 Python 코드)**

***

### **페이지 790**

**4단계: 학습률 스케줄.**

**제안된 실험 (시도해보세요!)**
*   **동일한 총 에포크로 스케줄 비교:**
    *   상수 1e-3 vs. 코사인(1e-3→1e-5) vs. 선형(1e-3→1e-5) vs. 역-제곱근.
*   **웜업:** 200–1000 웜업 스텝을 추가하고; 초기 훈련이 안정화되는지 확인합니다.
*   **각 실행에 대해, 에포크별로 기록:**
    *   **훈련 손실, 훈련 정확도, 현재 lr**
*   **손실 곡선과 정확도를 플롯**하고; 어떤 스케줄이 더 빠르거나 안정적으로 수렴하는지 간략하게 논의합니다.

이것들은 학습률 스케줄의 예시입니다—**시도해보고 비교해 보세요!**

***

### **페이지 791**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 792**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **목표**
    *   이 과제에서는 매우 작은 신경망을 처음부터 구현하고 **수동으로 계산**할 것입니다:
        *   순전파
        *   역전파 (그래디언트)
        *   경사 하강법 업데이트
        *   여러 반복에 대한 훈련
        *   시간에 따른 손실 및 예측 진화
*   이 과제는 딥러닝이 실제로 어떻게 그래디언트를 계산하고 내부적으로 가중치를 학습하는지 이해하는 데 도움이 될 것입니다.

***

### **페이지 793**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **네트워크 아키텍처**
    *   고정된 2계층 신경망:

| 레이어 | 모양 | 활성화 |
| :--- | :--- | :--- |
| 입력 | 2 | — |
| 은닉 | 2 | 시그모이드 |
| 출력 | 1 | 시그모이드 |

*   **주어진 값**
    *   입력 샘플: x =ᵀ
    *   레이블: y = 1
    *   재현성을 위해 `np.random.seed(0)` 사용
*   **제한 사항**
    *   **Python + NumPy**만 허용됩니다.

***

### **페이지 794**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **과제**
    *   1. 파라미터 무작위 초기화
    *   2. 순전파
    *   3. 손실 (이진 교차 엔트로피)
    *   4. 역전파
        *   연쇄 법칙을 사용하여 수동으로 그래디언트 계산 (자동 미분 없음).
    *   5. 경사 하강법 훈련
        *   **200회 반복** 동안 반복:
            *   순전파
            *   역전파
            *   그래디언트 업데이트
            *   학습률 η = 0.1
    *   6. 학습 진행 추적
        *   값이 어떻게 변하는지 기록하고 보여주기:
            *   감소하는 손실
            *   목표 y = 1에 접근하는 ŷ
        *   원한다면 10 스텝마다 (또는 더 자주) 출력.
    *   7. 최종 분석
        *   **비교:**
            *   초기 손실 vs 최종 손실
            *   초기 예측 vs 최종 예측
        *   네트워크가 학습했는지 여부와 그 이유를 설명.

***

### **페이지 795**

**프로그래밍 과제 07: 역전파 및 정규화**

**보고서 내용**
PDF 보고서에는 다음이 포함되어야 합니다:

**모델 및 수학**
*   순전파 방정식에 대한 간략한 설명
*   역전파 및 그래디언트에 대한 간략한 설명

**코드 스크린샷**
*   주요 구현 섹션을 보여주세요
    *   순전파
    *   역전파
    *   경사 하강법 루프

**출력 및 학습 로그**
*   훈련 과정을 보여주는 표 또는 로그
    (각 체크포인트에서의 손실 + 예측)
*   최종 출력된 값
*   손실 곡선 (선 그래프 강력 추천)
*   예측 곡선 (선택 사항이지만 권장)

**해석**
*   손실이 감소했나요? (설명)
*   ŷ가 1에 가까워졌나요? (설명)
*   역전파를 수동으로 구현하면서 무엇을 배웠나요?

필수 스크린샷, 로그 또는 설명이 누락된 보고서는 감점됩니다.

***

### **페이지 796**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

*   데이터(**노드**)와 연산(**게이트**)을 분리하는 작은 계산 그래프 엔진을 구축합니다. 네 가지 게이트를 구현합니다:
    *   AddGate / MulGate / SigmoidGate / CrossEntropyLossGate (이진)
*   손실로부터 **역전파**하고 모든 필요한 그래디언트를 출력합니다.

**구현해야 할 것**
**핵심 클래스**
*   **노드**
    *   필드: `value`, `grad`, `op`, `parents`
    *   메서드: `backward(upstream_grad=1.0)`
        *   `self.grad`에 누적
        *   `self.op`가 존재하면, `self.op.backward(self, upstream_grad)` 호출 (리프 노드에 도달할 때까지 재귀적으로)
*   **게이트 (기본 클래스)**
    *   `forward(*parents)` → `value`, `op=self`, `parents=(...)`를 가진 새로운 **노드**를 반환
    *   `backward(out_node, upstream_grad)` → `out_node.parents`를 읽고, 지역 기여도를 계산하고, 각 부모의 `.backward(...)`를 호출
*   **AddGate, MulGate, SigmoidGate, CrossEntropyLossGate**
    *   각각 올바른 `forward(...)`와 `backward(...)`를 제공해야 합니다.
    *   CrossEntropyLossGate의 경우, 레이블 y를 상수(그래디언트 없음)로 취급합니다. 안정성을 위해 내부적으로 작은 `eps=1e-12`를 사용합니다.

***

### **페이지 797**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

**예제 테스트 값**
*   입력: x1 = -1.0, x2 = -2.0
*   파라미터: w1 = 2.0, w2 = -3.0, b = -3.0
*   레이블: y = 1 (이진)

**구축할 그래프**
1.  s1 = w1 * x1
2.  s2 = w2 * x2
3.  s3 = s1 + s2 + b
4.  y_hat = sigmoid(s3)
5.  loss = cross_entropy(y_hat, y)
6.  `loss.backward(1.0)` 호출

**출력할 내용**
*   순전파 값: s1, s2, s3, y_hat, loss
*   역전파 후 그래디언트:
    *   파라미터: w1.grad, w2.grad, b.grad
    *   (선택 사항) 입력: x1.grad, x2.grad

**제출물**
*   **코드**: `gate_graph.py`
*   **보고서 (PDF)**: 각 게이트의 `backward`가 어떻게 그래디언트를 부모에게 보내는지에 대한 간략한 설명, 그리고 출력된 순전파 값과 그래디언트 (로그를 붙여넣거나 작은 표로 작성).

***

### **페이지 798**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

**(코드: 게이트 기반 역전파 구현을 위한 스켈레톤 코드. Node, Gate, AddGate, MulGate, SigmoidGate, CrossEntropyLossGate 클래스의 기본 구조와 테스트를 위한 main 함수 일부가 포함됨)**

***

### **페이지 799**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

*   **목표**
    *   이전에 구현한 동일한 **노드 + 게이트 클래스**(AddGate, MulGate, SigmoidGate, CrossEntropyLossGate)를 사용하여, 작은 **2계층 신경망**(입력→은닉→출력)을 스칼라 계산 그래프로 구축하고, **손실로부터 역전파**를 실행하고, 순전파 값과 그래디언트를 **출력**합니다.
*   **네트워크 사양**
    *   **아키텍처**: 2 → 2 → 1 (은닉 레이어는 2개의 유닛; 은닉 및 출력 모두 **시그모이드** 사용)
    *   **손실**: **이진 교차 엔트로피** (확률 입력)
    *   **단일 고정 샘플** (재현성):
        *   (주어진 입력, 가중치, 편향, 레이블 값)

***

### **페이지 800**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

**구현할 내용**
**1. 게이트를 사용하여 순전파 그래프 구축 (스칼라 스타일):**
1.  은닉 유닛 1: 두 스칼라 곱의 합 + 편향 → 시그모이드
2.  은닉 유닛 2: 동일
3.  출력: (h1, h2)의 가중 합 + 편향 → 시그모이드
4.  손실: CE(ŷ, y)
**2. `loss.backward(1.0)`을 호출하여 그래프를 통해 역전파.**
**3. 결과 출력:**
1.  순전파: h1_z, h1, h2_z, h2, s2, y_hat, loss
2.  그래디언트 (파라미터): dW1_11, dW1_12, dW1_21, dW1_22, db1_1, db1_2, dW2_1, dW2_2, db2
3.  (선택 사항) 입력: dx1, dx2

**규칙**
*   순수 Python; `exp`, `log`를 위해 `math`를 사용할 수 있습니다.
*   PyTorch/JAX/TF 또는 자동 미분을 사용하지 **마세요**.
*   타입 힌트 없음.
*   노드가 여러 하류 기여를 받을 때 그래디언트는 `+=`로 누적되어야 합니다.

**제출물**
*   `two_layer_scalar.py` (전체 구현)
*   `report.pdf` 내용:
    *   게이트가 어떻게 부모에게 그래디언트를 전파하는지에 대한 짧은 단락 설명
    *   출력된 순전파 값과 그래디언트 (로그 복사 또는 작은 표)

***

### **페이지 801**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

**메인 함수 (부분; 이것만 보여주세요)**

**(코드: 2계층 신경망의 계산 그래프를 스칼라 연산으로 구축하는 과정을 보여주는 Python 코드의 일부. 학생이 나머지 부분을 완성하도록 지시)**

***

### **페이지 802**

**4. 2계층 신경망 구축하기 (벡터 FC + ReLU + 시그모이드 + 교차 엔트로피)**

**목표**
자동 미분 엔진을 확장하여 **2계층 신경망**을 구현합니다:
**입력 → FC → ReLU → FC → 시그모이드 → 이진 교차 엔트로피 손실**
순전파/역전파는 **자신만의 노드/게이트 시스템만**을 사용하여 수행해야 합니다.
(PyTorch/JAX/TensorFlow 자동 미분 없음).

**아키텍처**
*   입력: 벡터 x ∈ ℝ³
*   은닉: 벡터 (크기 = 2)
*   출력: 스칼라 (이진 분류)

**제한 사항**
*   타입 힌트 없음
*   내장 자동 미분 사용 **금지**
*   기본 수학 연산에 `numpy` 또는 `math`를 사용할 수 있습니다.

***

### **페이지 803**

**4. 2계층 신경망 구축하기 (벡터 FC + ReLU + 시그모이드 + 교차 엔트로피)**

**과제**
**1. 다음 게이트들을 구현:**
1.  MatMulGate (행렬 × 벡터)
2.  AddVecGate
3.  ReLUVecGate (요소별)
4.  SigmoidVecGate (마지막 레이어 전용; 스칼라 반환)
5.  BinaryCrossEntropyGate (스칼라 출력)
**2. 2계층 NN에 대한 계산 그래프 구축**
**3. `loss.backward(1.0)` 실행**
**4. 출력:**
1.  순전파 값: h_pre, h, logit, y_hat, loss
2.  그래디언트: dW1, db1, dW2, db2, dx

**(주어진 입력, 가중치, 편향, 레이블 값)**

**제출물**
*   **코드 파일**: `two_layer_relu_sigmoid.py`
*   **PDF 보고서 내용**:
    *   각 게이트의 역전파 규칙에 대한 간략한 설명
    *   순전파 및 그래디언트 출력

***

### **페이지 804**

**코드 스켈레톤**

**(코드: 벡터 연산을 위한 게이트 기반 역전파 구현을 위한 전체 스켈레톤 코드. Node, Gate, MatMulGate, AddVecGate, ReLUVecGate, SigmoidGate, BinaryCrossEntropyGate 클래스의 기본 구조와 테스트를 위한 main 함수 일부가 포함됨)**

***

### **페이지 805**

**과제 — 수동 2계층 신경망을 MNIST에서 훈련시키기 (작은 데모)**

**목표**
자신이 만든 자동 미분 엔진을 사용하여:
*   **1개의 MNIST 샘플** 로드 (예: 레이블 = 5)
*   **훈련 루프** 실행 (50~200회 반복)
*   순전파 → 역전파 → 수동 가중치 업데이트 수행
*   **출력:**
    *   시간에 따른 손실 감소
    *   최종 예측 확률
    *   예측된 클래스 vs 실제 클래스
이것은 당신의 **수작업 자동 미분**이 실제 훈련에 작동함을 보여줍니다.

**네트워크 아키텍처**
입력: 784차원 벡터 (28×28)
은닉: 10 유닛 (ReLU)
출력: 1 유닛 (시그모이드) → 이진 분류기: "이것이 5인가?"
손실: 이진 교차 엔트로피
하나의 숫자 대 나머지만 분류합니다.
레이블 = 1 (숫자가 5인 경우), 그렇지 않으면 0.

***

### **페이지 806**

**과제 — 수동 2계층 신경망을 MNIST에서 훈련시키기 (작은 데모)**

**데이터셋**

**(코드: MNIST 데이터셋에서 하나의 샘플을 선택하고, 이를 이진 분류 문제("이것이 5인가?")에 맞게 전처리하는 Python 코드)**

**제한 사항**
*   PyTorch 자동 미분 사용 **금지**
*   sklearn / numpy 훈련 함수 사용 **금지**
*   자신의 **노드 + 게이트 + backward()** 시스템을 사용해야 합니다.
*   허용된 수학 연산: 원시 배열에 대한 numpy

***

### **페이지 807**

**과제 — 수동 2계층 신경망을 MNIST에서 훈련시키기 (작은 데모)**

**훈련 루프 의사코드**

**(코드: 수동으로 구현한 게이트를 사용하여 MNIST 샘플 하나를 훈련시키는 전체 과정을 보여주는 의사코드)**

***

### **페이지 808**

**과제 — 수동 2계층 신경망을 MNIST에서 훈련시키기 (작은 데모)**

**요구되는 출력**
**끝에 출력:**
실제 레이블: 5
예측 확률: 0.9321
예측 클래스: 1 (모델은 5라고 생각함)

**그리고 훈련 중, 손실 감소를 보여주기:**
스텝 0: loss = 0.71
스텝 20: loss = 0.54
스텝 40: loss = 0.38
스텝 60: loss = 0.25
...

**정확한 숫자: 3**
**최종 예측: 3**
단일 샘플 훈련으로도, 예측은 실제 숫자를 향해 움직여야 합니다.

**제출물**
*   **코드 파일** `mnist_manual_train.py`
*   **스크린샷 또는 텍스트 로그** 내용:
    *   감소하는 손실
    *   최종 예측 확률
    *   최종 예측 클래스
*   **3문장 설명:**
    *   그래프가 어떻게 구축되었는지
    *   그래디언트가 어떻게 흐르는지
    *   왜 프레임워크 자동 미분이 일반적으로 필요한지

***

### **페이지 809**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**목표**
2계층 자동 미분 네트워크를 확장하여 MNIST 숫자 0~9를 분류하도록 합니다:
**입력(784) → 은닉(10) → 출력(10)**
**ReLU → 소프트맥스 → 교차 엔트로피**
그리고 **하나의 MNIST 이미지**에 대해 반복적으로 훈련하여 예측된 클래스 확률이 실제 숫자로 이동하는지 확인합니다.

**네트워크 사양**
(수식)

**여기서:**
*   y = 실제 숫자에 대한 원-핫 벡터 (길이 10)
*   ŷ = 소프트맥스 확률 (길이 10)

**구현해야 할 것**
이 게이트 유형들을 추가합니다:

| 게이트 | 기능 |
| :--- | :--- |
| SoftmaxGate | 벡터 입력 → 벡터 확률 |
| CrossEntropyGate | 소프트맥스 출력 + 원-핫 → 스칼라 손실 |

내장된 딥러닝 라이브러리를 사용해서는 **안 됩니다**.

**(수식: 소프트맥스 및 교차 엔트로피 공식)**

***

### **페이지 810**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**훈련 루프 의사코드**
```
하나의 MNIST 이미지 x와 레이블 y_int 로드
y_onehot = one_hot(y_int, dim=10)

x_node = Node(x)
y_node = Node(y_onehot)

W1,b1,W2,b2 초기화

for step in range(num_steps):

 # 순전파
 h_pre = W1 @ x + b1
 h = ReLU(h_pre)
 logits = W2 @ h + b2
 y_hat = Softmax(logits)
 loss = CrossEntropy(y_hat, y_onehot)

 # 역전파
 loss.backward(1.0)

 # SGD 업데이트
 SGD로 W1,b1,W2,b2 업데이트
 모든 그래디언트 초기화

 if step % 20 == 0:
 print(step, loss, predicted_class, max_prob)
```

***

### **페이지 811**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**예상 학생 출력**
**예시 출력 스타일:**
```
스텝 0: loss=2.31, pred=7, prob=0.11
스텝 20: loss=1.85, pred=7, prob=0.18
스텝 40: loss=1.14, pred=3, prob=0.40
스텝 80: loss=0.52, pred=3, prob=0.75
...
```
**정확한 숫자: 3**
**최종 예측: 3**
단일 샘플 훈련으로도, 예측은 실제 숫자를 향해 움직여야 합니다.

**제출물**
*   **코드:** `mnist_softmax_manual.py`
*   **콘솔 로그 또는 스크린샷 내용:**
    *   감소하는 손실
    *   증가하는 예측 클래스 확률
*   **짧은 설명 (3–5 문장):**
    *   소프트맥스 역할
    *   왜 교차 엔트로피가 소프트맥스와 잘 맞는지
    *   이진 분류 사례에서 무엇이 변경되었는지

***

### **페이지 812**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**목표**
자신만의 자동 미분 엔진을 사용하여 MNIST에 대한 **완전한 훈련 파이프라인**을 구축할 것입니다:
*   데이터를 **훈련 / 검증 / 테스트**로 분할
*   2계층 (또는 더 깊은) 신경망 훈련
*   **다음으로 실험:**
    *   **완전 연결 레이어 수**
    *   **은닉 레이어 너비 (노드)**
    *   **L2 정규화 강도 λ**
*   분류 **정확도** 평가
*   결과 분석 및 토론
**PyTorch / TensorFlow 자동 미분 사용 금지.**
MNIST를 원시 데이터 소스로만 사용하세요. 모든 순전파/역전파는 여러분의 코드여야 합니다.

**과제**
**1. MNIST 로드 및 전처리**
*   각 이미지를 784차원 벡터로 펼치기
*   픽셀 값을로 정규화
*   **다음으로 분할:**
    train: 1,000 ~ 50,000 샘플 (부분 집합 가능)
    val: 100 ~ 10,000 샘플 (부분 집합 가능)
    test: 100 ~ 10,000 샘플 (부분 집합 가능)

sklearn 유틸리티는 훈련이 아닌 **분할에만** 사용할 수 있습니다.

***

### **페이지 813**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**2. 모델 변형 구축**
**(a) 기본 모델**
784 → H → 10
ReLU → Softmax

**(b) 레이어 수 변경**
예시:
784 → H → 10
784 → H → H → 10
784 → H → H → H → 10

**(c) 은닉 유닛 H 변경**
제안된 값:
H ∈ {10, 50, 100, 200}

**3. L2 가중치 정규화 게이트 구현**
`L2RegularizationGate` 추가:
L_reg = λ Σᵢ ||Wᵢ||²

**총 손실:**
L = L_crossentropy + Σ_layers L_reg

*   **손실 및 정규화 항을 통한 역전파**
*   **λ 값 스윕:**
    λ ∈ {0, 0.001, 0.01, 0.1}

***

### **페이지 814**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**4. 훈련 루프**
**각 구성에 대해:**
```
for epoch in 1..E:
    각 훈련 미니배치에 대해 순전파
    역전파
    SGD 업데이트
검증 세트에서 평가
```
**하이퍼파라미터 (권장):**
미니 배치 크기 = 1 (또는 64 또는 128)
에포크 = 3–5 (최소)
학습률 = 0.1 (또는 조정)
**훈련 데이터를 섞을 수 있습니다**
**원한다면 검증 세트 최고 모델에서 중지 (조기 종료 선택 사항)**

**5. 평가**
**훈련 후:**
*   테스트 세트에서 **정확도 계산**
*   혼동 행렬 출력 (선택 사항)
*   손실 곡선 및 검증 정확도 곡선 플롯 (선택 사항)

***

### **페이지 815**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**출력 샘플 (예시)**
```
=== RUN {'depth': 2, 'hidden_dim': 64, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.3652 val_acc=0.834
[epoch 2] train_loss/step_avg=0.8717 val_acc=0.871
[TEST] depth=2, hidden=64, lambda=0.0 --> acc=0.862

=== RUN {'depth': 2, 'hidden_dim': 64, 'lam': 0.01, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.4229 val_acc=0.842
[epoch 2] train_loss/step_avg=0.9381 val_acc=0.878
[TEST] depth=2, hidden=64, lambda=0.01 --> acc=0.872

=== RUN {'depth': 3, 'hidden_dim': 64, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.5023 val_acc=0.812
[epoch 2] train_loss/step_avg=0.9546 val_acc=0.861
[TEST] depth=3, hidden=64, lambda=0.0 --> acc=0.853

=== RUN {'depth': 2, 'hidden_dim': 128, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.2998 val_acc=0.846
[epoch 2] train_loss/step_avg=0.8135 val_acc=0.882
[TEST] depth=2, hidden=128, lambda=0.0 --> acc=0.879
```

***

### **페이지 816**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**6. 보고서 필수 사항**

| 섹션 | 설명 |
| :--- | :--- |
| 실험 설정 | 아키텍처, 하이퍼파라미터, 데이터셋 분할 |
| 결과 표 | 각 구성에 대한 정확도 |
| 손실 곡선 | 훈련 손실, 검증 손실 |
| 모델 분석 | 미적합 / 과적합 경향 |
| 정규화 효과 | λ 스윕 결과 |
| 최종 테스트 정확도 | 최고 모델로부터 |

**토론 프롬프트**
**보고서에 답변:**
1.  어떤 아키텍처가 가장 좋은 성능을 보였나요? 왜 그런가요?
2.  너비를 늘리는 것이 성능에 어떤 영향을 미치나요?
3.  깊이가 학습에 어떤 영향을 미치나요?
4.  λ가 너무 크거나 너무 작으면 어떻게 되나요?
5.  정규화 없이 과적합의 증거가 있나요?
6.  검증 정확도가 테스트 정확도와 상관관계가 있었나요?

**제출물**
*   **코드:** `mnist_full_manual_train.py`
*   **스크린샷/로그 내용:**
    *   에포크별 정확도 및 손실
    *   최종 테스트 정확도
*   **PDF 보고서 (2–5 페이지):**
    *   실험 설정
    *   표 및 그래프
    *   분석 및 토론

***

### **페이지 817**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 818**

**프로그래밍 과제 08: 실습: CNN 구현하기**

*   **목표:**
    *   이 과제에서는 **합성곱 신경망(CNN)을 처음부터 구현**하는 것을 연습할 것입니다.
    *   학생들은 합성곱, 풀링, 완전 연결 레이어가 어떻게 함께 작동하여 이미지를 분류하는지 배우게 됩니다.
*   **목표:**
    *   CNN의 구조와 순전파 과정을 이해합니다.
    *   NumPy만을 사용하여 합성곱 및 풀링 연산을 구현합니다.
    *   간단한 데이터셋(예: MNIST 또는 CIFAR-10)에서 모델을 훈련하고 평가합니다.

***

### **페이지 819**

**처음부터 CNN 구축하기 (MNIST)**

*   **1) 개요**
    *   딥러닝 프레임워크 없이 **NumPy만**을 사용하여 **합성곱 신경망(CNN)을 처음부터 구현**할 것입니다. 핵심 모듈(합성곱, 활성화, 풀링, 완전 연결, 손실, 옵티마이저)을 구축하고, 이를 모델로 쌓아 MNIST에서 훈련하고 결과를 분석합니다. 주요 목표는 속도보다는 **메커니즘(순전파 및 역전파)을 이해**하는 것입니다.
*   **2) 학습 목표**
    *   올바른 순전파/역전파를 가진 **Conv2D, ReLU, MaxPool2D, Flatten, Linear** 레이어를 구현합니다.
    *   **소프트맥스 교차 엔트로피** 손실과 간단한 옵티마이저(예: SGD)를 구현합니다.
    *   레이어들을 CNN으로 구성하고 MNIST에서 **훈련/평가**합니다.
    *   설정(필터, 커널 크기, 풀링, 학습률, 가중치 감쇠 등)을 변경하여 작은 **실험**을 실행하고 결과를 해석합니다.
*   **3) 데이터셋**
    *   **MNIST**와 **CIFAR-10**을 모두 사용하여 회색조 및 컬러 이미지에 대한 CNN 동작을 탐색합니다.

| 데이터셋 | 채널 | 이미지 크기 | 클래스 | 비고 |
| :--- | :--- | :--- | :--- | :--- |
| **MNIST** | 1 (회색조) | 28×28 | 10개 숫자 | 간단함; 디버깅에 좋음 |
| **CIFAR-10** | 3 (RGB 컬러) | 32×32 | 10개 객체 카테고리 | 컬러 + 더 복잡함 |

***

### **페이지 820**

**처음부터 CNN 구축하기 (MNIST)**

**4) 구현해야 할 것 (NumPy 전용)**
각각에 대해 **순전파**와 **역전파**를 구현합니다:
**1. Conv2D**
1.  입력: (N, C_in, H, W) → 출력: (N, C_out, H', W')
2.  하이퍼파라미터: `out_channels`, `kernel_size` (k×k), `stride=1`, `padding` ('same' = k//2 사용 가능).
3.  명확성을 위해 순진한 루프를 허용합니다. (선택 사항: 효율성을 위해 im2col 사용).
**2. 활성화**
1.  ReLU (필수). 선택적으로 Sigmoid/Tanh를 추가하여 비교.
**3. MaxPool2D**
1.  일반적: `kernel_size=2`, `stride=2`.
2.  역전파는 최대값 위치에만 그래디언트를 반환해야 합니다.
**4. Flatten**
**5. Linear (완전 연결)**
**6. 소프트맥스 교차 엔트로피 손실**
1.  수치적으로 안정적인 소프트맥스 (샘플별 최대값 빼기).
2.  배치에 대한 평균 손실과 로짓에 대한 그래디언트를 반환.
**7. 옵티마이저**
1.  학습률 η를 사용한 SGD.
2.  **가중치에만** 가중치 감쇠(L2)를 구현합니다 (편향에는 감쇠 없음).
**중요**: **그래디언트 계산(역전파)**을 **파라미터 업데이트(옵티마이저)**와 분리하여 유지하세요. `backward()` 내부에서 업데이트하지 마세요.

**5) 모델 구성 (기준선)**
작은 CNN을 구축합니다; 설정을 변경할 수 있습니다:
**기준선 예시**
Conv2D(1→8, k=3, padding='same') → ReLU → MaxPool2D(2)
Conv2D(8→16, k=3, padding='same') → ReLU → MaxPool2D(2)
Flatten → Linear(16*7*7 → 64) → ReLU → Linear(64 → 10)
*   `out_channels`, k를 변경하거나, 블록을 추가/제거하거나, 은닉 크기를 조정할 수 있지만 — **선택 사항을 문서화**하세요.

***

### **페이지 821**

**처음부터 CNN 구축하기 (MNIST)**

**6) 훈련 요구 사항**
*   **배칭**: 미니배치 SGD (예: 배치 크기 64 또는 128).
*   **에포크**: 수렴할 때까지 훈련 (예: 이 작은 네트워크에는 5–15 에포크가 일반적임).
*   **메트릭**: **에포크별 훈련 손실/정확도**와 **최종 테스트 정확도**를 보고합니다.
*   **목표**: 합리적인 CNN으로, **≥97% 테스트 정확도**가 예상됩니다. (더 낮으면 이유를 분석하세요.)
*   **결정성**: 재현성을 위해 랜덤 시드를 설정합니다.

**7) 실험 (설정 다양화)**
다음 중 **최소 두 가지**를 수행하고 결과(정확도, 손실 곡선, 훈련 행동)를 논의합니다:
*   Conv 레이어의 **#필터** 변경 (예: 8/16 vs 16/32).
*   **커널 크기** 변경 (3 vs 5).
*   **MaxPool 유/무** 시도 (또는 풀링 하나 vs 두 개).
*   **학습률** 변경 (예: 1e-1, 1e-2, 1e-3).
*   **가중치 감쇠** 추가 및 일반화 비교.
*   활성화 교체/삽입 (예: Tanh) 및 수렴 비교.

**8) 제출물**
다음을 포함해야 합니다:
**1. 코드** (수정 없이 실행 가능)
**2. 보고서**
1.  **모델 아키텍처** (작은 블록 다이어그램 또는 레이어 표).
2.  **훈련 곡선** (에포크별 손실/정확도).
3.  **최종 테스트 정확도** 및 혼동 행렬 또는 몇 가지 잘못 분류된 예시.
4.  **실험** (무엇을 변경했는지, 결과, 해석).
5.  **구현 노트** (설계 선택, 수치적 트릭, 수정된 버그).
6.  **재현성** (환경, 시드, 하이퍼파라미터).

***

### **페이지 822**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**1) PyTorch란 무엇인가?**
*   **PyTorch**는 다음과 같은 기능을 갖춘 Python **딥러닝 프레임워크**입니다:
    *   **텐서** (NumPy와 유사, CPU/GPU에서 실행)
    *   **자동 미분** (Autograd)
    *   **nn.Module & optim** (레이어, 손실, 옵티마이저)
    *   **torchvision** (데이터셋, 변환, 사전 훈련된 모델)
**왜 지금 사용하는가?** 여러분은 순전파/역전파를 직접 구축했습니다; PyTorch를 사용하면 다음을 할 수 있습니다:
*   그래디언트 배관이 아닌 **모델링 및 실험에 집중**
*   **더 큰 모델과 GPU로 확장**
*   표준 도구 사용 (DataLoader, 체크포인트, AMP 등)

**2) 설치 (하나의 경로 선택)**
**Conda (권장)**
```bash
# CPU 전용
conda create -n dl python=3.10 -y
conda activate dl
conda install pytorch torchvision torchaudio cpuonly -c pytorch -y

# 또는: CUDA (호환되는 NVIDIA GPU + 드라이버가 있는 경우)
conda create -n dl python=3.10 -y
conda activate dl
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```
**pip**
```bash
# CPU 전용
python -m venv dl && source dl/bin/activate # (Windows: dl\Scripts\activate)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# CUDA (시스템에서 지원되는 경우)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

***

### **페이지 823**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**3) 과제 개요**
이전 NumPy CNN을 **PyTorch**로 다시 구현한 다음, **MNIST**와 **CIFAR-10**에서 훈련/평가합니다.
**반드시 포함:**
*   **Conv → ReLU → Pool → … → Linear**를 사용한 모델 (합리적인 변형 OK)
*   **교차 엔트로피 손실, SGD 또는 Adam**
*   **DataLoaders, 변환, 훈련/검증/테스트** 루프
*   두 데이터셋 모두에 대한 **정확도 및 손실 곡선; 최종 테스트 정확도**
*   **간단한 비교**: MNIST vs CIFAR-10 (채널, 난이도, 수렴)

**4) 기준 아키텍처**
**MNIST (1×28×28)**
Conv(1→16, 3×3, padding=1) → ReLU → MaxPool(2)
Conv(16→32, 3×3, padding=1) → ReLU → MaxPool(2)
Flatten → Linear(32*7*7→128) → ReLU → Linear(128→10)
**CIFAR-10 (3×32×32)**
Conv(3→32, 3×3, padding=1) → ReLU → MaxPool(2)
Conv(32→64, 3×3, padding=1) → ReLU → MaxPool(2)
Flatten → Linear(64*8*8→256) → ReLU → Linear(256→10)
훈련이 너무 쉬우면 CIFAR-10의 채널을 늘릴 수 있습니다 (예: 32/64 → 64/128).

**5) 시작 코드 (PyTorch)**
`train_pytorch_cnn.py`로 저장하고 실행:
```bash
python train_pytorch_cnn.py --dataset mnist --epochs 8 --batch_size 128 --lr 0.01
python train_pytorch_cnn.py --dataset cifar10 --epochs 20 --batch_size 128 --lr 0.001
```

***

### **페이지 824**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: PyTorch로 간단한 CNN 모델(SmallCNN)을 정의하는 Python 코드. 입력 채널 수에 따라 다른 구조를 가지며, 분류기 부분은 첫 순전파 시 동적으로 생성됨)**

***

### **페이지 825**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: MNIST와 CIFAR-10 데이터셋에 대한 데이터 로더와 변환을 정의하는 Python 함수. 데이터셋에 따라 다른 정규화 값과 데이터 증강(CIFAR-10의 경우)을 적용함)**

***

### **페이지 826**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: PyTorch 모델을 위한 훈련(train_one_epoch) 및 평가(evaluate) 함수. 훈련 함수는 그래디언트 계산 및 파라미터 업데이트를 포함하며, 평가 함수는 그래디언트 계산 없이 실행됨)**

***

### **페이지 827**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: 명령줄 인자(데이터셋, 에포크, 배치 크기 등)를 파싱하고, 모델, 데이터 로더, 옵티마이저, 손실 함수를 설정한 후, 훈련 및 평가 루프를 실행하는 메인 Python 스크립트)**

***

### **페이지 828**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**6) 요구 사항 및 실험**
**필수**
*   **두 데이터셋 모두** 훈련 (MNIST & CIFAR-10).
*   에포크별 **훈련 손실/정확도**와 **테스트 정확도**를 기록.
*   **재현 가능한 시드** 사용.
*   두 데이터셋 모두에 대한 **손실/정확도 vs 에포크 플롯** 제출.

**실험 (≥2개 선택)**
*   **옵티마이저 교체** (SGD vs Adam).
*   **#필터** 또는 **은닉 크기** 변경.
*   **LR 값** 시도 (예: 1e-1/1e-2/1e-3) 및 비교.
*   CIFAR-10에 **데이터 증강** 유/무.
*   **드롭아웃** 또는 **배치 정규화** 추가 및 효과 논의.

**7) 제출물**
**1. 코드** (실행 가능): `train_pytorch_cnn.py` (및 모든 헬퍼)
**2. 보고서:**
1.  모델 다이어그램 (MNIST/CIFAR-10)
2.  훈련 곡선, 최종 정확도
3.  실험 결과 (≥2)
4.  간단한 MNIST vs CIFAR-10 비교 (채널, 난이도, 수렴)
5.  재현성 세부 정보 (환경, 시드, 하이퍼파라미터)

***

### **페이지 829**

**특징 맵 시각화**

**목표**
**CNN 필터가 무엇을 탐지하는지** 이해하기 위해, 중간 합성곱 레이어의 **특징 맵(활성화 출력)**을 시각화합니다.

**개념**
*   각 **합성곱 필터**는 특정 **패턴**(엣지, 질감, 색상, 모양 등)을 탐지하도록 학습합니다.
*   훈련 후, **활성화 맵**은 입력 이미지의 **어떤 영역**이 각 필터를 활성화하는지 보여줍니다.
*   초기 레이어 → 저수준 특징 (엣지, 블롭)
    나중 레이어 → 고수준 추상적 모양 (숫자, 객체 부분)

**과제 지침**
**1. 훈련된 모델 하나 선택**
1.  훈련된 PyTorch CNN (MNIST 또는 CIFAR-10)을 사용합니다.
2.  저장된 가중치를 로드하거나 훈련 직후에 시각화합니다.
**2. 하나 또는 두 개의 입력 이미지 선택**
1.  MNIST의 경우: 숫자 하나 (예: "5")
2.  CIFAR-10의 경우: 샘플 하나 (예: "고양이")
**3. 순전파만** 이미지를 통과시키고, 초기 Conv 레이어에서 출력을 추출합니다.

**(코드: 훈련된 모델의 첫 번째 합성곱 레이어 이후의 특징 맵을 추출하는 PyTorch 코드)**

***

### **페이지 830**

**특징 맵 시각화**

**4. 시각화**

**(코드: Matplotlib을 사용하여 추출된 특징 맵(활성화) 중 일부를 시각화하는 Python 코드)**

**5. 해석**
*   어떤 패턴이 보이는지 설명합니다 (예: 수직 엣지, 수평 엣지, 블롭 영역).
*   MNIST vs CIFAR-10 특징 맵 비교:
    *   "MNIST 필터는 획 엣지를 강조하고; CIFAR-10 필터는 색상 블롭과 질감을 포착합니다."

**6. (선택 사항 고급) 더 깊은 레이어 활성화 시각화 (2번째 conv).**
특징 맵이 어떻게 더 **추상적**이거나 **지역화**되는지 비교합니다.

**7. 보고서 제출물 (보고서에 추가)**
**포함 내용:**
*   사용한 입력 이미지.
*   최소 하나의 Conv 레이어의 특징 맵 시각화.
*   필터가 무엇을 탐지하는 것처럼 보이는지에 대한 3–5 문장 해석.
*   (선택 사항) MNIST vs CIFAR-10 시각적 패턴 비교.

***

### **페이지 831**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 832**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **목표**
    *   이 과제에서는 매우 작은 신경망을 처음부터 구현하고 **수동으로 계산**할 것입니다:
        *   순전파
        *   역전파 (그래디언트)
        *   경사 하강법 업데이트
        *   여러 반복에 대한 훈련
        *   시간에 따른 손실 및 예측 진화
*   이 과제는 딥러닝이 실제로 어떻게 그래디언트를 계산하고 내부적으로 가중치를 학습하는지 이해하는 데 도움이 될 것입니다.

***

### **페이지 833**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **네트워크 아키텍처**
    *   고정된 2계층 신경망:

| 레이어 | 모양 | 활성화 |
| :--- | :--- | :--- |
| 입력 | 2 | — |
| 은닉 | 2 | 시그모이드 |
| 출력 | 1 | 시그모이드 |

*   **주어진 값**
    *   입력 샘플: x =ᵀ
    *   레이블: y = 1
    *   재현성을 위해 `np.random.seed(0)` 사용
*   **제한 사항**
    *   **Python + NumPy**만 허용됩니다.

***

### **페이지 834**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **과제**
    *   1. 파라미터 무작위 초기화
    *   2. 순전파
    *   3. 손실 (이진 교차 엔트로피)
    *   4. 역전파
        *   연쇄 법칙을 사용하여 수동으로 그래디언트 계산 (자동 미분 없음).
    *   5. 경사 하강법 훈련
        *   **200회 반복** 동안 반복:
            *   순전파
            *   역전파
            *   그래디언트 업데이트
            *   학습률 η = 0.1
    *   6. 학습 진행 추적
        *   값이 어떻게 변하는지 기록하고 보여주기:
            *   감소하는 손실
            *   목표 y = 1에 접근하는 ŷ
        *   원한다면 10 스텝마다 (또는 더 자주) 출력.
    *   7. 최종 분석
        *   **비교:**
            *   초기 손실 vs 최종 손실
            *   초기 예측 vs 최종 예측
        *   네트워크가 학습했는지 여부와 그 이유를 설명.

***

### **페이지 835**

**프로그래밍 과제 07: 역전파 및 정규화**

**보고서 내용**
PDF 보고서에는 다음이 포함되어야 합니다:

**모델 및 수학**
*   순전파 방정식에 대한 간략한 설명
*   역전파 및 그래디언트에 대한 간략한 설명

**코드 스크린샷**
*   주요 구현 섹션을 보여주세요
    *   순전파
    *   역전파
    *   경사 하강법 루프

**출력 및 학습 로그**
*   훈련 과정을 보여주는 표 또는 로그
    (각 체크포인트에서의 손실 + 예측)
*   최종 출력된 값
*   손실 곡선 (선 그래프 강력 추천)
*   예측 곡선 (선택 사항이지만 권장)

**해석**
*   손실이 감소했나요? (설명)
*   ŷ가 1에 가까워졌나요? (설명)
*   역전파를 수동으로 구현하면서 무엇을 배웠나요?

필수 스크린샷, 로그 또는 설명이 누락된 보고서는 감점됩니다.

***

### **페이지 836**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

*   데이터(**노드**)와 연산(**게이트**)을 분리하는 작은 계산 그래프 엔진을 구축합니다. 네 가지 게이트를 구현합니다:
    *   AddGate / MulGate / SigmoidGate / CrossEntropyLossGate (이진)
*   손실로부터 **역전파**하고 모든 필요한 그래디언트를 출력합니다.

**구현해야 할 것**
**핵심 클래스**
*   **노드**
    *   필드: `value`, `grad`, `op`, `parents`
    *   메서드: `backward(upstream_grad=1.0)`
        *   `self.grad`에 누적
        *   `self.op`가 존재하면, `self.op.backward(self, upstream_grad)` 호출 (리프 노드에 도달할 때까지 재귀적으로)
*   **게이트 (기본 클래스)**
    *   `forward(*parents)` → `value`, `op=self`, `parents=(...)`를 가진 새로운 **노드**를 반환
    *   `backward(out_node, upstream_grad)` → `out_node.parents`를 읽고, 지역 기여도를 계산하고, 각 부모의 `.backward(...)`를 호출
*   **AddGate, MulGate, SigmoidGate, CrossEntropyLossGate**
    *   각각 올바른 `forward(...)`와 `backward(...)`를 제공해야 합니다.
    *   CrossEntropyLossGate의 경우, 레이블 y를 상수(그래디언트 없음)로 취급합니다. 안정성을 위해 내부적으로 작은 `eps=1e-12`를 사용합니다.

***

### **페이지 837**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

**예제 테스트 값**
*   입력: x1 = -1.0, x2 = -2.0
*   파라미터: w1 = 2.0, w2 = -3.0, b = -3.0
*   레이블: y = 1 (이진)

**구축할 그래프**
1.  s1 = w1 * x1
2.  s2 = w2 * x2
3.  s3 = s1 + s2 + b
4.  y_hat = sigmoid(s3)
5.  loss = cross_entropy(y_hat, y)
6.  `loss.backward(1.0)` 호출

**출력할 내용**
*   순전파 값: s1, s2, s3, y_hat, loss
*   역전파 후 그래디언트:
    *   파라미터: w1.grad, w2.grad, b.grad
    *   (선택 사항) 입력: x1.grad, x2.grad

**제출물**
*   **코드**: `gate_graph.py`
*   **보고서 (PDF)**: 각 게이트의 `backward`가 어떻게 그래디언트를 부모에게 보내는지에 대한 간략한 설명, 그리고 출력된 순전파 값과 그래디언트 (로그를 붙여넣거나 작은 표로 작성).

***

### **페이지 838**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

**(코드: 게이트 기반 역전파 구현을 위한 스켈레톤 코드. Node, Gate, AddGate, MulGate, SigmoidGate, CrossEntropyLossGate 클래스의 기본 구조와 테스트를 위한 main 함수 일부가 포함됨)**

***

### **페이지 839**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

*   **목표**
    *   이전에 구현한 동일한 **노드 + 게이트 클래스**(AddGate, MulGate, SigmoidGate, CrossEntropyLossGate)를 사용하여, 작은 **2계층 신경망**(입력→은닉→출력)을 스칼라 계산 그래프로 구축하고, **손실로부터 역전파**를 실행하고, 순전파 값과 그래디언트를 **출력**합니다.
*   **네트워크 사양**
    *   **아키텍처**: 2 → 2 → 1 (은닉 레이어는 2개의 유닛; 은닉 및 출력 모두 **시그모이드** 사용)
    *   **손실**: **이진 교차 엔트로피** (확률 입력)
    *   **단일 고정 샘플** (재현성):
        *   (주어진 입력, 가중치, 편향, 레이블 값)

***

### **페이지 840**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

**구현할 내용**
**1. 게이트를 사용하여 순전파 그래프 구축 (스칼라 스타일):**
1.  은닉 유닛 1: 두 스칼라 곱의 합 + 편향 → 시그모이드
2.  은닉 유닛 2: 동일
3.  출력: (h1, h2)의 가중 합 + 편향 → 시그모이드
4.  손실: CE(ŷ, y)
**2. `loss.backward(1.0)`을 호출하여 그래프를 통해 역전파.**
**3. 결과 출력:**
1.  순전파: h1_z, h1, h2_z, h2, s2, y_hat, loss
2.  그래디언트 (파라미터): dW1_11, dW1_12, dW1_21, dW1_22, db1_1, db1_2, dW2_1, dW2_2, db2
3.  (선택 사항) 입력: dx1, dx2

**규칙**
*   순수 Python; `exp`, `log`를 위해 `math`를 사용할 수 있습니다.
*   PyTorch/JAX/TF 또는 자동 미분을 사용하지 **마세요**.
*   타입 힌트 없음.
*   노드가 여러 하류 기여를 받을 때 그래디언트는 `+=`로 누적되어야 합니다.

**제출물**
*   `two_layer_scalar.py` (전체 구현)
*   `report.pdf` 내용:
    *   게이트가 어떻게 부모에게 그래디언트를 전파하는지에 대한 짧은 단락 설명
    *   출력된 순전파 값과 그래디언트 (로그 복사 또는 작은 표)

***

### **페이지 841**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

**메인 함수 (부분; 이것만 보여주세요)**

**(코드: 2계층 신경망의 계산 그래프를 스칼라 연산으로 구축하는 과정을 보여주는 Python 코드의 일부. 학생이 나머지 부분을 완성하도록 지시)**

***

### **페이지 842**

**4. 2계층 신경망 구축하기 (벡터 FC + ReLU + 시그모이드 + 교차 엔트로피)**

**목표**
자동 미분 엔진을 확장하여 **2계층 신경망**을 구현합니다:
**입력 → FC → ReLU → FC → 시그모이드 → 이진 교차 엔트로피 손실**
순전파/역전파는 **자신만의 노드/게이트 시스템만**을 사용하여 수행해야 합니다.
(PyTorch/JAX/TensorFlow 자동 미분 없음).

**아키텍처**
*   입력: 벡터 x ∈ ℝ³
*   은닉: 벡터 (크기 = 2)
*   출력: 스칼라 (이진 분류)

**제한 사항**
*   타입 힌트 없음
*   내장 자동 미분 사용 **금지**
*   기본 수학 연산에 `numpy` 또는 `math`를 사용할 수 있습니다.

***

### **페이지 843**

**4. 2계층 신경망 구축하기 (벡터 FC + ReLU + 시그모이드 + 교차 엔트로피)**

**과제**
**1. 다음 게이트들을 구현:**
1.  MatMulGate (행렬 × 벡터)
2.  AddVecGate
3.  ReLUVecGate (요소별)
4.  SigmoidVecGate (마지막 레이어 전용; 스칼라 반환)
5.  BinaryCrossEntropyGate (스칼라 출력)
**2. 2계층 NN에 대한 계산 그래프 구축**
**3. `loss.backward(1.0)` 실행**
**4. 출력:**
1.  순전파 값: h_pre, h, logit, y_hat, loss
2.  그래디언트: dW1, db1, dW2, db2, dx

**(주어진 입력, 가중치, 편향, 레이블 값)**

**제출물**
*   **코드 파일**: `two_layer_relu_sigmoid.py`
*   **PDF 보고서 내용**:
    *   각 게이트의 역전파 규칙에 대한 간략한 설명
    *   순전파 및 그래디언트 출력

***

### **페이지 844**

**코드 스켈레톤**

**(코드: 벡터 연산을 위한 게이트 기반 역전파 구현을 위한 전체 스켈레톤 코드. Node, Gate, MatMulGate, AddVecGate, ReLUVecGate, SigmoidGate, BinaryCrossEntropyGate 클래스의 기본 구조와 테스트를 위한 main 함수 일부가 포함됨)**

***

### **페이지 845**

**과제 — MNIST에서 수동 2계층 신경망 훈련시키기 (작은 데모)**

**목표**
자신이 만든 자동 미분 엔진을 사용하여:
*   **1개의 MNIST 샘플** 로드 (예: 레이블 = 5)
*   **훈련 루프** 실행 (50~200회 반복)
*   순전파 → 역전파 → 수동 가중치 업데이트 수행
*   **출력:**
    *   시간에 따른 손실 감소
    *   최종 예측 확률
    *   예측된 클래스 vs 실제 클래스
이것은 당신의 **수작업 자동 미분**이 실제 훈련에 작동함을 보여줍니다.

**네트워크 아키텍처**
입력: 784차원 벡터 (28×28)
은닉: 10 유닛 (ReLU)
출력: 1 유닛 (시그모이드) → 이진 분류기: "이것이 5인가?"
손실: 이진 교차 엔트로피
하나의 숫자 대 나머지만 분류합니다.
레이블 = 1 (숫자가 5인 경우), 그렇지 않으면 0.

***

### **페이지 846**

**과제 — MNIST에서 수동 2계층 신경망 훈련시키기 (작은 데모)**

**데이터셋**

**(코드: MNIST 데이터셋에서 하나의 샘플을 선택하고, 이를 이진 분류 문제("이것이 5인가?")에 맞게 전처리하는 Python 코드)**

**제한 사항**
*   PyTorch 자동 미분 사용 **금지**
*   sklearn / numpy 훈련 함수 사용 **금지**
*   자신의 **노드 + 게이트 + backward()** 시스템을 사용해야 합니다.
*   허용된 수학 연산: 원시 배열에 대한 numpy

***

### **페이지 847**

**과제 — MNIST에서 수동 2계층 신경망 훈련시키기 (작은 데모)**

**훈련 루프 의사코드**

**(코드: 수동으로 구현한 게이트를 사용하여 MNIST 샘플 하나를 훈련시키는 전체 과정을 보여주는 의사코드)**

***

### **페이지 848**

**과제 — MNIST에서 수동 2계층 신경망 훈련시키기 (작은 데모)**

**요구되는 출력**
**끝에 출력:**
실제 레이블: 5
예측 확률: 0.9321
예측 클래스: 1 (모델은 5라고 생각함)

**그리고 훈련 중, 손실 감소를 보여주기:**
스텝 0: loss = 0.71
스텝 20: loss = 0.54
스텝 40: loss = 0.38
스텝 60: loss = 0.25
...

**제출물**
*   **코드 파일** `mnist_manual_train.py`
*   **스크린샷 또는 텍스트 로그** 내용:
    *   감소하는 손실
    *   최종 예측 확률
    *   최종 예측 클래스
*   **3문장 설명:**
    *   그래프가 어떻게 구축되었는지
    *   그래디언트가 어떻게 흐르는지
    *   왜 프레임워크 자동 미분이 일반적으로 필요한지

***

### **페이지 849**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**목표**
2계층 자동 미분 네트워크를 확장하여 MNIST 숫자 0~9를 분류하도록 합니다:
**입력(784) → 은닉(10) → 출력(10)**
**ReLU → 소프트맥스 → 교차 엔트로피**
그리고 **하나의 MNIST 이미지**에 대해 반복적으로 훈련하여 예측된 클래스 확률이 실제 숫자로 이동하는지 확인합니다.

**네트워크 사양**
(수식)

**여기서:**
*   y = 실제 숫자에 대한 원-핫 벡터 (길이 10)
*   ŷ = 소프트맥스 확률 (길이 10)

**구현해야 할 것**
이 게이트 유형들을 추가합니다:

| 게이트 | 기능 |
| :--- | :--- |
| SoftmaxGate | 벡터 입력 → 벡터 확률 |
| CrossEntropyGate | 소프트맥스 출력 + 원-핫 → 스칼라 손실 |

내장된 딥러닝 라이브러리를 사용해서는 **안 됩니다**.

**(수식: 소프트맥스 및 교차 엔트로피 공식)**

***

### **페이지 850**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**훈련 루프 의사코드**

**(코드: 10클래스 분류를 위해 수동으로 구현한 게이트를 사용하여 MNIST 샘플 하나를 훈련시키는 전체 과정을 보여주는 의사코드)**

***

### **페이지 851**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**예상 학생 출력**
**예시 출력 스타일:**
```
스텝 0: loss=2.31, pred=7, prob=0.11
스텝 20: loss=1.85, pred=7, prob=0.18
스텝 40: loss=1.14, pred=3, prob=0.40
스텝 80: loss=0.52, pred=3, prob=0.75
...
```
**정확한 숫자: 3**
**최종 예측: 3**
단일 샘플 훈련으로도, 예측은 실제 숫자를 향해 움직여야 합니다.

**제출물**
*   **코드:** `mnist_softmax_manual.py`
*   **콘솔 로그 또는 스크린샷 내용:**
    *   감소하는 손실
    *   증가하는 예측 클래스 확률
*   **짧은 설명 (3–5 문장):**
    *   소프트맥스 역할
    *   왜 교차 엔트로피가 소프트맥스와 잘 맞는지
    *   이진 분류 사례에서 무엇이 변경되었는지

***

### **페이지 852**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**목표**
자신만의 자동 미분 엔진을 사용하여 MNIST에 대한 **완전한 훈련 파이프라인**을 구축할 것입니다:
*   데이터를 **훈련 / 검증 / 테스트**로 분할
*   2계층 (또는 더 깊은) 신경망 훈련
*   **다음으로 실험:**
    *   **완전 연결 레이어 수**
    *   **은닉 레이어 너비 (노드)**
    *   **L2 정규화 강도 λ**
*   분류 **정확도** 평가
*   결과 분석 및 토론
**PyTorch / TensorFlow 자동 미분 사용 금지.**
MNIST를 원시 데이터 소스로만 사용하세요. 모든 순전파/역전파는 여러분의 코드여야 합니다.

**과제**
**1. MNIST 로드 및 전처리**
*   각 이미지를 784차원 벡터로 펼치기
*   픽셀 값을로 정규화
*   **다음으로 분할:**
    train: 1,000 ~ 50,000 샘플 (부분 집합 가능)
    val: 100 ~ 10,000 샘플 (부분 집합 가능)
    test: 100 ~ 10,000 샘플 (부분 집합 가능)

sklearn 유틸리티는 훈련이 아닌 **분할에만** 사용할 수 있습니다.

***

### **페이지 853**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**2. 모델 변형 구축**
**(a) 기본 모델**
784 → H → 10
ReLU → Softmax

**(b) 레이어 수 변경**
예시:
784 → H → 10
784 → H → H → 10
784 → H → H → H → 10

**(c) 은닉 유닛 H 변경**
제안된 값:
H ∈ {10, 50, 100, 200}

**3. L2 가중치 정규화 게이트 구현**
`L2RegularizationGate` 추가:
L_reg = λ Σᵢ ||Wᵢ||²

**총 손실:**
L = L_crossentropy + Σ_layers L_reg

*   **손실 및 정규화 항을 통한 역전파**
*   **λ 값 스윕:**
    λ ∈ {0, 0.001, 0.01, 0.1}

***

### **페이지 854**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**4. 훈련 루프**
**각 구성에 대해:**```
for epoch in 1..E:
    각 훈련 미니배치에 대해 순전파
    역전파
    SGD 업데이트
검증 세트에서 평가
```
**하이퍼파라미터 (권장):**
미니 배치 크기 = 1 (또는 64 또는 128)
에포크 = 3–5 (최소)
학습률 = 0.1 (또는 조정)
**훈련 데이터를 섞을 수 있습니다**
**원한다면 검증 세트 최고 모델에서 중지 (조기 종료 선택 사항)**

**5. 평가**
**훈련 후:**
*   테스트 세트에서 **정확도 계산**
*   혼동 행렬 출력 (선택 사항)
*   손실 곡선 및 검증 정확도 곡선 플롯 (선택 사항)

***

### **페이지 855**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**출력 샘플 (예시)**
```
=== RUN {'depth': 2, 'hidden_dim': 64, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.3652 val_acc=0.834
[epoch 2] train_loss/step_avg=0.8717 val_acc=0.871
[TEST] depth=2, hidden=64, lambda=0.0 --> acc=0.862

=== RUN {'depth': 2, 'hidden_dim': 64, 'lam': 0.01, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.4229 val_acc=0.842
[epoch 2] train_loss/step_avg=0.9381 val_acc=0.878
[TEST] depth=2, hidden=64, lambda=0.01 --> acc=0.872

=== RUN {'depth': 3, 'hidden_dim': 64, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.5023 val_acc=0.812
[epoch 2] train_loss/step_avg=0.9546 val_acc=0.861
[TEST] depth=3, hidden=64, lambda=0.0 --> acc=0.853

=== RUN {'depth': 2, 'hidden_dim': 128, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.2998 val_acc=0.846
[epoch 2] train_loss/step_avg=0.8135 val_acc=0.882
[TEST] depth=2, hidden=128, lambda=0.0 --> acc=0.879
```

***

### **페이지 856**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**6. 보고서 필수 사항**

| 섹션 | 설명 |
| :--- | :--- |
| 실험 설정 | 아키텍처, 하이퍼파라미터, 데이터셋 분할 |
| 결과 표 | 각 구성에 대한 정확도 |
| 손실 곡선 | 훈련 손실, 검증 손실 |
| 모델 분석 | 미적합 / 과적합 경향 |
| 정규화 효과 | λ 스윕 결과 |
| 최종 테스트 정확도 | 최고 모델로부터 |

**토론 프롬프트**
**보고서에 답변:**
1.  어떤 아키텍처가 가장 좋은 성능을 보였나요? 왜 그런가요?
2.  너비를 늘리는 것이 성능에 어떤 영향을 미치나요?
3.  깊이가 학습에 어떤 영향을 미치나요?
4.  λ가 너무 크거나 너무 작으면 어떻게 되나요?
5.  정규화 없이 과적합의 증거가 있나요?
6.  검증 정확도가 테스트 정확도와 상관관계가 있었나요?

**제출물**
*   **코드:** `mnist_full_manual_train.py`
*   **스크린샷/로그 내용:**
    *   에포크별 정확도 및 손실
    *   최종 테스트 정확도
*   **PDF 보고서 (2–5 페이지):**
    *   실험 설정
    *   표 및 그래프
    *   분석 및 토론

***

### **페이지 857**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 858**

**프로그래밍 과제 08: 실습: CNN 구현하기**

*   **목표:**
    *   이 과제에서는 **합성곱 신경망(CNN)을 처음부터 구현**하는 것을 연습할 것입니다.
    *   학생들은 합성곱, 풀링, 완전 연결 레이어가 어떻게 함께 작동하여 이미지를 분류하는지 배우게 됩니다.
*   **목표:**
    *   CNN의 구조와 순전파 과정을 이해합니다.
    *   NumPy만을 사용하여 합성곱 및 풀링 연산을 구현합니다.
    *   간단한 데이터셋(예: MNIST 또는 CIFAR-10)에서 모델을 훈련하고 평가합니다.

***

### **페이지 859**

**처음부터 CNN 구축하기 (MNIST)**

*   **1) 개요**
    *   딥러닝 프레임워크 없이 **NumPy만**을 사용하여 **합성곱 신경망(CNN)을 처음부터 구현**할 것입니다. 핵심 모듈(합성곱, 활성화, 풀링, 완전 연결, 손실, 옵티마이저)을 구축하고, 이를 모델로 쌓아 MNIST에서 훈련하고 결과를 분석합니다. 주요 목표는 속도보다는 **메커니즘(순전파 및 역전파)을 이해**하는 것입니다.
*   **2) 학습 목표**
    *   올바른 순전파/역전파를 가진 **Conv2D, ReLU, MaxPool2D, Flatten, Linear** 레이어를 구현합니다.
    *   **소프트맥스 교차 엔트로피** 손실과 간단한 옵티마이저(예: SGD)를 구현합니다.
    *   레이어들을 CNN으로 구성하고 MNIST에서 **훈련/평가**합니다.
    *   설정(필터, 커널 크기, 풀링, 학습률, 가중치 감쇠 등)을 변경하여 작은 **실험**을 실행하고 결과를 해석합니다.
*   **3) 데이터셋**
    *   **MNIST**와 **CIFAR-10**을 모두 사용하여 회색조 및 컬러 이미지에 대한 CNN 동작을 탐색합니다.

| 데이터셋 | 채널 | 이미지 크기 | 클래스 | 비고 |
| :--- | :--- | :--- | :--- | :--- |
| **MNIST** | 1 (회색조) | 28×28 | 10개 숫자 | 간단함; 디버깅에 좋음 |
| **CIFAR-10** | 3 (RGB 컬러) | 32×32 | 10개 객체 카테고리 | 컬러 + 더 복잡함 |

***

### **페이지 860**

**처음부터 CNN 구축하기 (MNIST)**

**4) 구현해야 할 것 (NumPy 전용)**
각각에 대해 **순전파**와 **역전파**를 구현합니다:
**1. Conv2D**
1.  입력: (N, C_in, H, W) → 출력: (N, C_out, H', W')
2.  하이퍼파라미터: `out_channels`, `kernel_size` (k×k), `stride=1`, `padding` ('same' = k//2 사용 가능).
3.  명확성을 위해 순진한 루프를 허용합니다. (선택 사항: 효율성을 위해 im2col 사용).
**2. 활성화**
1.  ReLU (필수). 선택적으로 Sigmoid/Tanh를 추가하여 비교.
**3. MaxPool2D**
1.  일반적: `kernel_size=2`, `stride=2`.
2.  역전파는 최대값 위치에만 그래디언트를 반환해야 합니다.
**4. Flatten**
**5. Linear (완전 연결)**
**6. 소프트맥스 교차 엔트로피 손실**
1.  수치적으로 안정적인 소프트맥스 (샘플별 최대값 빼기).
2.  배치에 대한 평균 손실과 로짓에 대한 그래디언트를 반환.
**7. 옵티마이저**
1.  학습률 η를 사용한 SGD.
2.  **가중치에만** 가중치 감쇠(L2)를 구현합니다 (편향에는 감쇠 없음).
**중요**: **그래디언트 계산(역전파)**을 **파라미터 업데이트(옵티마이저)**와 분리하여 유지하세요. `backward()` 내부에서 업데이트하지 마세요.

**5) 모델 구성 (기준선)**
작은 CNN을 구축합니다; 설정을 변경할 수 있습니다:
**기준선 예시**
Conv2D(1→8, k=3, padding='same') → ReLU → MaxPool2D(2)
Conv2D(8→16, k=3, padding='same') → ReLU → MaxPool2D(2)
Flatten → Linear(16*7*7 → 64) → ReLU → Linear(64 → 10)
*   `out_channels`, k를 변경하거나, 블록을 추가/제거하거나, 은닉 크기를 조정할 수 있지만 — **선택 사항을 문서화**하세요.

***

### **페이지 861**

**처음부터 CNN 구축하기 (MNIST)**

**6) 훈련 요구 사항**
*   **배칭**: 미니배치 SGD (예: 배치 크기 64 또는 128).
*   **에포크**: 수렴할 때까지 훈련 (예: 이 작은 네트워크에는 5–15 에포크가 일반적임).
*   **메트릭**: **에포크별 훈련 손실/정확도**와 **최종 테스트 정확도**를 보고합니다.
*   **목표**: 합리적인 CNN으로, **≥97% 테스트 정확도**가 예상됩니다. (더 낮으면 이유를 분석하세요.)
*   **결정성**: 재현성을 위해 랜덤 시드를 설정합니다.

**7) 실험 (설정 다양화)**
다음 중 **최소 두 가지**를 수행하고 결과(정확도, 손실 곡선, 훈련 행동)를 논의합니다:
*   Conv 레이어의 **#필터** 변경 (예: 8/16 vs 16/32).
*   **커널 크기** 변경 (3 vs 5).
*   **MaxPool 유/무** 시도 (또는 풀링 하나 vs 두 개).
*   **학습률** 변경 (예: 1e-1, 1e-2, 1e-3).
*   **가중치 감쇠** 추가 및 일반화 비교.
*   활성화 교체/삽입 (예: Tanh) 및 수렴 비교.

**8) 제출물**
다음을 포함해야 합니다:
**1. 코드** (수정 없이 실행 가능)
**2. 보고서**
1.  **모델 아키텍처** (작은 블록 다이어그램 또는 레이어 표).
2.  **훈련 곡선** (에포크별 손실/정확도).
3.  **최종 테스트 정확도** 및 혼동 행렬 또는 몇 가지 잘못 분류된 예시.
4.  **실험** (무엇을 변경했는지, 결과, 해석).
5.  **구현 노트** (설계 선택, 수치적 트릭, 수정된 버그).
6.  **재현성** (환경, 시드, 하이퍼파라미터).

***

### **페이지 862**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**1) PyTorch란 무엇인가?**
*   **PyTorch**는 다음과 같은 기능을 갖춘 Python **딥러닝 프레임워크**입니다:
    *   **텐서** (NumPy와 유사, CPU/GPU에서 실행)
    *   **자동 미분** (Autograd)
    *   **nn.Module & optim** (레이어, 손실, 옵티마이저)
    *   **torchvision** (데이터셋, 변환, 사전 훈련된 모델)
**왜 지금 사용하는가?** 여러분은 순전파/역전파를 직접 구축했습니다; PyTorch를 사용하면 다음을 할 수 있습니다:
*   그래디언트 배관이 아닌 **모델링 및 실험에 집중**
*   **더 큰 모델과 GPU로 확장**
*   표준 도구 사용 (DataLoader, 체크포인트, AMP 등)

**2) 설치 (하나의 경로 선택)**
**Conda (권장)**
```bash
# CPU 전용
conda create -n dl python=3.10 -y
conda activate dl
conda install pytorch torchvision torchaudio cpuonly -c pytorch -y

# 또는: CUDA (호환되는 NVIDIA GPU + 드라이버가 있는 경우)
conda create -n dl python=3.10 -y
conda activate dl
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```
**pip**
```bash
# CPU 전용
python -m venv dl && source dl/bin/activate # (Windows: dl\Scripts\activate)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# CUDA (시스템에서 지원되는 경우)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

***

### **페이지 863**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**3) 과제 개요**
이전 NumPy CNN을 **PyTorch**로 다시 구현한 다음, **MNIST**와 **CIFAR-10**에서 훈련/평가합니다.
**반드시 포함:**
*   **Conv → ReLU → Pool → … → Linear**를 사용한 모델 (합리적인 변형 OK)
*   **교차 엔트로피 손실, SGD 또는 Adam**
*   **DataLoaders, 변환, 훈련/검증/테스트** 루프
*   두 데이터셋 모두에 대한 **정확도 및 손실 곡선; 최종 테스트 정확도**
*   **간단한 비교**: MNIST vs CIFAR-10 (채널, 난이도, 수렴)

**4) 기준 아키텍처**
**MNIST (1×28×28)**
Conv(1→16, 3×3, padding=1) → ReLU → MaxPool(2)
Conv(16→32, 3×3, padding=1) → ReLU → MaxPool(2)
Flatten → Linear(32*7*7→128) → ReLU → Linear(128→10)
**CIFAR-10 (3×32×32)**
Conv(3→32, 3×3, padding=1) → ReLU → MaxPool(2)
Conv(32→64, 3×3, padding=1) → ReLU → MaxPool(2)
Flatten → Linear(64*8*8→256) → ReLU → Linear(256→10)
훈련이 너무 쉬우면 CIFAR-10의 채널을 늘릴 수 있습니다 (예: 32/64 → 64/128).

**5) 시작 코드 (PyTorch)**
`train_pytorch_cnn.py`로 저장하고 실행:
```bash
python train_pytorch_cnn.py --dataset mnist --epochs 8 --batch_size 128 --lr 0.01
python train_pytorch_cnn.py --dataset cifar10 --epochs 20 --batch_size 128 --lr 0.001
```

***

### **페이지 864**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: PyTorch로 간단한 CNN 모델(SmallCNN)을 정의하는 Python 코드. 입력 채널 수에 따라 다른 구조를 가지며, 분류기 부분은 첫 순전파 시 동적으로 생성됨)**

***

### **페이지 865**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: MNIST와 CIFAR-10 데이터셋에 대한 데이터 로더와 변환을 정의하는 Python 함수. 데이터셋에 따라 다른 정규화 값과 데이터 증강(CIFAR-10의 경우)을 적용함)**

***

### **페이지 866**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: PyTorch 모델을 위한 훈련(train_one_epoch) 및 평가(evaluate) 함수. 훈련 함수는 그래디언트 계산 및 파라미터 업데이트를 포함하며, 평가 함수는 그래디언트 계산 없이 실행됨)**

***

### **페이지 867**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: 명령줄 인자(데이터셋, 에포크, 배치 크기 등)를 파싱하고, 모델, 데이터 로더, 옵티마이저, 손실 함수를 설정한 후, 훈련 및 평가 루프를 실행하는 메인 Python 스크립트)**

***

### **페이지 868**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**6) 요구 사항 및 실험**
**필수**
*   **두 데이터셋 모두** 훈련 (MNIST & CIFAR-10).
*   에포크별 **훈련 손실/정확도**와 **테스트 정확도**를 기록.
*   **재현 가능한 시드** 사용.
*   두 데이터셋 모두에 대한 **손실/정확도 vs 에포크 플롯** 제출.

**실험 (≥2개 선택)**
*   **옵티마이저 교체** (SGD vs Adam).
*   **#필터** 또는 **은닉 크기** 변경.
*   **LR 값** 시도 (예: 1e-1/1e-2/1e-3) 및 비교.
*   CIFAR-10에 **데이터 증강** 유/무.
*   **드롭아웃** 또는 **배치 정규화** 추가 및 효과 논의.

**7) 제출물**
**1. 코드** (실행 가능): `train_pytorch_cnn.py` (및 모든 헬퍼)
**2. 보고서:**
1.  모델 다이어그램 (MNIST/CIFAR-10)
2.  훈련 곡선, 최종 정확도
3.  실험 결과 (≥2)
4.  간단한 MNIST vs CIFAR-10 비교 (채널, 난이도, 수렴)
5.  재현성 세부 정보 (환경, 시드, 하이퍼파라미터)

***

### **페이지 869**

**특징 맵 시각화**

**목표**
**CNN 필터가 무엇을 탐지하는지** 이해하기 위해, 중간 합성곱 레이어의 **특징 맵(활성화 출력)**을 시각화합니다.

**개념**
*   각 **합성곱 필터**는 특정 **패턴**(엣지, 질감, 색상, 모양 등)을 탐지하도록 학습합니다.
*   훈련 후, **활성화 맵**은 입력 이미지의 **어떤 영역**이 각 필터를 활성화하는지 보여줍니다.
*   초기 레이어 → 저수준 특징 (엣지, 블롭)
    나중 레이어 → 고수준 추상적 모양 (숫자, 객체 부분)

**과제 지침**
**1. 훈련된 모델 하나 선택**
1.  훈련된 PyTorch CNN (MNIST 또는 CIFAR-10)을 사용합니다.
2.  저장된 가중치를 로드하거나 훈련 직후에 시각화합니다.
**2. 하나 또는 두 개의 입력 이미지 선택**
1.  MNIST의 경우: 숫자 하나 (예: "5")
2.  CIFAR-10의 경우: 샘플 하나 (예: "고양이")
**3. 순전파만** 이미지를 통과시키고, 초기 Conv 레이어에서 출력을 추출합니다.

**(코드: 훈련된 모델의 첫 번째 합성곱 레이어 이후의 특징 맵을 추출하는 PyTorch 코드)**

***

### **페이지 870**

**특징 맵 시각화**

**4. 시각화**

**(코드: Matplotlib을 사용하여 추출된 특징 맵(활성화) 중 일부를 시각화하는 Python 코드)**

**5. 해석**
*   어떤 패턴이 보이는지 설명합니다 (예: 수직 엣지, 수평 엣지, 블롭 영역).
*   MNIST vs CIFAR-10 특징 맵 비교:
    *   "MNIST 필터는 획 엣지를 강조하고; CIFAR-10 필터는 색상 블롭과 질감을 포착합니다."

**6. (선택 사항 고급) 더 깊은 레이어 활성화 시각화 (2번째 conv).**
특징 맵이 어떻게 더 **추상적**이거나 **지역화**되는지 비교합니다.

**7. 보고서 제출물 (보고서에 추가)**
**포함 내용:**
*   사용한 입력 이미지.
*   최소 하나의 Conv 레이어의 특징 맵 시각화.
*   필터가 무엇을 탐지하는 것처럼 보이는지에 대한 3–5 문장 해석.
*   (선택 사항) MNIST vs CIFAR-10 시각적 패턴 비교.

***

### **페이지 871**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 872**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **목표**
    *   이 과제에서는 매우 작은 신경망을 처음부터 구현하고 **수동으로 계산**할 것입니다:
        *   순전파
        *   역전파 (그래디언트)
        *   경사 하강법 업데이트
        *   여러 반복에 대한 훈련
        *   시간에 따른 손실 및 예측 진화
*   이 과제는 딥러닝이 실제로 어떻게 그래디언트를 계산하고 내부적으로 가중치를 학습하는지 이해하는 데 도움이 될 것입니다.

***

### **페이지 873**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **네트워크 아키텍처**
    *   고정된 2계층 신경망:

| 레이어 | 모양 | 활성화 |
| :--- | :--- | :--- |
| 입력 | 2 | — |
| 은닉 | 2 | 시그모이드 |
| 출력 | 1 | 시그모이드 |

*   **주어진 값**
    *   입력 샘플: x =ᵀ
    *   레이블: y = 1
    *   재현성을 위해 `np.random.seed(0)` 사용
*   **제한 사항**
    *   **Python + NumPy**만 허용됩니다.

***

### **페이지 874**

**프로그래밍 과제 07: 역전파 및 정규화**

*   **과제**
    *   1. 파라미터 무작위 초기화
    *   2. 순전파
    *   3. 손실 (이진 교차 엔트로피)
    *   4. 역전파
        *   연쇄 법칙을 사용하여 수동으로 그래디언트 계산 (자동 미분 없음).
    *   5. 경사 하강법 훈련
        *   **200회 반복** 동안 반복:
            *   순전파
            *   역전파
            *   그래디언트 업데이트
            *   학습률 η = 0.1
    *   6. 학습 진행 추적
        *   값이 어떻게 변하는지 기록하고 보여주기:
            *   감소하는 손실
            *   목표 y = 1에 접근하는 ŷ
        *   원한다면 10 스텝마다 (또는 더 자주) 출력.
    *   7. 최종 분석
        *   **비교:**
            *   초기 손실 vs 최종 손실
            *   초기 예측 vs 최종 예측
        *   네트워크가 학습했는지 여부와 그 이유를 설명.

***

### **페이지 875**

**프로그래밍 과제 07: 역전파 및 정규화**

**보고서 내용**
PDF 보고서에는 다음이 포함되어야 합니다:

**모델 및 수학**
*   순전파 방정식에 대한 간략한 설명
*   역전파 및 그래디언트에 대한 간략한 설명

**코드 스크린샷**
*   주요 구현 섹션을 보여주세요
    *   순전파
    *   역전파
    *   경사 하강법 루프

**출력 및 학습 로그**
*   훈련 과정을 보여주는 표 또는 로그
    (각 체크포인트에서의 손실 + 예측)
*   최종 출력된 값
*   손실 곡선 (선 그래프 강력 추천)
*   예측 곡선 (선택 사항이지만 권장)

**해석**
*   손실이 감소했나요? (설명)
*   ŷ가 1에 가까워졌나요? (설명)
*   역전파를 수동으로 구현하면서 무엇을 배웠나요?

필수 스크린샷, 로그 또는 설명이 누락된 보고서는 감점됩니다.

***

### **페이지 876**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

*   데이터(**노드**)와 연산(**게이트**)을 분리하는 작은 계산 그래프 엔진을 구축합니다. 네 가지 게이트를 구현합니다:
    *   AddGate / MulGate / SigmoidGate / CrossEntropyLossGate (이진)
*   손실로부터 **역전파**하고 모든 필요한 그래디언트를 출력합니다.

**구현해야 할 것**
**핵심 클래스**
*   **노드**
    *   필드: `value`, `grad`, `op`, `parents`
    *   메서드: `backward(upstream_grad=1.0)`
        *   `self.grad`에 누적
        *   `self.op`가 존재하면, `self.op.backward(self, upstream_grad)` 호출 (리프 노드에 도달할 때까지 재귀적으로)
*   **게이트 (기본 클래스)**
    *   `forward(*parents)` → `value`, `op=self`, `parents=(...)`를 가진 새로운 **노드**를 반환
    *   `backward(out_node, upstream_grad)` → `out_node.parents`를 읽고, 지역 기여도를 계산하고, 각 부모의 `.backward(...)`를 호출
*   **AddGate, MulGate, SigmoidGate, CrossEntropyLossGate**
    *   각각 올바른 `forward(...)`와 `backward(...)`를 제공해야 합니다.
    *   CrossEntropyLossGate의 경우, 레이블 y를 상수(그래디언트 없음)로 취급합니다. 안정성을 위해 내부적으로 작은 `eps=1e-12`를 사용합니다.

***

### **페이지 877**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

**예제 테스트 값**
*   입력: x1 = -1.0, x2 = -2.0
*   파라미터: w1 = 2.0, w2 = -3.0, b = -3.0
*   레이블: y = 1 (이진)

**구축할 그래프**
1.  s1 = w1 * x1
2.  s2 = w2 * x2
3.  s3 = s1 + s2 + b
4.  y_hat = sigmoid(s3)
5.  loss = cross_entropy(y_hat, y)
6.  `loss.backward(1.0)` 호출

**출력할 내용**
*   순전파 값: s1, s2, s3, y_hat, loss
*   역전파 후 그래디언트:
    *   파라미터: w1.grad, w2.grad, b.grad
    *   (선택 사항) 입력: x1.grad, x2.grad

**제출물**
*   **코드**: `gate_graph.py`
*   **보고서 (PDF)**: 각 게이트의 `backward`가 어떻게 그래디언트를 부모에게 보내는지에 대한 간략한 설명, 그리고 출력된 순전파 값과 그래디언트 (로그를 붙여넣거나 작은 표로 작성).

***

### **페이지 878**

**2. 게이트 기반 모듈식 역전파 (Python 전용, 자동 미분 없음)**

**(코드: 게이트 기반 역전파 구현을 위한 스켈레톤 코드. Node, Gate, AddGate, MulGate, SigmoidGate, CrossEntropyLossGate 클래스의 기본 구조와 테스트를 위한 main 함수 일부가 포함됨)**

***

### **페이지 879**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

*   **목표**
    *   이전에 구현한 동일한 **노드 + 게이트 클래스**(AddGate, MulGate, SigmoidGate, CrossEntropyLossGate)를 사용하여, 작은 **2계층 신경망**(입력→은닉→출력)을 스칼라 계산 그래프로 구축하고, **손실로부터 역전파**를 실행하고, 순전파 값과 그래디언트를 **출력**합니다.
*   **네트워크 사양**
    *   **아키텍처**: 2 → 2 → 1 (은닉 레이어는 2개의 유닛; 은닉 및 출력 모두 **시그모이드** 사용)
    *   **손실**: **이진 교차 엔트로피** (확률 입력)
    *   **단일 고정 샘플** (재현성):
        *   (주어진 입력, 가중치, 편향, 레이블 값)

***

### **페이지 880**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

**구현할 내용**
**1. 게이트를 사용하여 순전파 그래프 구축 (스칼라 스타일):**
1.  은닉 유닛 1: 두 스칼라 곱의 합 + 편향 → 시그모이드
2.  은닉 유닛 2: 동일
3.  출력: (h1, h2)의 가중 합 + 편향 → 시그모이드
4.  손실: CE(ŷ, y)
**2. `loss.backward(1.0)`을 호출하여 그래프를 통해 역전파.**
**3. 결과 출력:**
1.  순전파: h1_z, h1, h2_z, h2, s2, y_hat, loss
2.  그래디언트 (파라미터): dW1_11, dW1_12, dW1_21, dW1_22, db1_1, db1_2, dW2_1, dW2_2, db2
3.  (선택 사항) 입력: dx1, dx2

**규칙**
*   순수 Python; `exp`, `log`를 위해 `math`를 사용할 수 있습니다.
*   PyTorch/JAX/TF 또는 자동 미분을 사용하지 **마세요**.
*   타입 힌트 없음.
*   노드가 여러 하류 기여를 받을 때 그래디언트는 `+=`로 누적되어야 합니다.

**제출물**
*   `two_layer_scalar.py` (전체 구현)
*   `report.pdf` 내용:
    *   게이트가 어떻게 부모에게 그래디언트를 전파하는지에 대한 짧은 단락 설명
    *   출력된 순전파 값과 그래디언트 (로그 복사 또는 작은 표)

***

### **페이지 881**

**3. 게이트를 사용하여 2계층 신경망 구축하기**

**메인 함수 (부분; 이것만 보여주세요)**

**(코드: 2계층 신경망의 계산 그래프를 스칼라 연산으로 구축하는 과정을 보여주는 Python 코드의 일부. 학생이 나머지 부분을 완성하도록 지시)**

***

### **페이지 882**

**4. 2계층 신경망 구축하기 (벡터 FC + ReLU + 시그모이드 + 교차 엔트로피)**

**목표**
자동 미분 엔진을 확장하여 **2계층 신경망**을 구현합니다:
**입력 → FC → ReLU → FC → 시그모이드 → 이진 교차 엔트로피 손실**
순전파/역전파는 **자신만의 노드/게이트 시스템만**을 사용하여 수행해야 합니다.
(PyTorch/JAX/TensorFlow 자동 미분 없음).

**아키텍처**
*   입력: 벡터 x ∈ ℝ³
*   은닉: 벡터 (크기 = 2)
*   출력: 스칼라 (이진 분류)

**제한 사항**
*   타입 힌트 없음
*   내장 자동 미분 사용 **금지**
*   기본 수학 연산에 `numpy` 또는 `math`를 사용할 수 있습니다.

***

### **페이지 883**

**4. 2계층 신경망 구축하기 (벡터 FC + ReLU + 시그모이드 + 교차 엔트로피)**

**과제**
**1. 다음 게이트들을 구현:**
1.  MatMulGate (행렬 × 벡터)
2.  AddVecGate
3.  ReLUVecGate (요소별)
4.  SigmoidVecGate (마지막 레이어 전용; 스칼라 반환)
5.  BinaryCrossEntropyGate (스칼라 출력)
**2. 2계층 NN에 대한 계산 그래프 구축**
**3. `loss.backward(1.0)` 실행**
**4. 출력:**
1.  순전파 값: h_pre, h, logit, y_hat, loss
2.  그래디언트: dW1, db1, dW2, db2, dx

**(주어진 입력, 가중치, 편향, 레이블 값)**

**제출물**
*   **코드 파일**: `two_layer_relu_sigmoid.py`
*   **PDF 보고서 내용**:
    *   각 게이트의 역전파 규칙에 대한 간략한 설명
    *   순전파 및 그래디언트 출력

***

### **페이지 884**

**코드 스켈레톤**

**(코드: 벡터 연산을 위한 게이트 기반 역전파 구현을 위한 전체 스켈레톤 코드. Node, Gate, MatMulGate, AddVecGate, ReLUVecGate, SigmoidGate, BinaryCrossEntropyGate 클래스의 기본 구조와 테스트를 위한 main 함수 일부가 포함됨)**

***

### **페이지 885**

**과제 — MNIST에서 수동 2계층 신경망 훈련시키기 (작은 데모)**

**목표**
자신이 만든 자동 미분 엔진을 사용하여:
*   **1개의 MNIST 샘플** 로드 (예: 레이블 = 5)
*   **훈련 루프** 실행 (50~200회 반복)
*   순전파 → 역전파 → 수동 가중치 업데이트 수행
*   **출력:**
    *   시간에 따른 손실 감소
    *   최종 예측 확률
    *   예측된 클래스 vs 실제 클래스
이것은 당신의 **수작업 자동 미분**이 실제 훈련에 작동함을 보여줍니다.

**네트워크 아키텍처**
입력: 784차원 벡터 (28×28)
은닉: 10 유닛 (ReLU)
출력: 1 유닛 (시그모이드) → 이진 분류기: "이것이 5인가?"
손실: 이진 교차 엔트로피
하나의 숫자 대 나머지만 분류합니다.
레이블 = 1 (숫자가 5인 경우), 그렇지 않으면 0.

***

### **페이지 886**

**과제 — MNIST에서 수동 2계층 신경망 훈련시키기 (작은 데모)**

**데이터셋**

**(코드: MNIST 데이터셋에서 하나의 샘플을 선택하고, 이를 이진 분류 문제("이것이 5인가?")에 맞게 전처리하는 Python 코드)**

**제한 사항**
*   PyTorch 자동 미분 사용 **금지**
*   sklearn / numpy 훈련 함수 사용 **금지**
*   자신의 **노드 + 게이트 + backward()** 시스템을 사용해야 합니다.
*   허용된 수학 연산: 원시 배열에 대한 numpy

***

### **페이지 887**

**과제 — MNIST에서 수동 2계층 신경망 훈련시키기 (작은 데모)**

**훈련 루프 의사코드**

**(코드: 수동으로 구현한 게이트를 사용하여 MNIST 샘플 하나를 훈련시키는 전체 과정을 보여주는 의사코드)**

***

### **페이지 888**

**과제 — MNIST에서 수동 2계층 신경망 훈련시키기 (작은 데모)**

**요구되는 출력**
**끝에 출력:**
실제 레이블: 5
예측 확률: 0.9321
예측 클래스: 1 (모델은 5라고 생각함)

**그리고 훈련 중, 손실 감소를 보여주기:**
스텝 0: loss = 0.71
스텝 20: loss = 0.54
스텝 40: loss = 0.38
스텝 60: loss = 0.25
...

**제출물**
*   **코드 파일** `mnist_manual_train.py`
*   **스크린샷 또는 텍스트 로그** 내용:
    *   감소하는 손실
    *   최종 예측 확률
    *   최종 예측 클래스
*   **3문장 설명:**
    *   그래프가 어떻게 구축되었는지
    *   그래디언트가 어떻게 흐르는지
    *   왜 프레임워크 자동 미분이 일반적으로 필요한지

***

### **페이지 889**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**목표**
2계층 자동 미분 네트워크를 확장하여 MNIST 숫자 0~9를 분류하도록 합니다:
**입력(784) → 은닉(10) → 출력(10)**
**ReLU → 소프트맥스 → 교차 엔트로피**
그리고 **하나의 MNIST 이미지**에 대해 반복적으로 훈련하여 예측된 클래스 확률이 실제 숫자로 이동하는지 확인합니다.

**네트워크 사양**
(수식)

**여기서:**
*   y = 실제 숫자에 대한 원-핫 벡터 (길이 10)
*   ŷ = 소프트맥스 확률 (길이 10)

**구현해야 할 것**
이 게이트 유형들을 추가합니다:

| 게이트 | 기능 |
| :--- | :--- |
| SoftmaxGate | 벡터 입력 → 벡터 확률 |
| CrossEntropyGate | 소프트맥스 출력 + 원-핫 → 스칼라 손실 |

내장된 딥러닝 라이브러리를 사용해서는 **안 됩니다**.

**(수식: 소프트맥스 및 교차 엔트로피 공식)**

***

### **페이지 890**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**훈련 루프 의사코드**

**(코드: 10클래스 분류를 위해 수동으로 구현한 게이트를 사용하여 MNIST 샘플 하나를 훈련시키는 전체 과정을 보여주는 의사코드)**

***

### **페이지 891**

**수동 자동 미분 NN을 10클래스 소프트맥스로 확장하기**

**예상 학생 출력**
**예시 출력 스타일:**
```
스텝 0: loss=2.31, pred=7, prob=0.11
스텝 20: loss=1.85, pred=7, prob=0.18
스텝 40: loss=1.14, pred=3, prob=0.40
스텝 80: loss=0.52, pred=3, prob=0.75
...
```
**정확한 숫자: 3**
**최종 예측: 3**
단일 샘플 훈련으로도, 예측은 실제 숫자를 향해 움직여야 합니다.

**제출물**
*   **코드:** `mnist_softmax_manual.py`
*   **콘솔 로그 또는 스크린샷 내용:**
    *   감소하는 손실
    *   증가하는 예측 클래스 확률
*   **짧은 설명 (3–5 문장):**
    *   소프트맥스 역할
    *   왜 교차 엔트로피가 소프트맥스와 잘 맞는지
    *   이진 분류 사례에서 무엇이 변경되었는지

***

### **페이지 892**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**목표**
자신만의 자동 미분 엔진을 사용하여 MNIST에 대한 **완전한 훈련 파이프라인**을 구축할 것입니다:
*   데이터를 **훈련 / 검증 / 테스트**로 분할
*   2계층 (또는 더 깊은) 신경망 훈련
*   **다음으로 실험:**
    *   **완전 연결 레이어 수**
    *   **은닉 레이어 너비 (노드)**
    *   **L2 정규화 강도 λ**
*   분류 **정확도** 평가
*   결과 분석 및 토론
**PyTorch / TensorFlow 자동 미분 사용 금지.**
MNIST를 원시 데이터 소스로만 사용하세요. 모든 순전파/역전파는 여러분의 코드여야 합니다.

**과제**
**1. MNIST 로드 및 전처리**
*   각 이미지를 784차원 벡터로 펼치기
*   픽셀 값을로 정규화
*   **다음으로 분할:**
    train: 1,000 ~ 50,000 샘플 (부분 집합 가능)
    val: 100 ~ 10,000 샘플 (부분 집합 가능)
    test: 100 ~ 10,000 샘플 (부분 집합 가능)

sklearn 유틸리티는 훈련이 아닌 **분할에만** 사용할 수 있습니다.

***

### **페이지 893**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**2. 모델 변형 구축**
**(a) 기본 모델**
784 → H → 10
ReLU → Softmax

**(b) 레이어 수 변경**
예시:
784 → H → 10
784 → H → H → 10
784 → H → H → H → 10

**(c) 은닉 유닛 H 변경**
제안된 값:
H ∈ {10, 50, 100, 200}

**3. L2 가중치 정규화 게이트 구현**
`L2RegularizationGate` 추가:
L_reg = λ Σᵢ ||Wᵢ||²

**총 손실:**
L = L_crossentropy + Σ_layers L_reg

*   **손실 및 정규화 항을 통한 역전파**
*   **λ 값 스윕:**
    λ ∈ {0, 0.001, 0.01, 0.1}

***

### **페이지 894**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**4. 훈련 루프**
**각 구성에 대해:**
```
for epoch in 1..E:
    각 훈련 미니배치에 대해 순전파
    역전파
    SGD 업데이트
검증 세트에서 평가
```
**하이퍼파라미터 (권장):**
미니 배치 크기 = 1 (또는 64 또는 128)
에포크 = 3–5 (최소)
학습률 = 0.1 (또는 조정)
**훈련 데이터를 섞을 수 있습니다**
**원한다면 검증 세트 최고 모델에서 중지 (조기 종료 선택 사항)**

**5. 평가**
**훈련 후:**
*   테스트 세트에서 **정확도 계산**
*   혼동 행렬 출력 (선택 사항)
*   손실 곡선 및 검증 정확도 곡선 플롯 (선택 사항)

***

### **페이지 895**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**출력 샘플 (예시)**
```
=== RUN {'depth': 2, 'hidden_dim': 64, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.3652 val_acc=0.834
[epoch 2] train_loss/step_avg=0.8717 val_acc=0.871
[TEST] depth=2, hidden=64, lambda=0.0 --> acc=0.862

=== RUN {'depth': 2, 'hidden_dim': 64, 'lam': 0.01, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.4229 val_acc=0.842
[epoch 2] train_loss/step_avg=0.9381 val_acc=0.878
[TEST] depth=2, hidden=64, lambda=0.01 --> acc=0.872

=== RUN {'depth': 3, 'hidden_dim': 64, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.5023 val_acc=0.812
[epoch 2] train_loss/step_avg=0.9546 val_acc=0.861
[TEST] depth=3, hidden=64, lambda=0.0 --> acc=0.853

=== RUN {'depth': 2, 'hidden_dim': 128, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.2998 val_acc=0.846
[epoch 2] train_loss/step_avg=0.8135 val_acc=0.882
[TEST] depth=2, hidden=128, lambda=0.0 --> acc=0.879
```

***

### **페이지 896**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**6. 보고서 필수 사항**

| 섹션 | 설명 |
| :--- | :--- |
| 실험 설정 | 아키텍처, 하이퍼파라미터, 데이터셋 분할 |
| 결과 표 | 각 구성에 대한 정확도 |
| 손실 곡선 | 훈련 손실, 검증 손실 |
| 모델 분석 | 미적합 / 과적합 경향 |
| 정규화 효과 | λ 스윕 결과 |
| 최종 테스트 정확도 | 최고 모델로부터 |

**토론 프롬프트**
**보고서에 답변:**
1.  어떤 아키텍처가 가장 좋은 성능을 보였나요? 왜 그런가요?
2.  너비를 늘리는 것이 성능에 어떤 영향을 미치나요?
3.  깊이가 학습에 어떤 영향을 미치나요?
4.  λ가 너무 크거나 너무 작으면 어떻게 되나요?
5.  정규화 없이 과적합의 증거가 있나요?
6.  검증 정확도가 테스트 정확도와 상관관계가 있었나요?

**제출물**
*   **코드:** `mnist_full_manual_train.py`
*   **스크린샷/로그 내용:**
    *   에포크별 정확도 및 손실
    *   최종 테스트 정확도
*   **PDF 보고서 (2–5 페이지):**
    *   실험 설정
    *   표 및 그래프
    *   분석 및 토론

***

### **페이지 897**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 898**

**프로그래밍 과제 08: 실습: CNN 구현하기**

*   **목표:**
    *   이 과제에서는 **합성곱 신경망(CNN)을 처음부터 구현**하는 것을 연습할 것입니다.
    *   학생들은 합성곱, 풀링, 완전 연결 레이어가 어떻게 함께 작동하여 이미지를 분류하는지 배우게 됩니다.
*   **목표:**
    *   CNN의 구조와 순전파 과정을 이해합니다.
    *   NumPy만을 사용하여 합성곱 및 풀링 연산을 구현합니다.
    *   간단한 데이터셋(예: MNIST 또는 CIFAR-10)에서 모델을 훈련하고 평가합니다.

***

### **페이지 899**

**처음부터 CNN 구축하기 (MNIST)**

*   **1) 개요**
    *   딥러닝 프레임워크 없이 **NumPy만**을 사용하여 **합성곱 신경망(CNN)을 처음부터 구현**할 것입니다. 핵심 모듈(합성곱, 활성화, 풀링, 완전 연결, 손실, 옵티마이저)을 구축하고, 이를 모델로 쌓아 MNIST에서 훈련하고 결과를 분석합니다. 주요 목표는 속도보다는 **메커니즘(순전파 및 역전파)을 이해**하는 것입니다.
*   **2) 학습 목표**
    *   올바른 순전파/역전파를 가진 **Conv2D, ReLU, MaxPool2D, Flatten, Linear** 레이어를 구현합니다.
    *   **소프트맥스 교차 엔트로피** 손실과 간단한 옵티마이저(예: SGD)를 구현합니다.
    *   레이어들을 CNN으로 구성하고 MNIST에서 **훈련/평가**합니다.
    *   설정(필터, 커널 크기, 풀링, 학습률, 가중치 감쇠 등)을 변경하여 작은 **실험**을 실행하고 결과를 해석합니다.
*   **3) 데이터셋**
    *   **MNIST**와 **CIFAR-10**을 모두 사용하여 회색조 및 컬러 이미지에 대한 CNN 동작을 탐색합니다.

| 데이터셋 | 채널 | 이미지 크기 | 클래스 | 비고 |
| :--- | :--- | :--- | :--- | :--- |
| **MNIST** | 1 (회색조) | 28×28 | 10개 숫자 | 간단함; 디버깅에 좋음 |
| **CIFAR-10** | 3 (RGB 컬러) | 32×32 | 10개 객체 카테고리 | 컬러 + 더 복잡함 |

***

### **페이지 900**

**처음부터 CNN 구축하기 (MNIST)**

**4) 구현해야 할 것 (NumPy 전용)**
각각에 대해 **순전파**와 **역전파**를 구현합니다:
**1. Conv2D**
1.  입력: (N, C_in, H, W) → 출력: (N, C_out, H', W')
2.  하이퍼파라미터: `out_channels`, `kernel_size` (k×k), `stride=1`, `padding` ('same' = k//2 사용 가능).
3.  명확성을 위해 순진한 루프를 허용합니다. (선택 사항: 효율성을 위해 im2col 사용).
**2. 활성화**
1.  ReLU (필수). 선택적으로 Sigmoid/Tanh를 추가하여 비교.
**3. MaxPool2D**
1.  일반적: `kernel_size=2`, `stride=2`.
2.  역전파는 최대값 위치에만 그래디언트를 반환해야 합니다.
**4. Flatten**
**5. Linear (완전 연결)**
**6. 소프트맥스 교차 엔트로피 손실**
1.  수치적으로 안정적인 소프트맥스 (샘플별 최대값 빼기).
2.  배치에 대한 평균 손실과 로짓에 대한 그래디언트를 반환.
**7. 옵티마이저**
1.  학습률 η를 사용한 SGD.
2.  **가중치에만** 가중치 감쇠(L2)를 구현합니다 (편향에는 감쇠 없음).
**중요**: **그래디언트 계산(역전파)**을 **파라미터 업데이트(옵티마이저)**와 분리하여 유지하세요. `backward()` 내부에서 업데이트하지 마세요.

**5) 모델 구성 (기준선)**
작은 CNN을 구축합니다; 설정을 변경할 수 있습니다:
**기준선 예시**
Conv2D(1→8, k=3, padding='same') → ReLU → MaxPool2D(2)
Conv2D(8→16, k=3, padding='same') → ReLU → MaxPool2D(2)
Flatten → Linear(16*7*7 → 64) → ReLU → Linear(64 → 10)
*   `out_channels`, k를 변경하거나, 블록을 추가/제거하거나, 은닉 크기를 조정할 수 있지만 — **선택 사항을 문서화**하세요.

***

### **페이지 901**

**처음부터 CNN 구축하기 (MNIST)**

**6) 훈련 요구 사항**
*   **배칭**: 미니배치 SGD (예: 배치 크기 64 또는 128).
*   **에포크**: 수렴할 때까지 훈련 (예: 이 작은 네트워크에는 5–15 에포크가 일반적임).
*   **메트릭**: **에포크별 훈련 손실/정확도**와 **최종 테스트 정확도**를 보고합니다.
*   **목표**: 합리적인 CNN으로, **≥97% 테스트 정확도**가 예상됩니다. (더 낮으면 이유를 분석하세요.)
*   **결정성**: 재현성을 위해 랜덤 시드를 설정합니다.

**7) 실험 (설정 다양화)**
다음 중 **최소 두 가지**를 수행하고 결과(정확도, 손실 곡선, 훈련 행동)를 논의합니다:
*   Conv 레이어의 **#필터** 변경 (예: 8/16 vs 16/32).
*   **커널 크기** 변경 (3 vs 5).
*   **MaxPool 유/무** 시도 (또는 풀링 하나 vs 두 개).
*   **학습률** 변경 (예: 1e-1, 1e-2, 1e-3).
*   **가중치 감쇠** 추가 및 일반화 비교.
*   활성화 교체/삽입 (예: Tanh) 및 수렴 비교.

**8) 제출물**
다음을 포함해야 합니다:
**1. 코드** (수정 없이 실행 가능)
**2. 보고서**
1.  **모델 아키텍처** (작은 블록 다이어그램 또는 레이어 표).
2.  **훈련 곡선** (에포크별 손실/정확도).
3.  **최종 테스트 정확도** 및 혼동 행렬 또는 몇 가지 잘못 분류된 예시.
4.  **실험** (무엇을 변경했는지, 결과, 해석).
5.  **구현 노트** (설계 선택, 수치적 트릭, 수정된 버그).
6.  **재현성** (환경, 시드, 하이퍼파라미터).

***

### **페이지 902**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**1) PyTorch란 무엇인가?**
*   **PyTorch**는 다음과 같은 기능을 갖춘 Python **딥러닝 프레임워크**입니다:
    *   **텐서** (NumPy와 유사, CPU/GPU에서 실행)
    *   **자동 미분** (Autograd)
    *   **nn.Module & optim** (레이어, 손실, 옵티마이저)
    *   **torchvision** (데이터셋, 변환, 사전 훈련된 모델)
**왜 지금 사용하는가?** 여러분은 순전파/역전파를 직접 구축했습니다; PyTorch를 사용하면 다음을 할 수 있습니다:
*   그래디언트 배관이 아닌 **모델링 및 실험에 집중**
*   **더 큰 모델과 GPU로 확장**
*   표준 도구 사용 (DataLoader, 체크포인트, AMP 등)

**2) 설치 (하나의 경로 선택)**
**Conda (권장)**
```bash
# CPU 전용
conda create -n dl python=3.10 -y
conda activate dl
conda install pytorch torchvision torchaudio cpuonly -c pytorch -y

# 또는: CUDA (호환되는 NVIDIA GPU + 드라이버가 있는 경우)
conda create -n dl python=3.10 -y
conda activate dl
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```
**pip**
```bash
# CPU 전용
python -m venv dl && source dl/bin/activate # (Windows: dl\Scripts\activate)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# CUDA (시스템에서 지원되는 경우)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

***

### **페이지 903**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**3) 과제 개요**
이전 NumPy CNN을 **PyTorch**로 다시 구현한 다음, **MNIST**와 **CIFAR-10**에서 훈련/평가합니다.
**반드시 포함:**
*   **Conv → ReLU → Pool → … → Linear**를 사용한 모델 (합리적인 변형 OK)
*   **교차 엔트로피 손실, SGD 또는 Adam**
*   **DataLoaders, 변환, 훈련/검증/테스트** 루프
*   두 데이터셋 모두에 대한 **정확도 및 손실 곡선; 최종 테스트 정확도**
*   **간단한 비교**: MNIST vs CIFAR-10 (채널, 난이도, 수렴)

**4) 기준 아키텍처**
**MNIST (1×28×28)**
Conv(1→16, 3×3, padding=1) → ReLU → MaxPool(2)
Conv(16→32, 3×3, padding=1) → ReLU → MaxPool(2)
Flatten → Linear(32*7*7→128) → ReLU → Linear(128→10)
**CIFAR-10 (3×32×32)**
Conv(3→32, 3×3, padding=1) → ReLU → MaxPool(2)
Conv(32→64, 3×3, padding=1) → ReLU → MaxPool(2)
Flatten → Linear(64*8*8→256) → ReLU → Linear(256→10)
훈련이 너무 쉬우면 CIFAR-10의 채널을 늘릴 수 있습니다 (예: 32/64 → 64/128).

**5) 시작 코드 (PyTorch)**
`train_pytorch_cnn.py`로 저장하고 실행:
```bash
python train_pytorch_cnn.py --dataset mnist --epochs 8 --batch_size 128 --lr 0.01
python train_pytorch_cnn.py --dataset cifar10 --epochs 20 --batch_size 128 --lr 0.001
```

***

### **페이지 904**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: PyTorch로 간단한 CNN 모델(SmallCNN)을 정의하는 Python 코드. 입력 채널 수에 따라 다른 구조를 가지며, 분류기 부분은 첫 순전파 시 동적으로 생성됨)**

***

### **페이지 905**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: MNIST와 CIFAR-10 데이터셋에 대한 데이터 로더와 변환을 정의하는 Python 함수. 데이터셋에 따라 다른 정규화 값과 데이터 증강(CIFAR-10의 경우)을 적용함)**

***

### **페이지 906**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: PyTorch 모델을 위한 훈련(train_one_epoch) 및 평가(evaluate) 함수. 훈련 함수는 그래디언트 계산 및 파라미터 업데이트를 포함하며, 평가 함수는 그래디언트 계산 없이 실행됨)**

***

### **페이지 907**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: 명령줄 인자(데이터셋, 에포크, 배치 크기 등)를 파싱하고, 모델, 데이터 로더, 옵티마이저, 손실 함수를 설정한 후, 훈련 및 평가 루프를 실행하는 메인 Python 스크립트)**

***

### **페이지 908**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**6) 요구 사항 및 실험**
**필수**
*   **두 데이터셋 모두** 훈련 (MNIST & CIFAR-10).
*   에포크별 **훈련 손실/정확도**와 **테스트 정확도**를 기록.
*   **재현 가능한 시드** 사용.
*   두 데이터셋 모두에 대한 **손실/정확도 vs 에포크 플롯** 제출.

**실험 (≥2개 선택)**
*   **옵티마이저 교체** (SGD vs Adam).
*   **#필터** 또는 **은닉 크기** 변경.
*   **LR 값** 시도 (예: 1e-1/1e-2/1e-3) 및 비교.
*   CIFAR-10에 **데이터 증강** 유/무.
*   **드롭아웃** 또는 **배치 정규화** 추가 및 효과 논의.

**7) 제출물**
**1. 코드** (실행 가능): `train_pytorch_cnn.py` (및 모든 헬퍼)
**2. 보고서:**
1.  모델 다이어그램 (MNIST/CIFAR-10)
2.  훈련 곡선, 최종 정확도
3.  실험 결과 (≥2)
4.  간단한 MNIST vs CIFAR-10 비교 (채널, 난이도, 수렴)
5.  재현성 세부 정보 (환경, 시드, 하이퍼파라미터)

***

### **페이지 909**

**특징 맵 시각화**

**목표**
**CNN 필터가 무엇을 탐지하는지** 이해하기 위해, 중간 합성곱 레이어의 **특징 맵(활성화 출력)**을 시각화합니다.

**개념**
*   각 **합성곱 필터**는 특정 **패턴**(엣지, 질감, 색상, 모양 등)을 탐지하도록 학습합니다.
*   훈련 후, **활성화 맵**은 입력 이미지의 **어떤 영역**이 각 필터를 활성화하는지 보여줍니다.
*   초기 레이어 → 저수준 특징 (엣지, 블롭)
    나중 레이어 → 고수준 추상적 모양 (숫자, 객체 부분)

**과제 지침**
**1. 훈련된 모델 하나 선택**
1.  훈련된 PyTorch CNN (MNIST 또는 CIFAR-10)을 사용합니다.
2.  저장된 가중치를 로드하거나 훈련 직후에 시각화합니다.
**2. 하나 또는 두 개의 입력 이미지 선택**
1.  MNIST의 경우: 숫자 하나 (예: "5")
2.  CIFAR-10의 경우: 샘플 하나 (예: "고양이")
**3. 순전파만** 이미지를 통과시키고, 초기 Conv 레이어에서 출력을 추출합니다.

**(코드: 훈련된 모델의 첫 번째 합성곱 레이어 이후의 특징 맵을 추출하는 PyTorch 코드)**

***

### **페이지 910**

**특징 맵 시각화**

**4. 시각화**

**(코드: Matplotlib을 사용하여 추출된 특징 맵(활성화) 중 일부를 시각화하는 Python 코드)**

**5. 해석**
*   어떤 패턴이 보이는지 설명합니다 (예: 수직 엣지, 수평 엣지, 블롭 영역).
*   MNIST vs CIFAR-10 특징 맵 비교:
    *   "MNIST 필터는 획 엣지를 강조하고; CIFAR-10 필터는 색상 블롭과 질감을 포착합니다."

**6. (선택 사항 고급) 더 깊은 레이어 활성화 시각화 (2번째 conv).**
특징 맵이 어떻게 더 **추상적**이거나 **지역화**되는지 비교합니다.

**7. 보고서 제출물 (보고서에 추가)**
**포함 내용:**
*   사용한 입력 이미지.
*   최소 하나의 Conv 레이어의 특징 맵 시각화.
*   필터가 무엇을 탐지하는 것처럼 보이는지에 대한 3–5 문장 해석.
*   (선택 사항) MNIST vs CIFAR-10 시각적 패턴 비교.

***

### **페이지 911**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 912**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**목표**
자신만의 자동 미분 엔진을 사용하여 MNIST에 대한 **완전한 훈련 파이프라인**을 구축할 것입니다:
*   데이터를 **훈련 / 검증 / 테스트**로 분할
*   2계층 (또는 더 깊은) 신경망 훈련
*   **다음으로 실험:**
    *   **완전 연결 레이어 수**
    *   **은닉 레이어 너비 (노드)**
    *   **L2 정규화 강도 λ**
*   분류 **정확도** 평가
*   결과 분석 및 토론
**PyTorch / TensorFlow 자동 미분 사용 금지.**
MNIST를 원시 데이터 소스로만 사용하세요. 모든 순전파/역전파는 여러분의 코드여야 합니다.

**과제**
**1. MNIST 로드 및 전처리**
*   각 이미지를 784차원 벡터로 펼치기
*   픽셀 값을로 정규화
*   **다음으로 분할:**
    train: 1,000 ~ 50,000 샘플 (부분 집합 가능)
    val: 100 ~ 10,000 샘플 (부분 집합 가능)
    test: 100 ~ 10,000 샘플 (부분 집합 가능)

sklearn 유틸리티는 훈련이 아닌 **분할에만** 사용할 수 있습니다.

***

### **페이지 913**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**2. 모델 변형 구축**
**(a) 기본 모델**
784 → H → 10
ReLU → Softmax

**(b) 레이어 수 변경**
예시:
784 → H → 10
784 → H → H → 10
784 → H → H → H → 10

**(c) 은닉 유닛 H 변경**
제안된 값:
H ∈ {10, 50, 100, 200}

**3. L2 가중치 정규화 게이트 구현**
`L2RegularizationGate` 추가:
L_reg = λ Σᵢ ||Wᵢ||²

**총 손실:**
L = L_crossentropy + Σ_layers L_reg

*   **손실 및 정규화 항을 통한 역전파**
*   **λ 값 스윕:**
    λ ∈ {0, 0.001, 0.01, 0.1}

***

### **페이지 914**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**4. 훈련 루프**
**각 구성에 대해:**
```
for epoch in 1..E:
    각 훈련 미니배치에 대해 순전파
    역전파
    SGD 업데이트
검증 세트에서 평가
```
**하이퍼파라미터 (권장):**
미니 배치 크기 = 1 (또는 64 또는 128)
에포크 = 3–5 (최소)
학습률 = 0.1 (또는 조정)
**훈련 데이터를 섞을 수 있습니다**
**원한다면 검증 세트 최고 모델에서 중지 (조기 종료 선택 사항)**

**5. 평가**
**훈련 후:**
*   테스트 세트에서 **정확도 계산**
*   혼동 행렬 출력 (선택 사항)
*   손실 곡선 및 검증 정확도 곡선 플롯 (선택 사항)

***

### **페이지 915**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**출력 샘플 (예시)**
```
=== RUN {'depth': 2, 'hidden_dim': 64, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.3652 val_acc=0.834
[epoch 2] train_loss/step_avg=0.8717 val_acc=0.871
[TEST] depth=2, hidden=64, lambda=0.0 --> acc=0.862

=== RUN {'depth': 2, 'hidden_dim': 64, 'lam': 0.01, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.4229 val_acc=0.842
[epoch 2] train_loss/step_avg=0.9381 val_acc=0.878
[TEST] depth=2, hidden=64, lambda=0.01 --> acc=0.872

=== RUN {'depth': 3, 'hidden_dim': 64, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.5023 val_acc=0.812
[epoch 2] train_loss/step_avg=0.9546 val_acc=0.861
[TEST] depth=3, hidden=64, lambda=0.0 --> acc=0.853

=== RUN {'depth': 2, 'hidden_dim': 128, 'lam': 0.0, 'train_n': 2000, 'val_n': 500, 'test_n': 1000, 'epochs': 2, 'lr': 0.1} ===
[epoch 1] train_loss/step_avg=1.2998 val_acc=0.846
[epoch 2] train_loss/step_avg=0.8135 val_acc=0.882
[TEST] depth=2, hidden=128, lambda=0.0 --> acc=0.879
```

***

### **페이지 916**

**과제 — MNIST에서 수동 신경망 훈련 및 평가 (전체 파이프라인)**

**6. 보고서 필수 사항**

| 섹션 | 설명 |
| :--- | :--- |
| 실험 설정 | 아키텍처, 하이퍼파라미터, 데이터셋 분할 |
| 결과 표 | 각 구성에 대한 정확도 |
| 손실 곡선 | 훈련 손실, 검증 손실 |
| 모델 분석 | 미적합 / 과적합 경향 |
| 정규화 효과 | λ 스윕 결과 |
| 최종 테스트 정확도 | 최고 모델로부터 |

**토론 프롬프트**
**보고서에 답변:**
1.  어떤 아키텍처가 가장 좋은 성능을 보였나요? 왜 그런가요?
2.  너비를 늘리는 것이 성능에 어떤 영향을 미치나요?
3.  깊이가 학습에 어떤 영향을 미치나요?
4.  λ가 너무 크거나 너무 작으면 어떻게 되나요?
5.  정규화 없이 과적합의 증거가 있나요?
6.  검증 정확도가 테스트 정확도와 상관관계가 있었나요?

**제출물**
*   **코드:** `mnist_full_manual_train.py`
*   **스크린샷/로그 내용:**
    *   에포크별 정확도 및 손실
    *   최종 테스트 정확도
*   **PDF 보고서 (2–5 페이지):**
    *   실험 설정
    *   표 및 그래프
    *   분석 및 토론

***

### **페이지 917**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.

***

### **페이지 918**

**프로그래밍 과제 08: 실습: CNN 구현하기**

*   **목표:**
    *   이 과제에서는 **합성곱 신경망(CNN)을 처음부터 구현**하는 것을 연습할 것입니다.
    *   학생들은 합성곱, 풀링, 완전 연결 레이어가 어떻게 함께 작동하여 이미지를 분류하는지 배우게 됩니다.
*   **목표:**
    *   CNN의 구조와 순전파 과정을 이해합니다.
    *   NumPy만을 사용하여 합성곱 및 풀링 연산을 구현합니다.
    *   간단한 데이터셋(예: MNIST 또는 CIFAR-10)에서 모델을 훈련하고 평가합니다.

***

### **페이지 919**

**처음부터 CNN 구축하기 (MNIST)**

*   **1) 개요**
    *   딥러닝 프레임워크 없이 **NumPy만**을 사용하여 **합성곱 신경망(CNN)을 처음부터 구현**할 것입니다. 핵심 모듈(합성곱, 활성화, 풀링, 완전 연결, 손실, 옵티마이저)을 구축하고, 이를 모델로 쌓아 MNIST에서 훈련하고 결과를 분석합니다. 주요 목표는 속도보다는 **메커니즘(순전파 및 역전파)을 이해**하는 것입니다.
*   **2) 학습 목표**
    *   올바른 순전파/역전파를 가진 **Conv2D, ReLU, MaxPool2D, Flatten, Linear** 레이어를 구현합니다.
    *   **소프트맥스 교차 엔트로피** 손실과 간단한 옵티마이저(예: SGD)를 구현합니다.
    *   레이어들을 CNN으로 구성하고 MNIST에서 **훈련/평가**합니다.
    *   설정(필터, 커널 크기, 풀링, 학습률, 가중치 감쇠 등)을 변경하여 작은 **실험**을 실행하고 결과를 해석합니다.
*   **3) 데이터셋**
    *   **MNIST**와 **CIFAR-10**을 모두 사용하여 회색조 및 컬러 이미지에 대한 CNN 동작을 탐색합니다.

| 데이터셋 | 채널 | 이미지 크기 | 클래스 | 비고 |
| :--- | :--- | :--- | :--- | :--- |
| **MNIST** | 1 (회색조) | 28×28 | 10개 숫자 | 간단함; 디버깅에 좋음 |
| **CIFAR-10** | 3 (RGB 컬러) | 32×32 | 10개 객체 카테고리 | 컬러 + 더 복잡함 |

***

### **페이지 920**

**처음부터 CNN 구축하기 (MNIST)**

**4) 구현해야 할 것 (NumPy 전용)**
각각에 대해 **순전파**와 **역전파**를 구현합니다:
**1. Conv2D**
1.  입력: (N, C_in, H, W) → 출력: (N, C_out, H', W')
2.  하이퍼파라미터: `out_channels`, `kernel_size` (k×k), `stride=1`, `padding` ('same' = k//2 사용 가능).
3.  명확성을 위해 순진한 루프를 허용합니다. (선택 사항: 효율성을 위해 im2col 사용).
**2. 활성화**
1.  ReLU (필수). 선택적으로 Sigmoid/Tanh를 추가하여 비교.
**3. MaxPool2D**
1.  일반적: `kernel_size=2`, `stride=2`.
2.  역전파는 최대값 위치에만 그래디언트를 반환해야 합니다.
**4. Flatten**
**5. Linear (완전 연결)**
**6. 소프트맥스 교차 엔트로피 손실**
1.  수치적으로 안정적인 소프트맥스 (샘플별 최대값 빼기).
2.  배치에 대한 평균 손실과 로짓에 대한 그래디언트를 반환.
**7. 옵티마이저**
1.  학습률 η를 사용한 SGD.
2.  **가중치에만** 가중치 감쇠(L2)를 구현합니다 (편향에는 감쇠 없음).
**중요**: **그래디언트 계산(역전파)**을 **파라미터 업데이트(옵티마이저)**와 분리하여 유지하세요. `backward()` 내부에서 업데이트하지 마세요.

**5) 모델 구성 (기준선)**
작은 CNN을 구축합니다; 설정을 변경할 수 있습니다:
**기준선 예시**
Conv2D(1→8, k=3, padding='same') → ReLU → MaxPool2D(2)
Conv2D(8→16, k=3, padding='same') → ReLU → MaxPool2D(2)
Flatten → Linear(16*7*7 → 64) → ReLU → Linear(64 → 10)
*   `out_channels`, k를 변경하거나, 블록을 추가/제거하거나, 은닉 크기를 조정할 수 있지만 — **선택 사항을 문서화**하세요.

***

### **페이지 921**

**처음부터 CNN 구축하기 (MNIST)**

**6) 훈련 요구 사항**
*   **배칭**: 미니배치 SGD (예: 배치 크기 64 또는 128).
*   **에포크**: 수렴할 때까지 훈련 (예: 이 작은 네트워크에는 5–15 에포크가 일반적임).
*   **메트릭**: **에포크별 훈련 손실/정확도**와 **최종 테스트 정확도**를 보고합니다.
*   **목표**: 합리적인 CNN으로, **≥97% 테스트 정확도**가 예상됩니다. (더 낮으면 이유를 분석하세요.)
*   **결정성**: 재현성을 위해 랜덤 시드를 설정합니다.

**7) 실험 (설정 다양화)**
다음 중 **최소 두 가지**를 수행하고 결과(정확도, 손실 곡선, 훈련 행동)를 논의합니다:
*   Conv 레이어의 **#필터** 변경 (예: 8/16 vs 16/32).
*   **커널 크기** 변경 (3 vs 5).
*   **MaxPool 유/무** 시도 (또는 풀링 하나 vs 두 개).
*   **학습률** 변경 (예: 1e-1, 1e-2, 1e-3).
*   **가중치 감쇠** 추가 및 일반화 비교.
*   활성화 교체/삽입 (예: Tanh) 및 수렴 비교.

**8) 제출물**
다음을 포함해야 합니다:
**1. 코드** (수정 없이 실행 가능)
**2. 보고서**
1.  **모델 아키텍처** (작은 블록 다이어그램 또는 레이어 표).
2.  **훈련 곡선** (에포크별 손실/정확도).
3.  **최종 테스트 정확도** 및 혼동 행렬 또는 몇 가지 잘못 분류된 예시.
4.  **실험** (무엇을 변경했는지, 결과, 해석).
5.  **구현 노트** (설계 선택, 수치적 트릭, 수정된 버그).
6.  **재현성** (환경, 시드, 하이퍼파라미터).

***

### **페이지 922**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**1) PyTorch란 무엇인가?**
*   **PyTorch**는 다음과 같은 기능을 갖춘 Python **딥러닝 프레임워크**입니다:
    *   **텐서** (NumPy와 유사, CPU/GPU에서 실행)
    *   **자동 미분** (Autograd)
    *   **nn.Module & optim** (레이어, 손실, 옵티마이저)
    *   **torchvision** (데이터셋, 변환, 사전 훈련된 모델)
**왜 지금 사용하는가?** 여러분은 순전파/역전파를 직접 구축했습니다; PyTorch를 사용하면 다음을 할 수 있습니다:
*   그래디언트 배관이 아닌 **모델링 및 실험에 집중**
*   **더 큰 모델과 GPU로 확장**
*   표준 도구 사용 (DataLoader, 체크포인트, AMP 등)

**2) 설치 (하나의 경로 선택)**
**Conda (권장)**
```bash
# CPU 전용
conda create -n dl python=3.10 -y
conda activate dl
conda install pytorch torchvision torchaudio cpuonly -c pytorch -y

# 또는: CUDA (호환되는 NVIDIA GPU + 드라이버가 있는 경우)
conda create -n dl python=3.10 -y
conda activate dl
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```
**pip**
```bash
# CPU 전용
python -m venv dl && source dl/bin/activate # (Windows: dl\Scripts\activate)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# CUDA (시스템에서 지원되는 경우)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

***

### **페이지 923**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**3) 과제 개요**
이전 NumPy CNN을 **PyTorch**로 다시 구현한 다음, **MNIST**와 **CIFAR-10**에서 훈련/평가합니다.
**반드시 포함:**
*   **Conv → ReLU → Pool → … → Linear**를 사용한 모델 (합리적인 변형 OK)
*   **교차 엔트로피 손실, SGD 또는 Adam**
*   **DataLoaders, 변환, 훈련/검증/테스트** 루프
*   두 데이터셋 모두에 대한 **정확도 및 손실 곡선; 최종 테스트 정확도**
*   **간단한 비교**: MNIST vs CIFAR-10 (채널, 난이도, 수렴)

**4) 기준 아키텍처**
**MNIST (1×28×28)**
Conv(1→16, 3×3, padding=1) → ReLU → MaxPool(2)
Conv(16→32, 3×3, padding=1) → ReLU → MaxPool(2)
Flatten → Linear(32*7*7→128) → ReLU → Linear(128→10)
**CIFAR-10 (3×32×32)**
Conv(3→32, 3×3, padding=1) → ReLU → MaxPool(2)
Conv(32→64, 3×3, padding=1) → ReLU → MaxPool(2)
Flatten → Linear(64*8*8→256) → ReLU → Linear(256→10)
훈련이 너무 쉬우면 CIFAR-10의 채널을 늘릴 수 있습니다 (예: 32/64 → 64/128).

**5) 시작 코드 (PyTorch)**
`train_pytorch_cnn.py`로 저장하고 실행:
```bash
python train_pytorch_cnn.py --dataset mnist --epochs 8 --batch_size 128 --lr 0.01
python train_pytorch_cnn.py --dataset cifar10 --epochs 20 --batch_size 128 --lr 0.001
```

***

### **페이지 924**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: PyTorch로 간단한 CNN 모델(SmallCNN)을 정의하는 Python 코드. 입력 채널 수에 따라 다른 구조를 가지며, 분류기 부분은 첫 순전파 시 동적으로 생성됨)**

***

### **페이지 925**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: MNIST와 CIFAR-10 데이터셋에 대한 데이터 로더와 변환을 정의하는 Python 함수. 데이터셋에 따라 다른 정규화 값과 데이터 증강(CIFAR-10의 경우)을 적용함)**

***

### **페이지 926**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: PyTorch 모델을 위한 훈련(train_one_epoch) 및 평가(evaluate) 함수. 훈련 함수는 그래디언트 계산 및 파라미터 업데이트를 포함하며, 평가 함수는 그래디언트 계산 없이 실행됨)**

***

### **페이지 927**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**(코드: 명령줄 인자(데이터셋, 에포크, 배치 크기 등)를 파싱하고, 모델, 데이터 로더, 옵티마이저, 손실 함수를 설정한 후, 훈련 및 평가 루프를 실행하는 메인 Python 스크립트)**

***

### **페이지 928**

**PyTorch로 CNN 재구축하기 (MNIST & CIFAR-10)**

**6) 요구 사항 및 실험**
**필수**
*   **두 데이터셋 모두** 훈련 (MNIST & CIFAR-10).
*   에포크별 **훈련 손실/정확도**와 **테스트 정확도**를 기록.
*   **재현 가능한 시드** 사용.
*   두 데이터셋 모두에 대한 **손실/정확도 vs 에포크 플롯** 제출.

**실험 (≥2개 선택)**
*   **옵티마이저 교체** (SGD vs Adam).
*   **#필터** 또는 **은닉 크기** 변경.
*   **LR 값** 시도 (예: 1e-1/1e-2/1e-3) 및 비교.
*   CIFAR-10에 **데이터 증강** 유/무.
*   **드롭아웃** 또는 **배치 정규화** 추가 및 효과 논의.

**7) 제출물**
**1. 코드** (실행 가능): `train_pytorch_cnn.py` (및 모든 헬퍼)
**2. 보고서:**
1.  모델 다이어그램 (MNIST/CIFAR-10)
2.  훈련 곡선, 최종 정확도
3.  실험 결과 (≥2)
4.  간단한 MNIST vs CIFAR-10 비교 (채널, 난이도, 수렴)
5.  재현성 세부 정보 (환경, 시드, 하이퍼파라미터)

***

### **페이지 929**

**특징 맵 시각화**

**목표**
**CNN 필터가 무엇을 탐지하는지** 이해하기 위해, 중간 합성곱 레이어의 **특징 맵(활성화 출력)**을 시각화합니다.

**개념**
*   각 **합성곱 필터**는 특정 **패턴**(엣지, 질감, 색상, 모양 등)을 탐지하도록 학습합니다.
*   훈련 후, **활성화 맵**은 입력 이미지의 **어떤 영역**이 각 필터를 활성화하는지 보여줍니다.
*   초기 레이어 → 저수준 특징 (엣지, 블롭)
    나중 레이어 → 고수준 추상적 모양 (숫자, 객체 부분)

**과제 지침**
**1. 훈련된 모델 하나 선택**
1.  훈련된 PyTorch CNN (MNIST 또는 CIFAR-10)을 사용합니다.
2.  저장된 가중치를 로드하거나 훈련 직후에 시각화합니다.
**2. 하나 또는 두 개의 입력 이미지 선택**
1.  MNIST의 경우: 숫자 하나 (예: "5")
2.  CIFAR-10의 경우: 샘플 하나 (예: "고양이")
**3. 순전파만** 이미지를 통과시키고, 초기 Conv 레이어에서 출력을 추출합니다.

**(코드: 훈련된 모델의 첫 번째 합성곱 레이어 이후의 특징 맵을 추출하는 PyTorch 코드)**

***

### **페이지 930**

**특징 맵 시각화**

**4. 시각화**

**(코드: Matplotlib을 사용하여 추출된 특징 맵(활성화) 중 일부를 시각화하는 Python 코드)**

**5. 해석**
*   어떤 패턴이 보이는지 설명합니다 (예: 수직 엣지, 수평 엣지, 블롭 영역).
*   MNIST vs CIFAR-10 특징 맵 비교:
    *   "MNIST 필터는 획 엣지를 강조하고; CIFAR-10 필터는 색상 블롭과 질감을 포착합니다."

**6. (선택 사항 고급) 더 깊은 레이어 활성화 시각화 (2번째 conv).**
특징 맵이 어떻게 더 **추상적**이거나 **지역화**되는지 비교합니다.

**7. 보고서 제출물 (보고서에 추가)**
**포함 내용:**
*   사용한 입력 이미지.
*   최소 하나의 Conv 레이어의 특징 맵 시각화.
*   필터가 무엇을 탐지하는 것처럼 보이는지에 대한 3–5 문장 해석.
*   (선택 사항) MNIST vs CIFAR-10 시각적 패턴 비교.

***

### **페이지 931**

**제출 가이드라인**

*   **제출 내용**
    *   다음 사항을 제출해야 합니다:
*   **모든 코드 파일**
    *   Python 스크립트(.py) 또는 Jupyter 노트북(.ipynb)
    *   코드는 **수정 없이** 실행되어야 합니다.
*   **단일 PDF 보고서**
    *   PDF에는 다음이 포함되어야 합니다:
        *   모든 스크린샷
        *   표와 그림
        *   실험 결과 및 분석
        *   각 항목은 명확하게 레이블이 지정되고 읽기 쉬워야 합니다.
*   **압축 파일**
    *   모든 파일을 **하나의 압축 파일(.zip)**로 제출합니다.
    *   파일 이름 형식: **Assignment\_\<학번\>\_\<이름\>.zip**
*   *파일 이름 규칙을 준수하지 않으면 감점 또는 거부될 수 있습니다.*
*   **보고서 요구 사항**
    *   보고서는 이전에 지시된 모든 것을 포함해야 하며, 이에 국한되지 않습니다:
        *   코드 실행 스크린샷 (에디터 + 출력 보임)
        *   질문당 여러 테스트 케이스 및 결과
        *   각 출력이 발생하는 이유에 대한 설명
        *   요청된 경우 표/플롯 및 분석
*   **감점 정책**
    *   누락되거나 불분명한 항목은 감점될 수 있습니다.
    *   잘못된 제출 형식이나 파일 이름은 **감점 또는 거부**로 이어질 수 있습니다.
