## 🔍 강의 요약
이번 강의에서는 머신러닝을 위한 텍스트 처리(Text Processing)의 전반적인 과정을 다룹니다. 텍스트 데이터를 컴퓨터가 이해할 수 있는 숫자 형태(벡터)로 변환하는 것의 중요성부터 시작하여, 토큰화, 정규화 등 핵심적인 전처리 단계를 학습합니다. 또한, 텍스트를 벡터로 표현하는 대표적인 방법인 **Bag-of-Words**와 단어의 중요도를 측정하는 **TF-IDF 가중치**에 대해 심도 있게 알아봅니다. 마지막으로, 벡터 공간 모델(Vector Space Model)을 기반으로 문서 간의 유사성을 측정하는 **코사인 유사도(Cosine Similarity)**의 원리와 계산 방법을 학습합니다.

## 💡 핵심 포인트
-   **텍스트의 벡터화**: 머신러닝 알고리즘이 처리할 수 있도록 텍스트를 숫자 형태의 벡터로 변환하는 과정은 필수적입니다.
-   **전처리(Preprocessing)**: 텍스트 분석의 정확도를 높이기 위해 토큰화(Tokenization), 불용어(Stop words) 제거, 정규화(Normalization) 등의 전처리 과정이 중요합니다.
-   **TF-IDF (Term Frequency-Inverse Document Frequency)**: 문서 내 단어의 빈도(TF)와 전체 문서에서 단어의 희소성(IDF)을 조합하여 단어의 중요도를 나타내는 효과적인 가중치 계산 방식입니다.
-   **벡터 공간 모델 (Vector Space Model)**: 문서를 단어(Term)를 축으로 하는 고차원 벡터 공간의 한 점으로 표현하는 모델입니다.
-   **코사인 유사도 (Cosine Similarity)**: 두 벡터 사이의 각도를 이용하여 문서 간의 내용적 유사성을 측정하는 방법으로, 문서의 길이에 영향을 받지 않는 장점이 있습니다.

## 세부 내용 📚
### 1️⃣ 텍스트 분석(Text Analysis)의 개요
텍스트 분석은 통계 및 자연어 처리(NLP) 기술을 사용하여 텍스트 데이터에서 유용한 정보를 추출하고 이해하는 과정입니다.

-   **주요 기법**
    -   **단어 빈도 분포 분석 (Word frequency distribution analysis)**: 텍스트에 어떤 단어가 얼마나 자주 등장하는지 분석합니다.
    -   **패턴 인식 (Pattern recognition)**: 전화번호, 이메일 주소 등 정형화된 패턴을 인식합니다.
    -   **개체-관계 추출 (Entity-relationship extraction)**: 텍스트 내의 개체(사람, 기관 등)와 그들 간의 관계를 추출합니다.
-   **주요 응용 분야**
    -   인터넷 검색 (Internet Search)
    -   스팸 필터링 (Spam filtering)
    -   감성 분석 (Sentiment analysis)
    -   기계 번역 (Machine translation)

### 2️⃣ 텍스트 전처리(Text Preprocessing)
텍스트를 분석 모델에 사용하기 전에 정제하는 과정입니다.
-   **토큰화 (Tokenization)**
    -   긴 텍스트를 문단, 문장, 단어 등 의미 있는 작은 단위(토큰)로 나누는 작업입니다.
    -   이 과정에서 문장 부호, 특수 문자 등을 제거하고, 의미에 큰 영향을 주지 않는 **불용어(Stop words)** (예: 'a', 'the', 'in', 'of' 등)를 제거합니다.
-   **정규화 (Normalization)**
    -   텍스트의 모든 요소를 공통된 기준으로 맞추는 작업입니다.
    -   **어간 추출 (Stemming)**: 단어의 접두사나 접미사를 제거하여 어간(stem)을 추출합니다. (예: `running` → `run`, `ended` → `end`)
    -   **표제어 추출 (Lemmatization)**: 단어를 문법적 형태를 고려하여 기본형(canonical form)으로 변환합니다. (예: `better` → `good`, `ran` → `run`)
    -   이 외에 모든 문자를 소문자나 대문자로 통일하는 과정도 포함됩니다.

### 3️⃣ 문서 벡터 공간 모델 (Document Vector-Space Model)
텍스트 문서를 수학적으로 표현하기 위한 핵심 모델입니다.
-   **Bag-of-Words (BoW)**
    -   문서에 포함된 단어들의 순서나 문법 구조는 무시하고, 오직 단어들의 출현 빈도에만 집중하여 문서를 벡터로 표현하는 방식입니다.
    -   각 단어가 벡터의 한 차원(축)이 되며, 해당 단어의 빈도수가 그 차원의 값이 됩니다.
-   **TF-IDF (Term Frequency-Inverse Document Frequency)**
    -   BoW의 한계를 보완하기 위해 단어의 중요도를 가중치로 부여하는 방식입니다.
    -   **TF (Term Frequency)**: **단어 빈도**. 한 문서 내에서 특정 단어가 얼마나 자주 등장하는지를 나타냅니다. 관련성이 높을수록 값이 커지지만, 그 중요도가 비례하여 증가하지는 않으므로 보통 `log`를 취해 값을 조정합니다 (`1 + log(tf)`).
    -   **IDF (Inverse Document Frequency)**: **역문서 빈도**. 특정 단어가 전체 문서 집합에서 얼마나 드물게 나타나는지를 나타냅니다. 희귀한 단어일수록 정보 가치가 높다고 판단하여 높은 가중치를 부여합니다.
        -   `idf_t = log(N / df_t)` (N: 전체 문서 수, df_t: 단어 t가 포함된 문서 수)
    -   **TF-IDF 가중치**: `w = TF * IDF`. TF와 IDF 값을 곱하여 최종적인 단어의 중요도를 계산합니다.

### 4️⃣ 코사인 유사도(Cosine Similarity)
벡터 공간 모델에서 두 문서(또는 쿼리와 문서) 벡터 간의 유사성을 측정하는 방법입니다.
-   **원리**: 두 벡터가 이루는 각도의 코사인 값을 이용하여 유사도를 계산합니다.
    -   두 벡터의 방향이 완전히 같으면 각도가 0°이고 코사인 값은 1이 되어 최대 유사도를 가집니다.
    -   두 벡터의 방향이 직각(90°)이면 코사인 값은 0, 방향이 정반대(180°)이면 -1이 됩니다.
-   **장점**: 문서의 길이에 따라 벡터의 크기가 달라져도, 벡터 간의 방향(내용의 유사성)을 기준으로 측정하므로 길이에 덜 민감합니다.
-   **계산**:
    -   `cos(q, d) = (q · d) / (|q| * |d|)`
    -   분자는 쿼리(q)와 문서(d) 벡터의 내적(dot product)이고, 분모는 각 벡터의 크기(L2 norm)의 곱입니다.
    -   정확한 계산을 위해 각 벡터를 자신의 길이로 나누어 단위 벡터로 만드는 **벡터 정규화(Vector Normalization)** 과정이 필요합니다.

## 오늘의 연습문제 📝
-   **문제 1**: 어간 추출(Stemming)과 표제어 추출(Lemmatization)의 차이점을 설명하고, "studies", "studying"이라는 단어에 각각 두 기법을 적용했을 때 예상되는 결과를 작성해 보세요.
-   **문제 2**: TF-IDF 가중치 계산에서 IDF(역문서 빈도)가 왜 중요한지 설명해 보세요. 모든 문서에 공통적으로 나타나는 단어(예: 'the')의 IDF 값은 어떻게 될까요?
-   **문제 3**: 다음 두 개의 짧은 문서 A와 B가 있을 때, 코사인 유사도를 사용하여 두 문서의 유사성을 어떻게 계산할 수 있는지 그 과정을 설명해 보세요. (계산은 생략하고 과정만 설명)
    -   문서 A: "sun is shining"
    -   문서 B: "the weather is sweet and the sun is sweet"
