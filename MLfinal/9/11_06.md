## 🔍 강의 요약
이번 강의에서는 머신러닝의 한 분야인 강화학습(Reinforcement Learning)의 기본 개념을 다룹니다. 에이전트(Agent)가 환경(Environment)과 상호작용하며 보상(Reward)을 통해 최적의 행동을 학습하는 과정을 설명합니다. 핵심 알고리즘으로 Q-러닝(Q-learning)을 소개하고, R(보상) 행렬과 Q(품질) 행렬을 이용한 학습 과정을 '5개의 방 탈출하기' 예제를 통해 단계별로 상세히 살펴봅니다. 또한, 탐험(Exploration)과 활용(Exploitation)의 균형 문제, 그리고 강화학습의 수학적 모델인 마르코프 결정 과정(MDP)에 대해서도 학습합니다.

## 💡 핵심 포인트
- **강화학습(Reinforcement Learning)의 원리**: 에이전트가 환경 내에서 행동하고, 그 결과로 받는 보상을 통해 최적의 정책(Policy)을 스스로 학습하는 방식입니다.
- **Q-러닝(Q-learning) 알고리즘**: R(보상) 행렬과 Q(품질) 행렬을 사용하여 특정 상태에서 특정 행동을 했을 때의 가치를 학습하고 업데이트하는 강화학습 알고리즘입니다.
- **Q-러닝 핵심 공식**: `Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]`
- **탐험(Exploration)과 활용(Exploitation)**: 현재까지의 정보로 최선의 보상을 얻는 '활용'과, 더 나은 보상을 찾기 위해 새로운 행동을 시도하는 '탐험' 사이의 균형이 중요합니다.
- **MDP (Markov Decision Process)**: 상태(State), 행동(Action), 보상(Reward), 전이 확률(Transition Probability)을 이용해 강화학습 문제를 수학적으로 모델링하는 프레임워크입니다.

## 세부 내용 📚
### 1️⃣ 강화학습(Reinforcement Learning)이란? 🤖
- **정의**: 에이전트가 환경과 상호작용하며 '보상'을 최대화하는 방향으로 행동을 학습하는 머신러닝의 한 분야입니다.
- **학습 과정**:
    1.  **에이전트(Agent)**가 특정 **상태(State)**에서 **행동(Action)**을 취합니다.
    2.  **환경(Environment)**은 에이전트에게 **보상(Reward)**과 다음 상태 정보를 제공합니다.
    3.  에이전트는 이 보상을 바탕으로 더 나은 행동을 하도록 학습합니다.
- **특징**: 정답이 주어진 지도학습과 달리, 에이전트는 어떤 행동이 최선인지 직접 배우며 시행착오를 겪습니다.
- **응용 분야**:
    - 🎮 게임 (AlphaGo)
    - 🚗 자율 주행 및 교통 신호 제어
    - 🤖 로보틱스
    - 📈 금융 (자동 주식 거래)
    - 🛒 개인화 추천 시스템

### 2️⃣ Q-러닝(Q-learning) 알고리즘 🧠
- **개요**: 1989년 Christopher Watkins가 소개한 강화학습의 대표적인 알고리즘입니다.
- **핵심 요소**:
    - **R (Reward) 행렬**: 각 상태에서 특정 행동을 했을 때 즉시 얻는 보상을 정의한 행렬. 이동이 불가능한 경우 -1, 목표 도달 시 100과 같이 설정됩니다.
    - **Q (Quality) 행렬**: 에이전트의 '두뇌' 또는 '기억'에 해당하며, 특정 상태에서 특정 행동을 했을 때 미래에 받을 것으로 예상되는 보상의 총합(가치)을 저장합니다. 처음에는 0으로 초기화됩니다.
    - **에피소드(Episode)**: 시작 상태에서 목표 상태(Goal State)에 도달하기까지의 한 번의 시도를 의미합니다.
- **학습 메커니즘**:
    - 에이전트는 여러 에피소드를 반복하며 R 행렬을 참고해 환경을 탐색합니다.
    - 행동할 때마다 Q-러닝 공식을 사용해 Q 행렬 값을 업데이트하며 학습을 진행합니다.
    - `Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]`
        - **Gamma (감마)**: 할인 인자(Discount Factor, 0 <= Gamma < 1). 미래 보상의 가치를 얼마나 반영할지 결정합니다.
            - `Gamma`가 0에 가까우면: 즉각적인 보상을 중시합니다.
            - `Gamma`가 1에 가까우면: 미래의 보상을 더 중요하게 고려합니다.

### 3️⃣ Q-러닝 예제: 5개의 방 탈출하기 🚪
- **문제 설정**: 5개의 방(0~4)과 외부(5)가 있는 건물에서, 에이전트가 임의의 방에서 시작해 외부(목표 상태)로 나가는 최적의 경로를 찾는 문제입니다.
- **학습 과정 (Walkthrough)**:
    1.  **초기화**: R 행렬을 설정하고, Q 행렬은 모두 0으로 초기화합니다. (Gamma = 0.8)
    2.  **에피소드 1**:
        - `상태 1`에서 시작 → `상태 5`(목표)로 이동.
        - `Q(1, 5)` 값 계산: `R(1, 5) + 0.8 * Max[Q(5, actions)]` = `100 + 0.8 * 0` = `100`.
        - Q 행렬의 `(1, 5)` 위치에 100을 업데이트합니다.
    3.  **에피소드 2**:
        - `상태 3`에서 시작 → `상태 1`로 이동.
        - `Q(3, 1)` 값 계산: `R(3, 1) + 0.8 * Max[Q(1, actions)]` = `0 + 0.8 * Max(Q(1,3), Q(1,5))` = `0 + 0.8 * 100` = `80`.
        - Q 행렬의 `(3, 1)` 위치에 80을 업데이트합니다.
    4.  **수렴(Convergence)**: 수많은 에피소드를 반복하면 Q 행렬의 값들이 더 이상 변하지 않는 안정된 상태(수렴)에 도달합니다.
    5.  **최적 경로 찾기**: 학습이 완료된 Q 행렬을 보고, 현재 상태에서 가장 높은 Q 값을 가진 행동을 따라가면 최적의 경로를 찾을 수 있습니다. (예: 2 → 3 → 1 → 5)

### 4️⃣ 강화학습의 주요 개념 🧭
- **과제 유형 (Task Types)**:
    - **에피소드 과제 (Episodic Tasks)**: 명확한 시작과 끝(종료 상태)이 있는 과제. (예: 게임 한 판, 미로 찾기)
    - **연속 과제 (Continuous Tasks)**: 종료 상태 없이 계속해서 이어지는 과제. (예: 주식 거래 봇)
- **탐험(Exploration)과 활용(Exploitation)의 트레이드오프**:
    - **활용 (Exploitation)**: 이미 알고 있는 정보 중 가장 보상이 높은 행동을 선택하는 것.
    - **탐험 (Exploration)**: 더 좋은 보상을 찾기 위해 새로운, 가보지 않은 행동을 시도하는 것.
    - **중요성**: 단기적인 보상(활용)에만 집중하면 장기적으로 더 큰 보상(탐험으로 발견)을 놓칠 수 있으므로, 둘 사이의 적절한 균형이 필수적입니다.

## 오늘의 연습문제 📝
- **문제 1**: Q-러닝에서 R 행렬과 Q 행렬의 역할은 각각 무엇이며, 왜 두 가지 행렬이 모두 필요한가요?
- **문제 2**: 감마(Gamma) 값이 0.1일 때와 0.9일 때 에이전트의 행동 패턴은 어떻게 달라질까요? 각각 어떤 상황에 더 적합할지 설명해 보세요.
- **문제 3**: 강의 예제에서, 에이전트가 `상태 4`에 있고, 현재 Q 행렬이 모두 0이라고 가정합니다. `상태 4`에서 `상태 5`(목표)로 이동할 때의 `Q(4, 5)` 값을 Q-러닝 공식을 이용해 계산해 보세요. (단, Gamma = 0.8, R(4, 5) = 100)
